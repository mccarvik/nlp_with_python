{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 7\n",
    "\n",
    "## Extracting Information from Text\n",
    "\n",
    "*The html version of this chapter in the book is available [here](https://www.nltk.org/book/ch07.html \"ch07\").*\n",
    "\n",
    "### 1 Information Extraction\n",
    "\n",
    "*If location information was stored as a list of tuples and we wanted to extract information to answer the question \"Which organizations operate in Atlanta?\", we could use code thusly:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBDO South', 'Georgia-Pacific']\n"
     ]
    }
   ],
   "source": [
    "locs = [('Onmicom', 'IN', 'New York'),\n",
    "        ('DDB Needham', 'IN', 'New York'), \n",
    "        ('Kaplan Thaler Group', 'IN', 'New York'),\n",
    "        ('BBDO South', 'IN', 'Atlanta'),\n",
    "        ('Georgia-Pacific', 'IN', 'Atlanta')]\n",
    "query = [e1 for (e1, re1, e2) in locs if e2 == 'Atlanta']\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Information Extraction Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ie-architecture.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Figure 1.1__: Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (`entity`, `relation`, `entity`) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple `([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below could perform the first three tasks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chunk-segmentation.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Figure 2.1__: Segmentation and Labeling at both the Token and Chunk Levels*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Noun Phrase Chunking\n",
    "\n",
    "*Some WSJ text with `NP`-chunks marked using brackets:*\n",
    "\n",
    "[ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB well/RB there/RB ./.\n",
    "\n",
    "*To make a chunker, we have to define a chunk grammar:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), \n",
    "            (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "# look for an optional determiner, followed by any number of adjectives, \n",
    "# and a required noun\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "# result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`result.draw()` will draw the parse tree in a new window.  If I leave this cell in this notebook, none of the cells below it will run until the window is closed, so I'm not going to leave that cell in this notebook.*\n",
    "\n",
    "*This [SO discussion](https://stackoverflow.com/questions/23429117/saving-nltk-drawn-parse-tree-to-image-file \"Parse Tree to PNG\") has instructions on how to convert this to a .png file, but I could not get it to work.*\n",
    "\n",
    "*Below is the figure for the tree in the book:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch07-tree-1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tag Patterns\n",
    "\n",
    "*It's difficult to come up with __tag patterns__ that will match all possible cases.*\n",
    "\n",
    "__Your Turn__: Try to come up with tag patterns to cover the cases below. Test them using the graphical interface `nltk.app.chunkparser()`. Continue to refine your tag patterns with the help of the feedback given by this tool.\n",
    "\n",
    "```\n",
    "his/PRP$ Mansion/NNP House/NNP speech/NN\n",
    "the/DT price/NN cutting/VBG\n",
    "3/CD %/NN to/TO 4/CD %/NN\n",
    "more/JJR than/IN 10/CD %/NN\n",
    "the/DT fastest/JJS developing/VBG trends/NNS\n",
    "'s/POS skill/NN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The chunkparser app `nltk.app.chunkparser()` opens in a new window, so this is another piece of code I can't leave in the notebook.  Basically, to cover all of the cases listed in the book, I have to include every possible tag pattern.  I can't guarantee the order of the tags will match all possible noun phrases.*\n",
    "\n",
    "`NP:{<DT>?<PRP.>?<POS>?<TO>?<CD>?<JJ.*>*<IN>?<VBG>?<NN.*>+<VB.>?.}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Chunking with Regular Expressions\n",
    "\n",
    "*A simple chunk grammar with two rules:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and noun\n",
    "        {<NNP>+}              # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "            (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "print(cp.parse(sentence))\n",
    "# nltk.app.chunkparser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If a tag pattern matches at overlapping locations, the leftmost match takes precedence.  E.g., below there are three consecutive nouns, but only the first two will be chunked:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP money/NN market/NN) fund/NN)\n"
     ]
    }
   ],
   "source": [
    "nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "grammar = \"NP: {<NN><NN>}  # Chunk two consecutive nouns\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Exploring Text Corpora\n",
    "\n",
    "*We can use chunkers to extract phrases matching a particular sequence of part-of-speech tags:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "brown = nltk.corpus.brown\n",
    "chunked = []\n",
    "\n",
    "for sent in brown.tagged_sents():\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK': \n",
    "            chunked.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n"
     ]
    }
   ],
   "source": [
    "for t in chunked[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__: Encapsulate the above example inside a function `find_chunks()` that takes a chunk string like `\"CHUNK: {<V.*> <TO> <V.*>}\"` as an argument. Use it to search the corpus for several other patterns, such as four or more nouns in a row, e.g. `\"NOUNS: {<N.*>{4,}}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks(corpus, chunk):\n",
    "    \"\"\"\n",
    "    Returns chunks with a given set of tags from a corpus of \n",
    "    tagged sentences.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    corpus: corpus of chunked sentences.\n",
    "    chunk:  regular expression with POS tags. Should be in the format:\n",
    "            'CHUNK_LABEL: {TAG1}{TAG2}...'. N.B. a colon (:) must separate\n",
    "            the label from the tags.\n",
    "    \"\"\"\n",
    "    chunked = []\n",
    "    # retrieve label for chunks\n",
    "    chunk_label = re.search(r'[^\"][^:]*', chunk).group()\n",
    "    cp = nltk.RegexpParser(chunk)\n",
    "    for sent in corpus:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == chunk_label:\n",
    "                chunked.append(subtree)\n",
    "    \n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NOUNS Court/NN-TL Judge/NN-TL Durwood/NP Pye/NP)\n",
      "(NOUNS Mayor-nominate/NN-TL Ivan/NP Allen/NP Jr./NP)\n",
      "(NOUNS Georgia's/NP$ automobile/NN title/NN law/NN)\n",
      "(NOUNS State/NN-TL Welfare/NN-TL Department's/NN$-TL handling/NN)\n",
      "(NOUNS Fulton/NP-TL Tax/NN-TL Commissioner's/NN$-TL Office/NN-TL)\n"
     ]
    }
   ],
   "source": [
    "nouns_4Xs = find_chunks(brown.tagged_sents(), \"NOUNS: {<N.*>{4,}}\")\n",
    "for n in nouns_4Xs[:5]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Chinking\n",
    "\n",
    "*A __chink__ is the opposite of a chunk: it's a sequences of tokens that is not included in a chunk.  In the example following, `barked/VBD at/IN` is a chink:*\n",
    "\n",
    "`[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]`\n",
    "\n",
    "*To form a chink, put a set of tags __outside curly braces__, (e.g. `}<TAG1><TAG2>{` ).  Here's an example:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    " NP:\n",
    "  {<.*>+}      # Chunk everything\n",
    "  }<VBD|IN>+{  # Chink sequences of VBD and IN\n",
    " \"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"),\n",
    "            (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Representing Chunks: Tags vs Trees\n",
    "\n",
    "*Chunks can be represented with either tags or trees.  The most common scheme uses __IOB tags__: tokens are tagged either `I` (inside), `O` (outside), or `B` (begin).  `B` marks the beginning of the chunk, and all subsequent tokens within the chunk are tagged `I`.  Anything else is tagged `O`.  `B` and `I` are suffixed with the chunk type, e.g. `B-NP`, `I-NP`.  `O` doesn't get a suffix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chunk-tagrep.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Figure 2.5__: Tag Representation of Chunk Structures*\n",
    "\n",
    "*This is how the information above would appear in a file:*\n",
    "\n",
    "```\n",
    "We PRP B-NP\n",
    "saw VBD O\n",
    "the DT B-NP\n",
    "yellow JJ I-NP\n",
    "dog NN I-NP\n",
    "\n",
    "```\n",
    "\n",
    "*Here is a chunk structure represented as a tree:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chunk-treerep.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Developing and Evaluating Chunkers\n",
    "\n",
    "#### 3.1 Reading IOB Format and the CoNLL 2000 Corpus\n",
    "\n",
    "*The function `chunk.conllstr2tree()` builds a tree from IOB strings.  It also allows up to choose any subset of three chunk types to use.  The code below opens a new window, so I'm not going to include it in a code cell in this notebook:*\n",
    "\n",
    "```\n",
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "'''\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch07-tree-2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at a sentence from the corpus:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print(conll2000.chunked_sents('train.txt')[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at just the `NP` chunks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types = ['NP'])[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Simple Evaluation and Baselines\n",
    "\n",
    "*Evaluation score for a baseline parser that creates no chunks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\868133421.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This means that 43.4% of the words are tagged `O`, or not in an `NP` chunk.*\n",
    "\n",
    "*Here's a second regex chunker.  It's a very naive one that looks for tags that are characteristic of noun phrase tags (e.g., `CD`, `DT`, and `JJ`):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\1685729691.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We could also build a __unigram tagger__ that would find the correct chunk tag (either `B`, `I`, or `O`) based on the POS tag.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train the tagger with the CoNLL 2000 corpus and test it:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\4003938190.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(unigram_chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types = ['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can examine how this chunker would assign values to each of the POS tags in the corpus:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
     ]
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                     for (word, pos) in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We could just as easily make a bigram chunker:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\607639372.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(bigram_chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Training Classifier-Based Chunkers\n",
    "\n",
    "*POS tags alone are not enough to chunk a sentence.  Consider these two sentences:*\n",
    "\n",
    "* a.\t\tJoey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
    "\n",
    "* b.\t\tNick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
    "\n",
    "*Although the POS tags are the same, the chunks would be different: __a.__ would have three distinct chunks, whereas __b.__ would have just two.  We therefore need information about the content of the words.  We can use a classifier to attempt to get that.*\n",
    "\n",
    "*__N.B.:__ The code in the book has a classifier that uses the `megam` algorithm, but getting this algorithm to work is very buggy. I therefore just commented out this line, which will cause the classifier to use the default algorithm (which I think is IIS).  (Perhaps because of this,) (t)he classifier is very, very slow to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = [] \n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "                \n",
    "        self.classifier = nltk.MaxentClassifier.train(train_set, \n",
    "                                                     # algorithm = 'megam', \n",
    "                                                     trace = 0)\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w, t), c) for (w, t, c) in\n",
    "                          nltk.chunk.tree2conlltags(sent)]\n",
    "                          for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w, t, c) for ((w, t), c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\1162305550.py:8: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "#  START LONG RUN\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding a feature for the previous POS tag:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\2720991064.py:12: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.6%%\n",
      "    Precision:     82.0%%\n",
      "    Recall:        87.2%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "# adding feature for prevpos\n",
    "\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos}\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding a feature for the word:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\2146987520.py:10: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.6%%\n",
      "    Precision:     84.6%%\n",
      "    Recall:        89.8%%\n",
      "    F-Measure:     87.1%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding a number of different features:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\1089099831.py:30: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.0%%\n",
      "    Precision:     88.3%%\n",
      "    Recall:        91.1%%\n",
      "    F-Measure:     89.7%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    if i == len(sentence) - 1:\n",
    "        nextword, nextpos = '<END>', '<END>'\n",
    "    else:\n",
    "        nextwor, nextpos = sentence[i + 1]\n",
    "    \n",
    "    return {\"pos\": pos, \n",
    "            \"word\": word, \n",
    "            \"prevpos\": prevpos,\n",
    "            \"nextpos\": nextpos,\n",
    "            \"prevpos+pos\": \"%s + %s\" % (prevpos, pos),\n",
    "            \"pos+nextpos\": \"%s + %s\" % (pos, nextpos),\n",
    "            \"tags-since-dt\": tags_since_dt(sentence, i)}\n",
    "\n",
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn:__ Try adding different features to the feature extractor function `npchunk_features`, and see if you can further improve the performance of the NP chunker.\n",
    "\n",
    "*I just added all the features that were readily available but had yet not been used (i.e., `prevword` and `nextword`), and an easy-to-calculate one (`length`):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_13532\\1279625563.py:33: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.3%%\n",
      "    Precision:     89.7%%\n",
      "    Recall:        92.4%%\n",
      "    F-Measure:     91.0%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    if i == len(sentence) - 1:\n",
    "        nextword, nextpos = '<END>', '<END>'\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i + 1]\n",
    "    \n",
    "    return {\"pos\": pos, \n",
    "            \"word\": word, \n",
    "            \"prevpos\": prevpos,\n",
    "            \"prevword\": prevword,\n",
    "            \"nextpos\": nextpos,\n",
    "            \"nextword\": nextword,\n",
    "            \"prevpos+pos\": \"%s + %s\" % (prevpos, pos),\n",
    "            \"pos+nextpos\": \"%s + %s\" % (pos, nextpos),\n",
    "            \"tags-since-dt\": tags_since_dt(sentence, i),\n",
    "            \"length\": len(sentence)}\n",
    "\n",
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))\n",
    "# END LONG RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I was pleasantly surprised to see that this classifier performs slightly better than the ones exemplified in this chapter.  As mentioned above, the classifier is extremely slow.  Using trial and error to find the most useful features would be an exercise in patience that I'm not in the mood for now.*\n",
    "\n",
    "### 4 Recursion in Linguistic Structure\n",
    "\n",
    "#### 4.1 Building Nested Structure with Cascaded Chunkers\n",
    "\n",
    "*We can also build deeper chunks:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|JJ|NN.*>+}           # Chunk sequences of DT, JJ, NN\n",
    "    PP: {<IN><NP>}                # Chunk prepositions followed by NP\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments\n",
    "    CLAUSE: {<NP><VP>}            # Chunk NP, VP\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "            (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
    "            (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
    "            (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If the chunker misses the deeper structure, we can add the argument `loop` to specify the number of times the set of patterns should be run:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop = 2)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Trees\n",
    "\n",
    "*Here's a example:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch07-tree-3.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We use family metaphors for the relationships of nodes in a tree.  E.g., `S` is the __parent__ of `VP`, and `VP` is the child of `S`.  Since `NP` and `VP` are both __children__ of `S`, we could say they are __siblings__. Here is a text form for specifying trees:*\n",
    "\n",
    "```\n",
    "(S\n",
    "   (NP Alice)\n",
    "   (VP\n",
    "      (V chased)\n",
    "      (NP\n",
    "         (Det the)\n",
    "         (N rabbit))))\n",
    "```\n",
    "\n",
    "*To create a tree in NLTK, give a node label and a list of children:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n"
     ]
    }
   ],
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "print(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP the rabbit)\n"
     ]
    }
   ],
   "source": [
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These can be incorporated into larger trees:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP chased (NP the rabbit)))\n"
     ]
    }
   ],
   "source": [
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "print(tree4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Methods for trees:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP chased (NP the rabbit))\n"
     ]
    }
   ],
   "source": [
    "print(tree4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VP'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4[1].label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'chased', 'the', 'rabbit']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rabbit'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4[1][1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is also of course the `tree.draw()` method.*\n",
    "\n",
    "#### 4.3 Tree Traversal\n",
    "\n",
    "*It is standard to use a recursive function to traverse a tree:*\n",
    "\n",
    "*__N.B.:__ Code is not working.*\n",
    "\n",
    "```\n",
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        print(t, end = \" \")\n",
    "    else:\n",
    "        # Now we know that t.node is defined\n",
    "        print('(', t.label(), end = \" \")\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print(')', end = \" \")\n",
    "        \n",
    "t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))')\n",
    "traverse(t)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Named Entity Recognition\n",
    "\n",
    "*Commonly Used Types of NE:*\n",
    "\n",
    "| __NE Type__      | __Examples__                                |\n",
    "|:-------------|:----------------------------------------|\n",
    "| ORGANIZATION | *Georgia-Pacific Corp., WHO*              |\n",
    "| PERSON       | *Eddy Bonte, President Obama*             |\n",
    "| LOCATION     | *Murray River, Mount Everest*             |\n",
    "| DATE         | *June, 2008-06-29*                        |\n",
    "| TIME         | *two fifty a m, 1:30 p.m.*                |\n",
    "| MONEY        | *175 million Canadian Dollars, GBP 10.40* |\n",
    "| PERCENT      | *twenty pct, 18.75 %*                     |\n",
    "| FACILITY     | *Washington Monument, Stonehenge*         |\n",
    "| GPE          | *South East Asia, Midlothian*             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is common to use a __gazetteer__, or geographical dictionary, to identify NEs.  However, this is not error free:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\locations.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another problem is that while certain entries almost surely belong to specific categories, there may be some ambiguity.  E.g., __May__ is most likely part of DATE, but it could also be a PERSON.  Similarly, __Christian Dior__ looks like a PERSON, but it's more likely an ORGANIZATION.*\n",
    "\n",
    "*We can also use the IOB format for NEs:*\n",
    "\n",
    "```\n",
    "Eddy N B-PER\n",
    "Bonte N I-PER\n",
    "is V O\n",
    "woordvoerder N O\n",
    "van Prep O\n",
    "diezelfde Pron O\n",
    "Hogeschool N B-ORG\n",
    ". Punc O\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is a classifier that has been trainined to recognize named entities, and we can access it with `nltk.ne_chunk()`.  If we set `binary = True`, then NEs are tagged as `NE`.  Otherwise labels are used:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP)\n",
      "  T./NNP\n",
      "  Mossman/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE University/NNP)\n",
      "  of/IN\n",
      "  (NE Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (NE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(nltk.ne_chunk(sent, binary = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION University/NNP)\n",
      "  of/IN\n",
      "  (PERSON Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (GPE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Relation Extraction\n",
    "\n",
    "*Below is code that will find triples in the form (X __in__ Y). The regex `(?!\\b.+ing)` is negative lookahead assertion that allows us to ignore strings where __in__ is followed by a gerund.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "                                     corpus = 'ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the `conll2002` Dutch corpus has NE annotation and POS tags, we can devise patterns sensitive to these tags.  `.clause()` allows us to print the relations in a clausal form.  The parameter `relsym` specifies the binary relation symbol.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|      # 3rd sing present and\n",
    "was/V|     # past forms of the verb zijn ('be')\n",
    "werd/V     # and also present\n",
    "wordt/V    # past of worden ('become')\n",
    ")\n",
    ".*         # followed by anything\n",
    "van/Prep   # followed by van ('of')\n",
    "\"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
    "                                     corpus = 'conll2002', pattern = VAN):\n",
    "        print(nltk.sem.clause(rel, relsym = \"VAN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn:__ Replace the last line by `print(nltk.rtuple(rel, lcon=True, rcon=True))`. This will show you the actual words that intervene between the two NEs and also their left and right context, within a default 10-word window. With the help of a Dutch dictionary, you might be able to figure out why the result `VAN('annie_lennox', 'eurythmics')` is a false hit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Actually, the code no longer returns `VAN('annie_lennox', 'eurythmics')`, so it's not possible to see why it's a false hit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Summary\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "### 8 Further Reading\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "### 9 Exercises\n",
    "\n",
    "##### 1. \n",
    "\n",
    "☼ The IOB format categorizes tagged tokens as `I`, `O` and `B`. Why are three tags necessary? What problem would be caused if we used `I` and `O` tags exclusively?\n",
    "\n",
    "*One issue that would arise with using only `I` and `O` tags is that it would be impossible to distinguish one long chunk from two smaller chunks immediately adjacent to each other, because there would be no tag identifying the boundary.*\n",
    "\n",
    "##### 2. \n",
    "\n",
    "☼ Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases.\n",
    "\n",
    "*This grammar is designed to work only with the examples in this exercise, and will likely fail when applied to general sentences.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<CD|JJ><NN.>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP many/JJ researchers/NNS))\n",
      "(S (NP two/CD weeks/NNS))\n",
      "(S (NP both/DT new/JJ positions/NNS))\n"
     ]
    }
   ],
   "source": [
    "nps =[[(\"many\", \"JJ\"), (\"researchers\", \"NNS\")], \n",
    "      [(\"two\", \"CD\"), (\"weeks\", \"NNS\")], \n",
    "      [(\"both\", \"DT\"), (\"new\", \"JJ\"), (\"positions\", \"NNS\")]]\n",
    "\n",
    "for n in nps:\n",
    "    print(cp.parse(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "\n",
    "☼ Pick one of the three chunk types in the CoNLL corpus. Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular expression chunker `nltk.RegexpParser`. Discuss any tag sequences that are difficult to chunk reliably.\n",
    "\n",
    "*Inspect the first 20 sentences in the corpus:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (S\n",
      "  Confidence/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  another/DT\n",
      "  sharp/JJ\n",
      "  dive/NN\n",
      "  if/IN\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  for/IN\n",
      "  September/NNP\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  for/IN\n",
      "  release/NN\n",
      "  tomorrow/NN\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  a/DT\n",
      "  substantial/JJ\n",
      "  improvement/NN\n",
      "  from/IN\n",
      "  July/NNP\n",
      "  and/CC\n",
      "  August/NNP\n",
      "  's/POS\n",
      "  near-record/JJ\n",
      "  deficits/NNS\n",
      "  ./.)\n",
      "1 (S\n",
      "  Chancellor/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Exchequer/NNP\n",
      "  Nigel/NNP\n",
      "  Lawson/NNP\n",
      "  's/POS\n",
      "  restated/VBN\n",
      "  commitment/NN\n",
      "  to/TO\n",
      "  a/DT\n",
      "  firm/NN\n",
      "  monetary/JJ\n",
      "  policy/NN\n",
      "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "  a/DT\n",
      "  freefall/NN\n",
      "  in/IN\n",
      "  sterling/NN\n",
      "  over/IN\n",
      "  the/DT\n",
      "  past/JJ\n",
      "  week/NN\n",
      "  ./.)\n",
      "2 (S\n",
      "  But/CC\n",
      "  analysts/NNS\n",
      "  (VP reckon/VBP)\n",
      "  underlying/VBG\n",
      "  support/NN\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  (VP has/VBZ been/VBN eroded/VBN)\n",
      "  by/IN\n",
      "  the/DT\n",
      "  chancellor/NN\n",
      "  's/POS\n",
      "  failure/NN\n",
      "  (VP to/TO announce/VB)\n",
      "  any/DT\n",
      "  new/JJ\n",
      "  policy/NN\n",
      "  measures/NNS\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  Mansion/NNP\n",
      "  House/NNP\n",
      "  speech/NN\n",
      "  last/JJ\n",
      "  Thursday/NNP\n",
      "  ./.)\n",
      "3 (S\n",
      "  This/DT\n",
      "  (VP has/VBZ increased/VBN)\n",
      "  the/DT\n",
      "  risk/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  government/NN\n",
      "  (VP being/VBG forced/VBN to/TO increase/VB)\n",
      "  base/NN\n",
      "  rates/NNS\n",
      "  to/TO\n",
      "  16/CD\n",
      "  %/NN\n",
      "  from/IN\n",
      "  their/PRP$\n",
      "  current/JJ\n",
      "  15/CD\n",
      "  %/NN\n",
      "  level/NN\n",
      "  (VP to/TO defend/VB)\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  ,/,\n",
      "  economists/NNS\n",
      "  and/CC\n",
      "  foreign/JJ\n",
      "  exchange/NN\n",
      "  market/NN\n",
      "  analysts/NNS\n",
      "  (VP say/VBP)\n",
      "  ./.)\n",
      "4 (S\n",
      "  ``/``\n",
      "  The/DT\n",
      "  risks/NNS\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  bad/JJ\n",
      "  trade/NN\n",
      "  figure/NN\n",
      "  (VP are/VBP)\n",
      "  very/RB\n",
      "  heavily/RB\n",
      "  on/IN\n",
      "  the/DT\n",
      "  down/JJ\n",
      "  side/NN\n",
      "  ,/,\n",
      "  ''/''\n",
      "  (VP said/VBD)\n",
      "  Chris/NNP\n",
      "  Dillow/NNP\n",
      "  ,/,\n",
      "  senior/JJ\n",
      "  U.K./NNP\n",
      "  economist/NN\n",
      "  at/IN\n",
      "  Nomura/NNP\n",
      "  Research/NNP\n",
      "  Institute/NNP\n",
      "  ./.)\n",
      "5 (S\n",
      "  ``/``\n",
      "  If/IN\n",
      "  there/EX\n",
      "  (VP is/VBZ)\n",
      "  another/DT\n",
      "  bad/JJ\n",
      "  trade/NN\n",
      "  number/NN\n",
      "  ,/,\n",
      "  there/EX\n",
      "  (VP could/MD be/VB)\n",
      "  an/DT\n",
      "  awful/JJ\n",
      "  lot/NN\n",
      "  of/IN\n",
      "  pressure/NN\n",
      "  ,/,\n",
      "  ''/''\n",
      "  (VP noted/VBD)\n",
      "  Simon/NNP\n",
      "  Briscoe/NNP\n",
      "  ,/,\n",
      "  U.K./NNP\n",
      "  economist/NN\n",
      "  for/IN\n",
      "  Midland/NNP\n",
      "  Montagu/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  unit/NN\n",
      "  of/IN\n",
      "  Midland/NNP\n",
      "  Bank/NNP\n",
      "  PLC/NNP\n",
      "  ./.)\n",
      "6 (S\n",
      "  Forecasts/NNS\n",
      "  for/IN\n",
      "  the/DT\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  (VP range/VBP)\n",
      "  widely/RB\n",
      "  ,/,\n",
      "  but/CC\n",
      "  few/JJ\n",
      "  economists/NNS\n",
      "  (VP expect/VBP)\n",
      "  the/DT\n",
      "  data/NNS\n",
      "  (VP to/TO show/VB)\n",
      "  a/DT\n",
      "  very/RB\n",
      "  marked/VBN\n",
      "  improvement/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  #/#\n",
      "  2/CD\n",
      "  billion/CD\n",
      "  -LRB-/(\n",
      "  $/$\n",
      "  3.2/CD\n",
      "  billion/CD\n",
      "  -RRB-/)\n",
      "  deficit/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  current/JJ\n",
      "  account/NN\n",
      "  (VP reported/VBD)\n",
      "  for/IN\n",
      "  August/NNP\n",
      "  ./.)\n",
      "7 (S\n",
      "  The/DT\n",
      "  August/NNP\n",
      "  deficit/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  #/#\n",
      "  2.2/CD\n",
      "  billion/CD\n",
      "  gap/NN\n",
      "  (VP registered/VBN)\n",
      "  in/IN\n",
      "  July/NNP\n",
      "  (VP are/VBP topped/VBN)\n",
      "  only/RB\n",
      "  by/IN\n",
      "  the/DT\n",
      "  #/#\n",
      "  2.3/CD\n",
      "  billion/CD\n",
      "  deficit/NN\n",
      "  of/IN\n",
      "  October/NNP\n",
      "  1988/CD\n",
      "  ./.)\n",
      "8 (S\n",
      "  Sanjay/NNP\n",
      "  Joshi/NNP\n",
      "  ,/,\n",
      "  European/JJ\n",
      "  economist/NN\n",
      "  at/IN\n",
      "  Baring/NNP\n",
      "  Brothers/NNPS\n",
      "  &/CC\n",
      "  Co./NNP\n",
      "  ,/,\n",
      "  (VP said/VBD)\n",
      "  there/EX\n",
      "  (VP is/VBZ)\n",
      "  no/DT\n",
      "  sign/NN\n",
      "  that/IN\n",
      "  Britain/NNP\n",
      "  's/POS\n",
      "  manufacturing/NN\n",
      "  industry/NN\n",
      "  (VP is/VBZ transforming/VBG)\n",
      "  itself/PRP\n",
      "  (VP to/TO boost/VB)\n",
      "  exports/NNS\n",
      "  ./.)\n",
      "9 (S\n",
      "  At/IN\n",
      "  the/DT\n",
      "  same/JJ\n",
      "  time/NN\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  (VP remains/VBZ)\n",
      "  fairly/RB\n",
      "  pessimistic/JJ\n",
      "  about/IN\n",
      "  the/DT\n",
      "  outlook/NN\n",
      "  for/IN\n",
      "  imports/NNS\n",
      "  ,/,\n",
      "  given/VBN\n",
      "  continued/VBD\n",
      "  high/JJ\n",
      "  consumer/NN\n",
      "  and/CC\n",
      "  capital/NN\n",
      "  goods/NNS\n",
      "  inflows/NNS\n",
      "  ./.)\n",
      "10 (S\n",
      "  He/PRP\n",
      "  (VP reckons/VBZ)\n",
      "  the/DT\n",
      "  current/JJ\n",
      "  account/NN\n",
      "  deficit/NN\n",
      "  (VP will/MD narrow/VB)\n",
      "  to/TO\n",
      "  only/RB\n",
      "  #/#\n",
      "  1.8/CD\n",
      "  billion/CD\n",
      "  in/IN\n",
      "  September/NNP\n",
      "  ./.)\n",
      "11 (S\n",
      "  However/RB\n",
      "  ,/,\n",
      "  Mr./NNP\n",
      "  Dillow/NNP\n",
      "  (VP said/VBD)\n",
      "  he/PRP\n",
      "  (VP believes/VBZ)\n",
      "  that/IN\n",
      "  a/DT\n",
      "  reduction/NN\n",
      "  in/IN\n",
      "  raw/JJ\n",
      "  material/NN\n",
      "  stockbuilding/VBG\n",
      "  by/IN\n",
      "  industry/NN\n",
      "  (VP could/MD lead/VB)\n",
      "  to/TO\n",
      "  a/DT\n",
      "  sharp/JJ\n",
      "  drop/NN\n",
      "  in/IN\n",
      "  imports/NNS\n",
      "  ./.)\n",
      "12 (S\n",
      "  Combined/VBN\n",
      "  with/IN\n",
      "  at/IN\n",
      "  least/JJS\n",
      "  some/DT\n",
      "  rebound/NN\n",
      "  in/IN\n",
      "  exports/NNS\n",
      "  after/IN\n",
      "  August/NNP\n",
      "  's/POS\n",
      "  unexpected/JJ\n",
      "  decline/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  deficit/NN\n",
      "  (VP could/MD narrow/VB)\n",
      "  to/TO\n",
      "  as/RB\n",
      "  little/JJ\n",
      "  as/IN\n",
      "  #/#\n",
      "  1.3/CD\n",
      "  billion/CD\n",
      "  ./.)\n",
      "13 (S\n",
      "  Mr./NNP\n",
      "  Briscoe/NNP\n",
      "  ,/,\n",
      "  who/WP\n",
      "  also/RB\n",
      "  (VP forecasts/VBZ)\n",
      "  a/DT\n",
      "  #/#\n",
      "  1.3/CD\n",
      "  billion/CD\n",
      "  current/JJ\n",
      "  account/NN\n",
      "  gap/NN\n",
      "  ,/,\n",
      "  (VP warns/VBZ)\n",
      "  that/IN\n",
      "  even/RB\n",
      "  if/IN\n",
      "  the/DT\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  (VP are/VBP)\n",
      "  bullish/JJ\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  currency/NN\n",
      "  (VP wo/MD n't/RB advance/VB)\n",
      "  much/JJ\n",
      "  because/IN\n",
      "  investors/NNS\n",
      "  (VP will/MD want/VB to/TO see/VB)\n",
      "  further/JJ\n",
      "  evidence/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  turnaround/NN\n",
      "  before/IN\n",
      "  (VP adjusting/VBG)\n",
      "  positions/NNS\n",
      "  ./.)\n",
      "14 (S\n",
      "  Nevertheless/RB\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  (VP noted/VBD)\n",
      "  ,/,\n",
      "  ``/``\n",
      "  No/DT\n",
      "  one/PRP\n",
      "  (VP will/MD want/VB to/TO go/VB)\n",
      "  into/IN\n",
      "  the/DT\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  without/IN\n",
      "  a/DT\n",
      "  flat/JJ\n",
      "  position/NN\n",
      "  ''/''\n",
      "  in/IN\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  ./.)\n",
      "15 (S\n",
      "  Meanwhile/RB\n",
      "  ,/,\n",
      "  overall/JJ\n",
      "  evidence/NN\n",
      "  on/IN\n",
      "  the/DT\n",
      "  economy/NN\n",
      "  (VP remains/VBZ)\n",
      "  fairly/RB\n",
      "  clouded/VBN\n",
      "  ./.)\n",
      "16 (S\n",
      "  In/IN\n",
      "  his/PRP$\n",
      "  Mansion/NNP\n",
      "  House/NNP\n",
      "  speech/NN\n",
      "  ,/,\n",
      "  Mr./NNP\n",
      "  Lawson/NNP\n",
      "  (VP warned/VBD)\n",
      "  that/IN\n",
      "  a/DT\n",
      "  further/JJ\n",
      "  slowdown/NN\n",
      "  (VP can/MD be/VB expected/VBN)\n",
      "  as/IN\n",
      "  the/DT\n",
      "  impact/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  last/JJ\n",
      "  rise/NN\n",
      "  in/IN\n",
      "  interest/NN\n",
      "  rates/NNS\n",
      "  earlier/RBR\n",
      "  this/DT\n",
      "  month/NN\n",
      "  (VP takes/VBZ)\n",
      "  effect/NN\n",
      "  ./.)\n",
      "17 (S\n",
      "  U.K./JJ\n",
      "  base/NN\n",
      "  rates/NNS\n",
      "  (VP are/VBP)\n",
      "  at/IN\n",
      "  their/PRP$\n",
      "  highest/JJS\n",
      "  level/NN\n",
      "  in/IN\n",
      "  eight/CD\n",
      "  years/NNS\n",
      "  ./.)\n",
      "18 (S\n",
      "  But/CC\n",
      "  consumer/NN\n",
      "  expenditure/NN\n",
      "  data/NNS\n",
      "  (VP released/VBD)\n",
      "  Friday/NNP\n",
      "  (VP do/VBP n't/RB suggest/VB)\n",
      "  that/IN\n",
      "  the/DT\n",
      "  U.K./NNP\n",
      "  economy/NN\n",
      "  (VP is/VBZ slowing/VBG)\n",
      "  that/DT\n",
      "  quickly/RB\n",
      "  ./.)\n",
      "19 (S\n",
      "  The/DT\n",
      "  figures/NNS\n",
      "  (VP show/VBP)\n",
      "  that/DT\n",
      "  spending/NN\n",
      "  (VP rose/VBD)\n",
      "  0.1/CD\n",
      "  %/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  third/JJ\n",
      "  quarter/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  second/JJ\n",
      "  quarter/NN\n",
      "  and/CC\n",
      "  (VP was/VBD)\n",
      "  up/IN\n",
      "  3.8/CD\n",
      "  %/NN\n",
      "  from/IN\n",
      "  a/DT\n",
      "  year/NN\n",
      "  ago/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "for i in range(20):\n",
    "    print(i, conll2000.chunked_sents('train.txt', chunk_types = ['VP'])[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The most common patterns I observed were:\n",
    "\n",
    "* __verbs alone:__\n",
    "\n",
    "`(VP rose/VBD)`\n",
    "\n",
    "* __verbs preceded by auxiliary verbs:__\n",
    "\n",
    "`(VP are/VBP topped/VBN)`\n",
    "`(VP has/VBZ been/VBN eroded/VBN)`\n",
    "\n",
    "* __verbs preceded by modals:__ \n",
    "\n",
    "`(VP could/MD be/VB)`\n",
    "\n",
    "* __verbs followed with the infinitive:__\n",
    "\n",
    "`(VP has/VBZ helped/VBN to/TO prevent/VB)`\n",
    "`(VP being/VBG forced/VBN to/TO increase/VB)`\n",
    "\n",
    "* __verbs preceded by adverbs:__*\n",
    "`(VP is/VBZ widely/RB expected/VBN to/TO take/VB)`\n",
    "\n",
    "*It would be difficult to craft regex patterns to capture all possible patterns, let alone the most common ones.*\n",
    "\n",
    "*Let's try with a naive approach, using tags that begin with the letters that are characteristic of VP tags:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\812375391.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.3%%\n",
      "    Precision:     64.2%%\n",
      "    Recall:        80.4%%\n",
      "    F-Measure:     71.4%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<[VRMT].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The parser does quite well, with a very high accuracy and recall.  However, its precision score is quite low.*\n",
    "\n",
    "*Let's try a more convoluted model that will attempts to use common tags in the places where they are most often found:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\3642609304.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.3%%\n",
      "    Precision:     74.4%%\n",
      "    Recall:        66.8%%\n",
      "    F-Measure:     70.4%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB>*<MD>?<VB.>?<TO>?<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The precision is significantly higher, but all other metrics have decreased slightly, or in the case of recall, significantly.*\n",
    "\n",
    "*Let's try to account for all of the types of adverb which we may encounter:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2221650695.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.4%%\n",
      "    Precision:     72.7%%\n",
      "    Recall:        66.9%%\n",
      "    F-Measure:     69.7%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB.>*<MD>?<VB.>?<TO>?<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pretty much a wash: half of the metrics went up slightly, the other half down slightly.*\n",
    "\n",
    "*Let's try adding a possible modal just after `<TO>?`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2754670863.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.3%%\n",
      "    Precision:     74.6%%\n",
      "    Recall:        66.9%%\n",
      "    F-Measure:     70.5%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB>*<MD>?<VB.>?<TO>?<MD>?<RB>*<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, nothing but slight changes. I have the feeling we could go on forever like this, making slight adjustments without creating any huge breakthroughs.*\n",
    "\n",
    "*Although this wasn't specified in the instructions, I'm curious to see what would happen if we used unigram and bigram chunkers:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\3219626249.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(unigram_chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.3%%\n",
      "    Precision:     60.5%%\n",
      "    Recall:        74.2%%\n",
      "    F-Measure:     66.7%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types = ['VP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\607639372.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(bigram_chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.5%%\n",
      "    Precision:     75.1%%\n",
      "    Recall:        85.7%%\n",
      "    F-Measure:     80.0%%\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The best scores I had for my `RegexpParser` were comparable to ones produced by the `UnigramChunker`, but the `BigramChunker` blew them all out of the water.  As was the case with POS taggers, regex classifiers seem to lag behind $n$-gram classifiers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ An early definition of *chunk* was the material that occurs between chinks. Develop a chunker that starts by putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program. Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules.\n",
    "\n",
    "*The practically simplest way to chink sentences would be to exclude verbs and prepositions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2409094270.py:9: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  65.5%%\n",
      "    Precision:     32.6%%\n",
      "    Recall:        26.0%%\n",
      "    F-Measure:     28.9%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: \n",
    "    {<.*>+}          \n",
    "    }<VB.|IN>+{           \n",
    "  \"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We didn't spend much time analyzing the mistakes of the parsers in this unit, nor did we spend a lot of time on extracting information from the parser.  So coming up with a \"utility program\" to analyze mistakes was no trivial task, and in the end I wasn't able to get anything.  The biggest part of the problem was that I wasn't sure what to compare. The test sentences and the parsed sentences always had the same part of speech tags, so there was no point comparing those.  The structure of the trees was of course different, but it wasn't clear how these were to be compared.  I tried using `.subtrees()` and `tree2conlltags` to get the IOB tags, but the problem with this is that the IOB tags are different depending upon which level of the tree we are in.  I thought it might be possible to use some recursive function to drill into the layers of the tree, but this will actually go all the way down to the individual letters of the words in the sentence, which will all be labeled `O`. So for the moment I'll put this problem on the back burner.*\n",
    "\n",
    "*What I ended up using as a stopgap was the `.incorrect()` method which was introduced in question 7.  I looked at the first 20 incorrect parses, and tried to identify any tags that may have cause the chinker to incorrectly parse the sentence.  I repeated this step several times, and for the sake of space I won't leave all the steps in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\3556782958.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cp.evaluate(test_sents).incorrect()[20:40]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ImmutableTree('NP', [('due', 'JJ')]),\n",
       " ImmutableTree('NP', [('Mr.', 'NNP'), ('Noriega', 'NNP'), (\"'s\", 'POS'), ('relationship', 'NN'), ('to', 'TO'), ('American', 'JJ'), ('intelligence', 'NN'), ('agencies', 'NNS')]),\n",
       " ImmutableTree('NP', [('George', 'NNP'), ('Kadonada', 'NNP'), (',', ','), ('US', 'PRP'), ('Facilities', 'NNPS'), ('chairman', 'NN'), ('and', 'CC'), ('president', 'NN'), ('.', '.')]),\n",
       " ImmutableTree('NP', [('to', 'TO'), ('$', '$'), ('550', 'CD'), ('million', 'CD')]),\n",
       " ImmutableTree('NP', [('$', '$'), ('852', 'CD'), ('million', 'CD'), ('a', 'DT'), ('year', 'NN'), ('ago', 'RB'), (',', ','), ('when', 'WRB'), ('the', 'DT'), ('bank', 'NN'), ('still', 'RB')]),\n",
       " ImmutableTree('NP', [('the', 'DT'), ('1987', 'CD'), ('crash', 'NN'), (',', ','), ('an', 'DT')]),\n",
       " ImmutableTree('NP', [('when', 'WRB'), ('GM', 'NNP'), (',', ','), ('the', 'DT'), ('world', 'NN'), (\"'s\", 'POS'), ('largest', 'JJS'), ('auto', 'NN'), ('maker', 'NN'), (',', ',')]),\n",
       " ImmutableTree('NP', [('1988', 'CD'), ('--', ':')]),\n",
       " ImmutableTree('NP', [('.', '.')]),\n",
       " ImmutableTree('NP', [('performance', 'NN')]),\n",
       " ImmutableTree('NP', [('$', '$'), ('983', 'CD'), ('million', 'CD')]),\n",
       " ImmutableTree('NP', [('special', 'JJ'), ('treatment', 'NN'), (',', ','), ('Mr.', 'NNP'), ('Allen', 'NNP')]),\n",
       " ImmutableTree('NP', [('the', 'DT'), ('human', 'NN'), ('and', 'CC'), ('animal', 'NN'), ('health-products', 'NNS')]),\n",
       " ImmutableTree('NP', [('Mr.', 'NNP'), ('Sanford', 'NNP'), (',', ','), ('U.S.', 'NNP'), ('Attorney', 'NNP'), ('Jack', 'NNP'), ('Eskenazi', 'NNP')]),\n",
       " ImmutableTree('NP', [('97', 'CD'), ('.', '.')]),\n",
       " ImmutableTree('NP', [('Beantown', 'NNP'), ('scribes', 'NNS'), (',', ','), ('who', 'WP'), ('spare', 'VB'), ('no', 'DT'), ('invective', 'NN'), ('when', 'WRB')]),\n",
       " ImmutableTree('NP', [('And', 'CC'), ('they', 'PRP'), ('wo', 'MD'), (\"n't\", 'RB'), ('consider', 'VB'), ('such', 'JJ'), ('a', 'DT'), ('dividend', 'NN'), (',', ','), ('the', 'DT'), ('trust', 'NN')]),\n",
       " ImmutableTree('NP', [('to', 'TO'), ('Mr.', 'NNP'), ('Seidman', 'NNP'), ('.', '.')]),\n",
       " ImmutableTree('NP', [('Linear', 'NNP'), ('Technology', 'NNP'), (',', ','), ('Milpitas', 'NNP'), (',', ','), ('Calif.', 'NNP'), (',', ',')]),\n",
       " ImmutableTree('NP', [('an', 'DT'), ('advertisement', 'NN'), ('Monday', 'NNP')])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).incorrect()[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\269162025.py:9: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.6%%\n",
      "    Precision:     64.8%%\n",
      "    Recall:        68.0%%\n",
      "    F-Measure:     66.3%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: \n",
    "    {<.*>+}          \n",
    "    }<VB.|IN|,|.|CC|TO|''|MD|``|RB|>+{           \n",
    "  \"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As was the case earlier, I used trial and error to settle upon the tags above.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.\n",
    "\n",
    "◑ Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the/DT receiving/VBG end/NN))\n",
      "(S (NP assistant/NN managing/VBG editor/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "    NP: {<DT><VBG><NN>}    # chunk determiner, gerund, and noun\n",
    "        {<NN><VBG><NN>}    # chunk noun, gerund, and noun\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = [[(\"the\", \"DT\"), (\"receiving\", \"VBG\"), (\"end\", \"NN\")], \n",
    "             [(\"assistant\", \"NN\"),  (\"managing\", \"VBG\"),  (\"editor\", \"NN\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It would be quite simple to create a one-line regular expression that can handle this:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the/DT receiving/VBG end/NN))\n",
      "(S (NP assistant/NN managing/VBG editor/NN))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|NN><VBG><NN>}    # chunk determiner/noun, gerund, and noun\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = [[(\"the\", \"DT\"), (\"receiving\", \"VBG\"), (\"end\", \"NN\")], \n",
    "             [(\"assistant\", \"NN\"),  (\"managing\", \"VBG\"),  (\"editor\", \"NN\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sentences of my own:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP a/DT thriving/VBG metropolis/NN))\n",
      "(S (NP temporary/NN acting/VBG director/NN))\n"
     ]
    }
   ],
   "source": [
    "sentences = [[(\"a\", \"DT\"), (\"thriving\", \"VBG\"), (\"metropolis\", \"NN\")], \n",
    "             [(\"temporary\", \"NN\"),  (\"acting\", \"VBG\"),  (\"director\", \"NN\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. \n",
    "\n",
    "◑ Write one or more tag patterns to handle coordinated noun phrases, e.g. \"July/NNP and/CC August/NNP\", \"all/DT your/PRP\\$ managers/NNS and/CC supervisors/NNS\", \"company/NN courts/NNS and/CC adjudicators/NNS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP July/NNP and/CC August/NNP))\n",
      "(S (NP all/DT your/PRP$ managers/NNS and/CC supervisors/NNS))\n",
      "(S (NP company/NN courts/NNS and/CC adjudicators/NNS))\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "    NP: {<DT>?<PRP.>?<NN.*>+<CC><NN.>} # Chunk coordinated noun phrases\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = [[(\"July\", \"NNP\"),  (\"and\", \"CC\"), (\"August\", \"NNP\")], \n",
    "             [(\"all\", \"DT\"), (\"your\", \"PRP$\"), (\"managers\", \"NNS\"), \n",
    "              (\"and\", \"CC\"), (\"supervisors\", \"NNS\")], \n",
    "             [(\"company\", \"NN\"), (\"courts\", \"NNS\"), \n",
    "              (\"and\", \"CC\"), (\"adjudicators\", \"NNS\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. \n",
    "\n",
    "◑ Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will produce errors.)\n",
    "\n",
    "* a. Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure.\n",
    "\n",
    "* b. Use the `chunkscore.missed()` and `chunkscore.incorrect()` methods to identify the errors made by your chunker. Discuss.\n",
    "\n",
    "* c. Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part a.:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\1034154155.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.3%%\n",
      "    Precision:     74.6%%\n",
      "    Recall:        66.9%%\n",
      "    F-Measure:     70.5%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB>*<MD>?<VB.>?<TO>?<MD>?<RB>*<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt'[:100], chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part b. Looking at the missed and incorrect chunks, no patterns that I could easily incorporate jump out at me:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2495054575.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cp.evaluate(test_sents).missed()[:20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ImmutableTree('VP', [('guarantee', 'VB')]),\n",
       " ImmutableTree('VP', [('to', 'TO'), ('be', 'VB')]),\n",
       " ImmutableTree('VP', [('to', 'TO'), ('provide', 'VB')]),\n",
       " ImmutableTree('VP', [('causes', 'NNS')]),\n",
       " ImmutableTree('VP', [('will', 'MD'), ('see', 'VB')]),\n",
       " ImmutableTree('VP', [('to', 'TO'), ('develop', 'VB')]),\n",
       " ImmutableTree('VP', [('do', 'VBP'), (\"n't\", 'RB'), ('want', 'VB'), ('to', 'TO'), ('sell', 'VB')]),\n",
       " ImmutableTree('VP', [('trying', 'VBG'), ('to', 'TO'), ('profit', 'VB')]),\n",
       " ImmutableTree('VP', [('to', 'TO'), ('head', 'VB')]),\n",
       " ImmutableTree('VP', [('are', 'VBP'), ('insisting', 'VBG')]),\n",
       " ImmutableTree('VP', [('seeking', 'VBG'), ('to', 'TO'), ('cut', 'VB')]),\n",
       " ImmutableTree('VP', [('accounted', 'VBD')]),\n",
       " ImmutableTree('VP', [('must', 'MD'), ('be', 'VB'), ('received', 'VBN')]),\n",
       " ImmutableTree('VP', [('should', 'MD'), ('begin', 'VB'), ('to', 'TO'), ('drop', 'VB')]),\n",
       " ImmutableTree('VP', [('can', 'MD'), ('force', 'VB')]),\n",
       " ImmutableTree('VP', [('were', 'VBD')]),\n",
       " ImmutableTree('VP', [('will', 'MD'), ('retire', 'VB')]),\n",
       " ImmutableTree('VP', [('to', 'TO'), ('yield', 'VB')]),\n",
       " ImmutableTree('VP', [('may', 'MD'), ('bring', 'VB')]),\n",
       " ImmutableTree('VP', [('will', 'MD'), ('assist', 'VB')])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).missed()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2174777772.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cp.evaluate(test_sents).incorrect()[:20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ImmutableTree('VP', [('reported', 'VBD'), ('operating', 'VBG')]),\n",
       " ImmutableTree('VP', [('done', 'VBN')]),\n",
       " ImmutableTree('VP', [('do', 'VBP')]),\n",
       " ImmutableTree('VP', [('did', 'VBD')]),\n",
       " ImmutableTree('VP', [('also', 'RB'), ('said', 'VBD')]),\n",
       " ImmutableTree('VP', [('continued', 'VBD')]),\n",
       " ImmutableTree('VP', [(\"'S\", 'VBZ')]),\n",
       " ImmutableTree('VP', [('developing', 'VBG')]),\n",
       " ImmutableTree('VP', [('increasing', 'VBG')]),\n",
       " ImmutableTree('VP', [('according', 'VBG')]),\n",
       " ImmutableTree('VP', [('escrowed', 'VBN')]),\n",
       " ImmutableTree('VP', [('openended', 'VBN')]),\n",
       " ImmutableTree('VP', [('also', 'RB'), ('posted', 'VBD')]),\n",
       " ImmutableTree('VP', [('holding', 'VBG')]),\n",
       " ImmutableTree('VP', [('once', 'RB'), ('heard', 'VBD'), ('arguing', 'VBG')]),\n",
       " ImmutableTree('VP', [('waiting', 'VBG')]),\n",
       " ImmutableTree('VP', [('also', 'RB'), ('reported', 'VBD')]),\n",
       " ImmutableTree('VP', [('helped', 'VBD')]),\n",
       " ImmutableTree('VP', [('attempt', 'VBP')]),\n",
       " ImmutableTree('VP', [('also', 'RB'), ('said', 'VBD')])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).incorrect()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\446699983.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.6%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt'[:100], chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The accuracy seems surprisingly high, but this just means that approximately 85% of the words are not in a `VP` chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. \n",
    "\n",
    "◑ Develop a chunker for one of the chunk types in the CoNLL corpus using a regular-expression based chunk grammar `RegexpChunk`. Use any combination of rules for chunking, chinking, merging or splitting.\n",
    "\n",
    "*I don't understand the difference between this question and question 3. The only thing that seems different is the inclusion of chinking, which seems less accurate than chunking. Also, we never discussed merging or splitting in this chapter, so I have no idea how to incorporate this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. \n",
    "\n",
    "◑ Sometimes a word is incorrectly tagged, e.g. the head noun in \"12/CD or/CC so/RB cases/VBZ\". Instead of requiring manual correction of tagger output, good chunkers are able to work with the erroneous output of taggers. Look for other examples of correctly chunked noun phrases with incorrect tags.\n",
    "\n",
    "*I eyeballed a number of example sentences, and it felt like looking for a proverbial needle in a haystack. Instead of searching through 10,000 hard-to-read sentences for a rather rare phenomenon, I decided to use a programmatic solution. I tried a number of approaches before I figured out how to use regular expressions to find subtrees labelled as `NP` that had verbs, but no nouns.  The code feels rather gnarly, so I left detailed comments:*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "misses = []\n",
    "\n",
    "for (i, sent) in enumerate(conll2000.chunked_sents('train.txt')):\n",
    "    for subtree in sent:\n",
    "        # only want subtrees, so use `try-except` to eliminate\n",
    "        # single nodes\n",
    "        try:\n",
    "            subtree.label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if subtree.label() == 'NP':\n",
    "                # set flag only for NPs\n",
    "                flag = True\n",
    "                # exclude subtrees that have nouns, pronouns,\n",
    "                # numbers, relative pronouns, etc...\n",
    "                for leaf in subtree.leaves():\n",
    "                    if re.match(r'NN|PRP|CD|WP|EX|DT', leaf[1]):\n",
    "                        flag = False\n",
    "\n",
    "        # the flag will only still be True if the subtree is a NP\n",
    "        # and doesn't have a noun, pronoun, etc...\n",
    "        # if it does have a verb, though, we'll want to \n",
    "        # inspect it\n",
    "        if flag == True and re.match(r'VB.*', leaf[1]):\n",
    "            # print the id and the subtree\n",
    "            misses.append( (i, subtree) )\n",
    "            # reset flag\n",
    "            flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at first ten instances:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293, Tree('NP', [('estimates', 'VBZ')]))\n",
      "(324, Tree('NP', [(\"'s\", 'POS'), ('holding', 'VBG')]))\n",
      "(498, Tree('NP', [('operating', 'VBG'), ('results', 'VBZ')]))\n",
      "(524, Tree('NP', [(\"'s\", 'POS'), ('backing', 'VBG')]))\n",
      "(587, Tree('NP', [('offers', 'VBZ')]))\n",
      "(611, Tree('NP', [(\"'s\", 'VBZ')]))\n",
      "(725, Tree('NP', [(\"'s\", 'POS'), ('standing', 'VBG')]))\n",
      "(827, Tree('NP', [('around', 'IN'), ('$', '$'), ('5', 'VBG')]))\n",
      "(867, Tree('NP', [('trading', 'VBG')]))\n",
      "(876, Tree('NP', [('employees', 'VBZ')]))\n"
     ]
    }
   ],
   "source": [
    "for m in misses[:10]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at the sentence with the first instance:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP As/IN)\n",
      "  usual/JJ\n",
      "  ,/,\n",
      "  (NP estimates/VBZ)\n",
      "  (PP on/IN)\n",
      "  (NP the/DT fickle/JJ report/NN)\n",
      "  (VP are/VBP)\n",
      "  wide/JJ\n",
      "  ,/,\n",
      "  (VP running/VBG)\n",
      "  (PP from/IN)\n",
      "  (NP a/DT drop/NN)\n",
      "  (PP of/IN)\n",
      "  (NP 3.5/CD %/NN)\n",
      "  (PP to/TO)\n",
      "  (NP a/DT gain/NN)\n",
      "  (PP of/IN)\n",
      "  (NP 1.6/CD %/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt')[293])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.\n",
    "\n",
    "◑ The bigram chunker scores about 90% accuracy. Study its errors and try to work out why it doesn't get 100% accuracy. Experiment with trigram chunking. Are you able to improve the performance any more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\1800826511.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(bigram_chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.3%%\n",
      "    Precision:     81.2%%\n",
      "    Recall:        86.2%%\n",
      "    F-Measure:     83.6%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt')\n",
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2495054575.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cp.evaluate(test_sents).missed()[:20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ImmutableTree('VP', [('having', 'VBG')]),\n",
       " ImmutableTree('NP', [('that', 'WDT')]),\n",
       " ImmutableTree('NP', [('contrast', 'NN')]),\n",
       " ImmutableTree('NP', [('the', 'DT'), ('elephant', 'NN')]),\n",
       " ImmutableTree('VP', [('will', 'MD'), ('see', 'VB')]),\n",
       " ImmutableTree('NP', [('every', 'DT'), ('night', 'NN')]),\n",
       " ImmutableTree('NP', [('A-2', 'NN'), ('senior', 'JJ'), ('notes', 'NNS'), ('and', 'CC'), ('bonds', 'NNS')]),\n",
       " ImmutableTree('NP', [(\"'s\", 'POS'), ('cost', 'NN')]),\n",
       " ImmutableTree('NP', [('The', 'DT'), ('three', 'CD'), ('most', 'RBS'), ('important', 'JJ'), ('things', 'NNS')]),\n",
       " ImmutableTree('NP', [('which', 'WDT')]),\n",
       " ImmutableTree('NP', [(\"'s\", 'POS'), ('close', 'NN')]),\n",
       " ImmutableTree('PP', [('in', 'IN')]),\n",
       " ImmutableTree('VP', [('overwhelmed', 'VBD')]),\n",
       " ImmutableTree('NP', [('the', 'DT'), ('growing', 'VBG'), ('confidence', 'NN')]),\n",
       " ImmutableTree('PP', [('in', 'IN')]),\n",
       " ImmutableTree('NP', [('3\\\\/4', 'CD')]),\n",
       " ImmutableTree('PP', [('In', 'IN')]),\n",
       " ImmutableTree('NP', [('who', 'WP')]),\n",
       " ImmutableTree('NP', [('the', 'DT'), ('rumor', 'NN')]),\n",
       " ImmutableTree('PP', [('of', 'IN')])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).missed()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\2174777772.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cp.evaluate(test_sents).incorrect()[:20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).incorrect()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at the first 20 incorrect/missed, it's not very easy to see how we could improve the accuracy.*\n",
    "\n",
    "*As for setting up a trigram chunker, it's very easy.  However, the trigram chunker is somewhat less accurate than the bigram chunker:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = nltk.TrigramTagger(train_data)\n",
    "        \n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\3088361561.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(trigram_chunker.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     81.0%%\n",
      "    Recall:        84.4%%\n",
      "    F-Measure:     82.6%%\n"
     ]
    }
   ],
   "source": [
    "trigram_chunker = TrigramChunker(train_sents)\n",
    "print(trigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. \n",
    "\n",
    "★ Apply the n-gram and Brill tagging methods to IOB chunk tagging. Instead of assigning POS tags to words, here we will assign IOB tags to the POS tags. E.g. if the tag `DT` (determiner) often occurs at the start of a chunk, it will be tagged `B` (begin). Evaluate the performance of these chunking methods relative to the regular expression chunking methods covered in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Modified from the code supplied as an [answer to a question from this SO discussion](https://stackoverflow.com/a/40508641/9097207 \"code from Brill Trainer API author\"). This code was supplied by the author of NLTK's Brill Trainer api. I frankly don't think I could craft code like this from scratch at the moment, as the NLTK didn't spent that much time on the Brill Trainer, and some of the things they considered are no longer working.  In his SO response, the author of the Brill Trainer did give a quite lengthy answer, but I felt he could have gone into a little more detail regarding the inner workings of his code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'word': 'Confidence', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'in', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'the', 'pos': 'DT', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'pound', 'pos': 'NN', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'is', 'pos': 'VBZ', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'widely', 'pos': 'RB', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'expected', 'pos': 'VBN', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'take', 'pos': 'VB', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'another', 'pos': 'DT', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'sharp', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'dive', 'pos': 'NN', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'if', 'pos': 'IN', 'trueiob': 'O'}, 'O'), ({'word': 'trade', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'figures', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'September', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'due', 'pos': 'JJ', 'trueiob': 'O'}, 'O'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'release', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'tomorrow', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'fail', 'pos': 'VB', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'show', 'pos': 'VB', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'a', 'pos': 'DT', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'substantial', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'improvement', 'pos': 'NN', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'from', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'July', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'and', 'pos': 'CC', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'August', 'pos': 'NNP', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': \"'s\", 'pos': 'POS', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'near-record', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'deficits', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')] \n",
      "\n",
      "[({'word': 'Confidence', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': 'in', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'the', 'pos': 'DT', 'trueiob': 'B-NP'}, 'O'), ({'word': 'pound', 'pos': 'NN', 'trueiob': 'I-NP'}, 'O'), ({'word': 'is', 'pos': 'VBZ', 'trueiob': 'B-VP'}, 'O'), ({'word': 'widely', 'pos': 'RB', 'trueiob': 'I-VP'}, 'O'), ({'word': 'expected', 'pos': 'VBN', 'trueiob': 'I-VP'}, 'O'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'O'), ({'word': 'take', 'pos': 'VB', 'trueiob': 'I-VP'}, 'O'), ({'word': 'another', 'pos': 'DT', 'trueiob': 'B-NP'}, 'O'), ({'word': 'sharp', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'O'), ({'word': 'dive', 'pos': 'NN', 'trueiob': 'I-NP'}, 'O'), ({'word': 'if', 'pos': 'IN', 'trueiob': 'O'}, 'O'), ({'word': 'trade', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': 'figures', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'O'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'September', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'O'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'due', 'pos': 'JJ', 'trueiob': 'O'}, 'O'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'release', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': 'tomorrow', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'fail', 'pos': 'VB', 'trueiob': 'B-VP'}, 'O'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'O'), ({'word': 'show', 'pos': 'VB', 'trueiob': 'I-VP'}, 'O'), ({'word': 'a', 'pos': 'DT', 'trueiob': 'B-NP'}, 'O'), ({'word': 'substantial', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'O'), ({'word': 'improvement', 'pos': 'NN', 'trueiob': 'I-NP'}, 'O'), ({'word': 'from', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'July', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'O'), ({'word': 'and', 'pos': 'CC', 'trueiob': 'I-NP'}, 'O'), ({'word': 'August', 'pos': 'NNP', 'trueiob': 'I-NP'}, 'O'), ({'word': \"'s\", 'pos': 'POS', 'trueiob': 'B-NP'}, 'O'), ({'word': 'near-record', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'O'), ({'word': 'deficits', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'O'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')] \n",
      "\n",
      "TBL train (fast) (seqs: 100; tokens: 2440; tpls: 50; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 15137 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      " 288 290   2  74  | O->I-NP if POS:NN@[0] & IOB:O@[0]\n",
      " 207 221  14  22  | O->B-NP if POS:IN@[-1] & IOB:O@[0]\n",
      " 169 218  49  10  | O->B-PP if POS:IN@[0] & IOB:O@[0]\n",
      " 145 146   1  53  | O->I-NP if POS:NNP@[0] & IOB:O@[0]\n",
      " 106 106   0  29  | O->I-NP if POS:NNS@[0] & IOB:O@[0]\n",
      " 101 104   3   7  | O->B-NP if POS:DT@[0] & IOB:O@[0]\n",
      "  75  75   0   2  | O->B-VP if POS:VBD@[0] & IOB:O@[0]\n",
      "  72  72   0  13  | O->I-VP if POS:VB@[0] & IOB:O@[0]\n",
      "  67  81  14  46  | O->I-NP if POS:NN@[1,2,3] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "  39  39   0   0  | O->B-VP if POS:VBP@[0] & IOB:O@[0]\n",
      "  39  39   0   0  | O->B-VP if POS:VBZ@[0] & IOB:O@[0]\n",
      "  34  34   0   0  | I-NP->B-NP if POS:IN@[-1] & IOB:I-NP@[0]\n",
      "  32  36   4   7  | O->I-NP if POS:CD@[0] & IOB:O@[0]\n",
      "  30  30   0   1  | O->B-NP if POS:PRP@[0] & IOB:O@[0]\n",
      "  29  30   1  13  | O->I-VP if POS:VBN@[0] & IOB:O@[0]\n",
      "  29  39  10  34  | O->B-VP if POS:VB@[1,2,3] & IOB:O@[0] & IOB:I-VP@[1]\n",
      "  22  23   1   0  | I-NP->B-NP if POS:VB@[-1] & IOB:I-NP@[0]\n",
      "  19  19   0  16  | O->I-NP if POS:JJ@[0] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "  20  22   2   0  | I-NP->B-NP if POS:VBD@[-1] & IOB:I-NP@[0]\n",
      "  18  18   0   5  | O->B-PP if POS:TO@[0] & IOB:O@[0]\n",
      "  14  14   0  18  | O->B-VP if POS:VBG@[0] & IOB:O@[0]\n",
      "  18  18   0   0  | B-VP->I-VP if POS:VB@[1,2,3] & IOB:B-VP@[-1] & IOB:B-VP@[0]\n",
      "  12  19   7   0  | I-NP->B-NP if POS:,@[-1] & IOB:I-NP@[0]\n",
      "   9  10   1   3  | B-NP->B-PP if POS:IN@[0] & IOB:B-NP@[0]\n",
      "   9   9   0   0  | I-NP->B-NP if POS:POS@[0] & IOB:I-NP@[0]\n",
      "   9  10   1   1  | B-VP->I-VP if POS:TO@[0] & IOB:I-VP@[-1] & IOB:B-VP@[0]\n",
      "   7   8   1   0  | I-NP->B-NP if POS:TO@[-1] & IOB:I-NP@[0]\n",
      "   7   8   1   0  | O->I-NP if POS:DT@[-1] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   7   8   1   0  | B-PP->O if POS:PRP@[1] & IOB:B-PP@[0]\n",
      "   7   7   0   0  | O->B-NP if POS:NNS@[1,2,3] & IOB:I-VP@[-1] & IOB:O@[0]\n",
      "   6   6   0   0  | B-VP->I-VP if POS:RB@[0] & IOB:B-VP@[0]\n",
      "   6   6   0   0  | O->B-VP if POS:MD@[0] & IOB:O@[0]\n",
      "   6   6   0   0  | I-NP->B-NP if POS:VBP@[1,2,3] & IOB:O@[-1] & IOB:I-NP@[0]\n",
      "   5   5   0   0  | B-VP->O if POS:CC@[0] & IOB:B-VP@[0]\n",
      "   6   6   0   1  | I-VP->B-VP if POS:NN@[-3,-2,-1] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   5   5   0   0  | B-VP->I-VP if POS:VBG@[0] & IOB:B-VP@[-1] & IOB:B-VP@[0]\n",
      "   5   5   0   0  | I-VP->B-VP if POS:VBN@[0] & IOB:I-NP@[-1] & IOB:I-VP@[0]\n",
      "   5   9   4   1  | B-PP->O if POS:IN@[0] & IOB:B-PP@[0] & IOB:B-PP@[1]\n",
      "   5   5   0   1  | I-NP->B-NP if POS:VBZ@[-1] & IOB:I-NP@[0]\n",
      "   5   6   1   0  | I-NP->B-NP if POS:JJ@[0] & POS:NNP@[1] & IOB:I-NP@[0]\n",
      "   4   7   3   0  | I-NP->O if POS:,@[0] & IOB:I-NP@[0]\n",
      "   4   4   0   0  | O->B-NP if POS:EX@[0] & IOB:O@[0]\n",
      "   4   4   0   1  | O->B-NP if POS:POS@[0] & IOB:O@[0]\n",
      "   4   4   0   0  | O->B-NP if POS:WDT@[0] & IOB:O@[0]\n",
      "   4   5   1   1  | O->B-NP if POS:TO@[-1] & IOB:O@[0]\n",
      "   4   4   0   0  | O->I-NP if POS:RB@[-1] & IOB:B-NP@[-1] & IOB:O@[0]\n",
      "   4   4   0   1  | I-NP->B-NP if POS:NNS@[-3,-2,-1] & IOB:B-VP@[-1] &\n",
      "                  |   IOB:I-NP@[0]\n",
      "   4   5   1   0  | O->I-NP if POS:NNP@[-3,-2,-1] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   5   7   2   0  | I-NP->B-NP if POS:CC@[-1] & IOB:O@[-1] & IOB:I-NP@[0]\n",
      "   3   3   0   0  | B-NP->O if POS:,@[0] & IOB:B-NP@[0]\n",
      "   3   3   0   0  | B-VP->O if POS:,@[0] & IOB:B-VP@[0]\n",
      "   4   4   0   0  | I-VP->B-VP if POS:VB@[0] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   3   3   0   0  | O->B-NP if POS:WP@[0] & IOB:O@[0]\n",
      "   3   3   0   0  | O->I-NP if POS:NNPS@[0] & IOB:O@[0]\n",
      "   3   3   0   0  | O->B-NP if POS:JJR@[0] & IOB:B-VP@[-1] & IOB:O@[0]\n",
      "   3   3   0   0  | B-PP->O if POS:IN@[0] & IOB:B-PP@[0] & IOB:O@[1]\n",
      "   3   3   0   1  | B-VP->I-VP if POS:RB@[-1] & IOB:B-VP@[-2] & IOB:O@[-1]\n",
      "   3   3   0   0  | O->I-VP if POS:VBG@[1] & IOB:O@[0] & IOB:I-VP@[1]\n",
      "   3   3   0   0  | I-NP->B-NP if POS:VBP@[-2] & POS:VBN@[-1] & IOB:I-NP@[0]\n",
      "   3   3   0   0  | I-NP->B-NP if POS:NNS@[1] & POS:VBG@[2] & IOB:I-NP@[0]\n",
      "   3   3   0   0  | I-NP->O if POS:(@[1,2,3] & IOB:I-NP@[0]\n",
      "   3   3   0   1  | B-VP->I-NP if POS:NNS@[1,2,3] & IOB:B-VP@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   1  | B-NP->O if POS:JJS@[0] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-VP->O if POS:``@[0] & IOB:B-VP@[0]\n",
      "   2   2   0   0  | I-NP->B-VP if POS:VBP@[0] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | I-NP->B-VP if POS:VBZ@[0] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | O->B-NP if POS:PRP$@[0] & IOB:O@[0]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:NN@[0] & IOB:B-VP@[-1] & IOB:I-NP@[0]\n",
      "   2   2   0   1  | I-VP->B-PP if POS:VBN@[0] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   2   2   0   0  | I-VP->B-VP if POS:VB@[0] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   2   2   0   0  | I-VP->B-VP if POS:VBN@[0] & IOB:B-NP@[-1] & IOB:I-VP@[0]\n",
      "   2   3   1   1  | B-NP->O if POS:DT@[0] & IOB:B-NP@[0] & IOB:O@[1]\n",
      "   3   3   0   0  | O->I-NP if POS:JJ@[-1] & POS:JJ@[1] & IOB:O@[0]\n",
      "   3   3   0   0  | O->I-NP if POS:IN@[-3,-2,-1] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   0  | O->B-NP if POS:RBR@[0] & IOB:O@[0] & IOB:B-NP@[1]\n",
      "   3   3   0   0  | B-NP->I-NP if POS:DT@[0] & IOB:B-NP@[-1] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-NP->B-VP if POS:VBG@[0] & IOB:I-NP@[-2] & IOB:B-PP@[-1]\n",
      "   2   3   1   0  | B-PP->O if POS:VBZ@[-1] & IOB:B-PP@[0]\n",
      "   2   3   1   0  | I-NP->B-NP if POS:``@[-1] & IOB:I-NP@[0]\n",
      "   2   2   0   1  | I-NP->B-NP if POS:VBG@[-1] & IOB:B-VP@[-1] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | I-VP->B-VP if POS:VBN@[-1] & IOB:B-VP@[-1] & IOB:I-VP@[0]\n",
      "   2   2   0   0  | B-PP->B-NP if POS:CC@[-1] & IOB:O@[-2] & IOB:O@[-1]\n",
      "   2   2   0   0  | B-PP->O if POS:JJS@[1] & IOB:B-PP@[0]\n",
      "   2   2   0   0  | O->B-PP if POS:IN@[1] & IOB:B-PP@[-2] & IOB:B-NP@[-1]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:NN@[-1] & POS:NNP@[0] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | O->B-NP if POS:JJ@[0] & POS:DT@[1] & IOB:O@[0]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:JJ@[0] & POS:NN@[1] & IOB:I-NP@[-1] &\n",
      "                  |   IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-PP->I-NP if POS:JJR@[-1] & POS:CD@[1] & IOB:B-PP@[0]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:CD@[0] & IOB:I-NP@[-1] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-PP->O if POS:VBD@[-1] & POS:DT@[1] & IOB:I-NP@[-2] &\n",
      "                  |   IOB:B-VP@[-1]\n",
      "   2   2   0   0  | B-NP->O if POS:TO@[1] & POS:DT@[2] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-PP->O if POS:RB@[-1] & IOB:B-PP@[0] & IOB:O@[1]\n",
      "   2   2   0   0  | B-PP->O if POS:NNS@[1] & POS:MD@[2] & IOB:B-PP@[0]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:NNP@[1] & POS:RB@[2] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | I-NP->O if POS:(@[-3,-2,-1] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:,@[-3,-2,-1] & IOB:O@[-2] & IOB:B-NP@[-1]\n",
      "   2   2   0   0  | B-PP->O if POS:VBZ@[1,2,3] & IOB:O@[-1] & IOB:B-PP@[0]\n",
      "   2   2   0   1  | O->B-NP if POS:CD@[1,2,3] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:IN@[1] & IOB:O@[-2] & IOB:I-NP@[-1]\n",
      "   2   2   0   0  | I-NP->O if POS:CC@[0] & IOB:I-NP@[0] & IOB:B-NP@[1]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:CD@[1] & POS:CD@[2] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | O->I-NP if POS:CC@[1,2,3] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   0  | B-PP->O if POS:DT@[1,2,3] & IOB:O@[-2] & IOB:B-NP@[-1]\n",
      "   2   2   0   0  | B-PP->O if POS:IN@[1,2,3] & IOB:I-VP@[-2] & IOB:I-VP@[-1]\n",
      "   2   2   0   0  | O->B-NP if POS:CD@[1,2,3] & IOB:I-NP@[-2] & IOB:B-VP@[-1]\n",
      "   2   4   2   0  | I-NP->B-NP if POS:DT@[1,2,3] & IOB:I-NP@[1] & IOB:B-VP@[2]\n",
      "   2   3   1   0  | I-NP->O if POS:NN@[1] & IOB:I-NP@[0] & IOB:B-NP@[1]\n",
      "[({'word': 'He', 'pos': 'PRP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'talked', 'pos': 'VBD', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'about', 'pos': 'IN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': '20', 'pos': 'CD', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'minutes', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')]\n",
      "[({'word': 'He', 'pos': 'PRP', 'trueiob': 'B-NP'}, 'O'), ({'word': 'talked', 'pos': 'VBD', 'trueiob': 'B-VP'}, 'O'), ({'word': 'about', 'pos': 'IN', 'trueiob': 'B-NP'}, 'O'), ({'word': '20', 'pos': 'CD', 'trueiob': 'I-NP'}, 'O'), ({'word': 'minutes', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'O'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')]\n",
      "[({'word': 'He', 'pos': 'PRP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'talked', 'pos': 'VBD', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'about', 'pos': 'IN', 'trueiob': 'B-NP'}, 'B-PP'), ({'word': '20', 'pos': 'CD', 'trueiob': 'I-NP'}, 'B-NP'), ({'word': 'minutes', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')]\n",
      "\n",
      "TEMPLATE STATISTICS (TRAIN)  27 templates, 107 rules)\n",
      "TRAIN (   2440 tokens) initial  2018 0.1730 final:   154 0.9369\n",
      "#ID | Score (train) |  #Rules     | Template\n",
      "--------------------------------------------\n",
      "000 |  1230   0.660 |  32   0.299 | Template(POS([0]),IOB([0]))\n",
      "005 |   315   0.169 |  10   0.093 | Template(POS([-1]),IOB([0]))\n",
      "047 |   103   0.055 |   5   0.047 | Template(POS([1, 2, 3]),IOB([0]),IOB([1]))\n",
      "001 |    39   0.021 |  11   0.103 | Template(POS([0]),IOB([-1]),IOB([0]))\n",
      "046 |    33   0.018 |   4   0.037 | Template(POS([1, 2, 3]),IOB([-1]),IOB([0]))\n",
      "002 |    33   0.018 |   6   0.056 | Template(POS([0]),IOB([0]),IOB([1]))\n",
      "006 |    13   0.007 |   4   0.037 | Template(POS([-1]),IOB([-1]),IOB([0]))\n",
      "035 |    11   0.006 |   5   0.047 | Template(POS([1]),POS([2]),IOB([0]))\n",
      "041 |    10   0.005 |   2   0.019 | Template(POS([-3, -2, -1]),IOB([-1]),IOB([0]))\n",
      "010 |     9   0.005 |   2   0.019 | Template(POS([1]),IOB([0]))\n",
      "007 |     9   0.005 |   2   0.019 | Template(POS([-1]),IOB([0]),IOB([1]))\n",
      "042 |     7   0.004 |   2   0.019 | Template(POS([-3, -2, -1]),IOB([0]),IOB([1]))\n",
      "020 |     7   0.004 |   2   0.019 | Template(POS([0]),POS([1]),IOB([0]))\n",
      "048 |     6   0.003 |   3   0.028 | Template(POS([1, 2, 3]),IOB([-2]),IOB([-1]))\n",
      "025 |     5   0.003 |   2   0.019 | Template(POS([-1]),POS([1]),IOB([0]))\n",
      "012 |     5   0.003 |   2   0.019 | Template(POS([1]),IOB([0]),IOB([1]))\n",
      "008 |     5   0.003 |   2   0.019 | Template(POS([-1]),IOB([-2]),IOB([-1]))\n",
      "013 |     4   0.002 |   2   0.019 | Template(POS([1]),IOB([-2]),IOB([-1]))\n",
      "045 |     3   0.002 |   1   0.009 | Template(POS([1, 2, 3]),IOB([0]))\n",
      "030 |     3   0.002 |   1   0.009 | Template(POS([-2]),POS([-1]),IOB([0]))\n",
      "049 |     2   0.001 |   1   0.009 | Template(POS([1, 2, 3]),IOB([1]),IOB([2]))\n",
      "043 |     2   0.001 |   1   0.009 | Template(POS([-3, -2, -1]),IOB([-2]),IOB([-1]))\n",
      "040 |     2   0.001 |   1   0.009 | Template(POS([-3, -2, -1]),IOB([0]))\n",
      "028 |     2   0.001 |   1   0.009 | Template(POS([-1]),POS([1]),IOB([-2]),IOB([-1]))\n",
      "021 |     2   0.001 |   1   0.009 | Template(POS([0]),POS([1]),IOB([-1]),IOB([0]))\n",
      "015 |     2   0.001 |   1   0.009 | Template(POS([-1]),POS([0]),IOB([0]))\n",
      "003 |     2   0.001 |   1   0.009 | Template(POS([0]),IOB([-2]),IOB([-1]))\n",
      "\n",
      "UNUSED TEMPLATES (23)\n",
      "004 Template(POS([0]),IOB([1]),IOB([2]))\n",
      "009 Template(POS([-1]),IOB([1]),IOB([2]))\n",
      "011 Template(POS([1]),IOB([-1]),IOB([0]))\n",
      "014 Template(POS([1]),IOB([1]),IOB([2]))\n",
      "016 Template(POS([-1]),POS([0]),IOB([-1]),IOB([0]))\n",
      "017 Template(POS([-1]),POS([0]),IOB([0]),IOB([1]))\n",
      "018 Template(POS([-1]),POS([0]),IOB([-2]),IOB([-1]))\n",
      "019 Template(POS([-1]),POS([0]),IOB([1]),IOB([2]))\n",
      "022 Template(POS([0]),POS([1]),IOB([0]),IOB([1]))\n",
      "023 Template(POS([0]),POS([1]),IOB([-2]),IOB([-1]))\n",
      "024 Template(POS([0]),POS([1]),IOB([1]),IOB([2]))\n",
      "026 Template(POS([-1]),POS([1]),IOB([-1]),IOB([0]))\n",
      "027 Template(POS([-1]),POS([1]),IOB([0]),IOB([1]))\n",
      "029 Template(POS([-1]),POS([1]),IOB([1]),IOB([2]))\n",
      "031 Template(POS([-2]),POS([-1]),IOB([-1]),IOB([0]))\n",
      "032 Template(POS([-2]),POS([-1]),IOB([0]),IOB([1]))\n",
      "033 Template(POS([-2]),POS([-1]),IOB([-2]),IOB([-1]))\n",
      "034 Template(POS([-2]),POS([-1]),IOB([1]),IOB([2]))\n",
      "036 Template(POS([1]),POS([2]),IOB([-1]),IOB([0]))\n",
      "037 Template(POS([1]),POS([2]),IOB([0]),IOB([1]))\n",
      "038 Template(POS([1]),POS([2]),IOB([-2]),IOB([-1]))\n",
      "039 Template(POS([1]),POS([2]),IOB([1]),IOB([2]))\n",
      "044 Template(POS([-3, -2, -1]),IOB([1]),IOB([2]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "\n",
    "from nltk import tbl, untag\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk.util import tree2conlltags\n",
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "def get_templates():\n",
    "    \n",
    "    pos10 = [[POS([0])],\n",
    "             [POS([-1])],\n",
    "             [POS([1])],\n",
    "             [POS([-1]), POS([0])],\n",
    "             [POS([0]), POS([1])],\n",
    "             [POS([-1]), POS([1])],\n",
    "             [POS([-2]), POS([-1])],\n",
    "             [POS([1]), POS([2])],\n",
    "             [POS([-1, -2, -3])],\n",
    "             [POS([1, 2, 3])]]\n",
    "    \n",
    "    iobs5 = [[IOB([0])],\n",
    "             [IOB([-1]), IOB([0])],\n",
    "             [IOB([0]), IOB([1])],\n",
    "             [IOB([-2]), IOB([-1])],\n",
    "             [IOB([1]), IOB([2])]]\n",
    "    \n",
    "    templates = [tbl.Template(*pos + iob) for pos in pos10 for iob in iobs5]\n",
    "    \n",
    "    return templates\n",
    "\n",
    "def build_multifeature_corpus():\n",
    "    \n",
    "    def tuple2dict_featureset(sent, tagnames = (\"word\", \"pos\", \"trueiob\")):\n",
    "        return (dict(zip(tagnames, t)) for t in sent)\n",
    "    \n",
    "    def tag_tokens(tokens):\n",
    "        return [(t, t[\"trueiob\"]) for t in tokens]\n",
    "    \n",
    "    train_sents = conll2000.chunked_sents('train.txt')\n",
    "    conlltagged_sents = (tree2conlltags(sent)\n",
    "                        for sent in train_sents)\n",
    "    conlltagged_tokens = (tuple2dict_featureset(sent) \n",
    "                        for sent in conlltagged_sents)\n",
    "    conlltagged_sequences = (tag_tokens(sent)\n",
    "                            for sent in conlltagged_tokens)\n",
    "    \n",
    "    return conlltagged_sequences\n",
    "\n",
    "class POS(tbl.Feature):\n",
    "    @staticmethod\n",
    "    def extract_property(tokens, index):\n",
    "        return tokens[index][0][\"pos\"]\n",
    "\n",
    "class IOB(tbl.Feature):\n",
    "    @staticmethod\n",
    "    def extract_property(tokens, index):\n",
    "        return tokens[index][1]\n",
    "    \n",
    "\n",
    "    \n",
    "class MyInitialTagger(DefaultTagger):\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        tokens_ = [t[\"word\"] for t in tokens]\n",
    "        return super().choose_tag(tokens_, index, history)\n",
    "    \n",
    "    \n",
    "templates = get_templates()\n",
    "trainon = 100\n",
    "\n",
    "corpus = list(build_multifeature_corpus())\n",
    "train, test = corpus[:trainon], corpus[trainon:]\n",
    "\n",
    "print(train[0], \"\\n\")\n",
    "\n",
    "initial_tagger = MyInitialTagger('O')\n",
    "print(initial_tagger.tag(untag(train[0])), \"\\n\")\n",
    "\n",
    "trainer = BrillTaggerTrainer(initial_tagger, templates, trace = 3)\n",
    "tagger = trainer.train(train)\n",
    "\n",
    "taggedtest = tagger.tag_sents([untag(t) for t in test])\n",
    "print(test[0])\n",
    "print(initial_tagger.tag(untag(test[0])))\n",
    "print(taggedtest[0])\n",
    "print()\n",
    "\n",
    "tagger.print_template_statistics()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I did examine the rules and played around with new chunkers to see if I could make new parsers with increased accuracy, but unfortunately I wasn't able to get anything that performed any better than the parsers we've been using.  I suspect there are two reason for this: one is that Brill Trainer examines the POS and IOB tags of previous and subsequent words, but we're not able to use this information when we hand craft our parsers. In addition, it's not so simple to tease the rules out of the Brill Trainer's output, as previous rules are often superceded by future rules.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12.\n",
    "\n",
    "★ We saw in [5](https://www.nltk.org/book/ch05.html#chap-tag \"Chapter 5\") that it is possible to establish an upper limit to tagging performance by looking for ambiguous n-grams, n-grams that are tagged in more than one possible way in the training data. Apply the same method to determine an upper bound on the performance of an n-gram chunker.\n",
    "\n",
    "*Looking at the IOB tags of the previous two words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35955948727207077"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[2], y[2], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at the POS tags of the previous two words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22214427565573983"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], y[1], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at both the IOB and POS tags of the previous two words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15939442395481393"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], x[2], y[1], y[2], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at just the IOB tag of the previous word:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43632113851206417"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[2], y[0]), y[2])\n",
    "           for sent in train_sents\n",
    "           for x, y in nltk.bigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at just the POS tag of the previous word:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3850516048542588"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], y[0]), y[2])\n",
    "           for sent in train_sents\n",
    "           for x, y in nltk.bigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Looking at the POS and IOB tag of the previous word:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27115601777199183"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], x[2], y[0]), y[2])\n",
    "           for sent in train_sents\n",
    "           for x, y in nltk.bigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From these experiments we can see that there is quite a bit of ambiguity with IOB tags.  Looking at the tags of the previous two words is more unambiguous than looking at just the tag of the last word, and looking at both the POS and IOB tags is more unambiguous than looking at either just the POS or IOB tags. As might be expected, ambiguity is very low if we look at __the previous words and their tags__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005591519872075929"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[0], x[1], x[2], y[0], y[1], y[2], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. \n",
    "\n",
    "★ Pick one of the three chunk types in the CoNLL corpus. Write functions to do the following tasks for your chosen type:\n",
    "\n",
    "* a. List all the tag sequences that occur with each instance of this chunk type.\n",
    "\n",
    "* b. Count the frequency of each tag sequence, and produce a ranked list in order of decreasing frequency; each line should consist of an integer (the frequency) and the tag sequence.\n",
    "\n",
    "* c. Inspect the high-frequency tag sequences. Use these as the basis for developing a better chunker.\n",
    "\n",
    "*__a.__ I'll focus on `NP`s.  Below is code similar to what I used in exercise 9 to find `NP`s within the sentences. I'll extract the POS tags from the words in `NP`s and store their values inside concatenated strings.  This will make things easier later when I use `set()` to get the number of unique tag sequences.  My initial inclination was to use lists of tags instead of strings, but `set()` won't work on nested lists.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_tags = []\n",
    "\n",
    "for (i, sent) in enumerate(conll2000.chunked_sents('train.txt')):\n",
    "    for subtree in sent:\n",
    "        # only want subtrees, so use `try-except` to eliminate\n",
    "        # single nodes\n",
    "        try:\n",
    "            subtree.label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if subtree.label() == 'NP':\n",
    "                # concatenating a string with all the POS tags\n",
    "                subtree_tag = \"\"\n",
    "                for t in tree2conlltags(subtree):\n",
    "                    if subtree_tag == \"\":\n",
    "                        subtree_tag += t[1]\n",
    "                    else:\n",
    "                        subtree_tag += \"/\" + t[1]\n",
    "                np_tags.append(subtree_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2283"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(np_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__ With 2283 distinct POS-tag combinations, I'm obviously not going to print all of them out.  With instructions like these, I often wonder if the authors attempted these exercises themselves before assigning them... For the sake of brevity, I'll just print out the first 200, but it would be a trivial exercise to print all of the tags:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7223 DT/NN\n",
      "3802 PRP\n",
      "3282 NNS\n",
      "3249 NNP\n",
      "3245 NN\n",
      "2642 NNP/NNP\n",
      "2119 DT/JJ/NN\n",
      "1722 JJ/NNS\n",
      "1173 DT/NNS\n",
      "1143 JJ/NN\n",
      "1012 NN/NNS\n",
      " 930 WDT\n",
      " 921 DT/NN/NN\n",
      " 866 CD\n",
      " 830 CD/NN\n",
      " 824 $/CD/CD\n",
      " 690 CD/NNS\n",
      " 677 NNP/NNP/NNP\n",
      " 624 PRP$/NN\n",
      " 552 POS/NN\n",
      " 540 DT\n",
      " 509 WP\n",
      " 463 DT/NNP\n",
      " 454 NN/NN\n",
      " 446 $/CD\n",
      " 399 DT/NNP/NN\n",
      " 355 PRP$/NNS\n",
      " 313 JJ/NN/NNS\n",
      " 311 DT/NNP/NNP\n",
      " 277 DT/JJ/NN/NN\n",
      " 276 DT/JJ/NNS\n",
      " 220 NNP/NNP/NNP/NNP\n",
      " 204 POS/NNS\n",
      " 200 CD/CD\n",
      " 195 PRP$/JJ/NN\n",
      " 189 EX\n",
      " 183 NNP/CD\n",
      " 183 NNP/NNS\n",
      " 182 DT/JJ/JJ/NN\n",
      " 171 POS/JJ/NN\n",
      " 161 DT/NN/NNS\n",
      " 158 DT/NNP/NNP/NNP\n",
      " 149 JJ/JJ/NN\n",
      " 146 JJ/JJ/NNS\n",
      " 141 DT/VBN/NN\n",
      " 124 DT/CD/NNS\n",
      " 124 IN\n",
      " 115 NNP/NN\n",
      " 113 NNP/CC/NNP\n",
      " 113 POS/NN/NN\n"
     ]
    }
   ],
   "source": [
    "fd = nltk.FreqDist(np_tags)\n",
    "for tag, value in fd.most_common(50):\n",
    "    print(\"{:>4} {}\".format(value, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_23296\\3165305913.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(cp.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.6%%\n",
      "    Precision:     71.9%%\n",
      "    Recall:        74.1%%\n",
      "    F-Measure:     73.0%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PRP$|POS|$|WP|EX>?<JJ.*>*<CD>*<NN.*|WDT>*<CD>*}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt'[:100], chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By looking at the most common tag sequences, it is possible to generate regexes that will cover most occurrences.  However, if we try to cover every possible occurrence, we increase the number of false positives returned, which will drive down the accuracy metrics.  For example, it's possible that prepositions or verbs will occur within noun phrases.  However, including tags for these POS will drive down the accuracy considerably.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I'd like to finish this book before the end of the year, so to reach that goal I've decided to postpone working on some of the more difficult questions until I've finished the rest of the book.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "★ The baseline chunker presented in the evaluation section tends to create larger chunks than it should. For example, the phrase: `[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]` contains two consecutive chunks, and our baseline chunker will incorrectly combine the first two: `[every/DT time/NN she/PRP]`. Write a program that finds which of these chunk-internal tags typically occur at the start of a chunk, then devise one or more rules that will split up these chunks. Combine these with the existing baseline chunker and re-evaluate it, to see if you have discovered an improved baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15.\n",
    "\n",
    "★ Develop an `NP` chunker that converts POS-tagged text into a list of tuples, where each tuple consists of a verb followed by a sequence of noun phrases and prepositions, e.g. `the little cat sat on the mat` becomes (`'sat'`, `'on'`, `'NP'`)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16.\n",
    "\n",
    "★ The Penn Treebank contains a section of tagged Wall Street Journal text that has been chunked into noun phrases. The format uses square brackets, and we have encountered it several times during this chapter. The Treebank corpus can be accessed using: `for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid)`. These are flat trees, just as we got using `nltk.corpus.conll2000.chunked_sents()`.\n",
    "\n",
    "* a. The functions `nltk.tree.pprint()` and `nltk.chunk.tree2conllstr()` can be used to create Treebank and IOB strings from a tree. Write functions `chunk2brackets()` and `chunk2iob()` that take a single chunk tree as their sole argument, and return the required multi-line string representation.\n",
    "\n",
    "* b. Write command-line conversion utilities `bracket2iob.py` and `iob2bracket.py` that take a file in Treebank or CoNLL format (resp) and convert it to the other format. (Obtain some raw Treebank or CoNLL data from the NLTK Corpora, save it to a file, and then use `for line in open(filename)` to access it from Python.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Color me confused.  If I follow the instructions above to letter, the IPython interpreter throws an error:*\n",
    "\n",
    "```\n",
    "nltk.tree.pprint(nltk.corpus.treebank_chunk.chunked_sents()[0])\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-158-32f265c33d32> in <module>\n",
    "----> 1 nltk.tree.pprint(nltk.corpus.treebank_chunk.chunked_sents()[0])\n",
    "\n",
    "AttributeError: module 'nltk.tree' has no attribute 'pprint'\n",
    "```\n",
    "\n",
    "*But if I modify the syntax somewhat, it works:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "nltk.tree.Tree.pprint(nltk.corpus.treebank_chunk.chunked_sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But the output is the same as just using `print`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.treebank_chunk.chunked_sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Mr./NNP Vinken/NNP)\n",
      "  is/VBZ\n",
      "  (NP chairman/NN)\n",
      "  of/IN\n",
      "  (NP Elsevier/NNP N.V./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT Dutch/NNP publishing/VBG group/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Mr./NNP Vinken/NNP)\n",
      "  is/VBZ\n",
      "  (NP chairman/NN)\n",
      "  of/IN\n",
      "  (NP Elsevier/NNP N.V./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT Dutch/NNP publishing/VBG group/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Rudolph/NNP Agnew/NNP)\n",
      "  ,/,\n",
      "  (NP 55/CD years/NNS)\n",
      "  old/JJ\n",
      "  and/CC\n",
      "  (NP former/JJ chairman/NN)\n",
      "  of/IN\n",
      "  (NP Consolidated/NNP Gold/NNP Fields/NNP PLC/NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  named/VBN\n",
      "  (NP a/DT nonexecutive/JJ director/NN)\n",
      "  of/IN\n",
      "  (NP this/DT British/JJ industrial/JJ conglomerate/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Rudolph/NNP Agnew/NNP)\n",
      "  ,/,\n",
      "  (NP 55/CD years/NNS)\n",
      "  old/JJ\n",
      "  and/CC\n",
      "  (NP former/JJ chairman/NN)\n",
      "  of/IN\n",
      "  (NP Consolidated/NNP Gold/NNP Fields/NNP PLC/NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  named/VBN\n",
      "  (NP a/DT nonexecutive/JJ director/NN)\n",
      "  of/IN\n",
      "  (NP this/DT British/JJ industrial/JJ conglomerate/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT form/NN)\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  once/RB\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  make/VB\n",
      "  Kent/NNP\n",
      "  (NP cigarette/NN filters/NNS)\n",
      "  has/VBZ\n",
      "  caused/VBN\n",
      "  (NP a/DT high/JJ percentage/NN)\n",
      "  of/IN\n",
      "  (NP cancer/NN deaths/NNS)\n",
      "  among/IN\n",
      "  (NP a/DT group/NN)\n",
      "  of/IN\n",
      "  (NP workers/NNS)\n",
      "  exposed/VBN\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP 30/CD years/NNS)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  reported/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT form/NN)\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  once/RB\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  make/VB\n",
      "  Kent/NNP\n",
      "  (NP cigarette/NN filters/NNS)\n",
      "  has/VBZ\n",
      "  caused/VBN\n",
      "  (NP a/DT high/JJ percentage/NN)\n",
      "  of/IN\n",
      "  (NP cancer/NN deaths/NNS)\n",
      "  among/IN\n",
      "  (NP a/DT group/NN)\n",
      "  of/IN\n",
      "  (NP workers/NNS)\n",
      "  exposed/VBN\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP 30/CD years/NNS)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  reported/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT asbestos/NN fiber/NN)\n",
      "  ,/,\n",
      "  (NP crocidolite/NN)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  unusually/RB\n",
      "  resilient/JJ\n",
      "  once/IN\n",
      "  (NP it/PRP)\n",
      "  enters/VBZ\n",
      "  (NP the/DT lungs/NNS)\n",
      "  ,/,\n",
      "  with/IN\n",
      "  (NP even/RB brief/JJ exposures/NNS)\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  causing/VBG\n",
      "  (NP symptoms/NNS)\n",
      "  (NP that/WDT)\n",
      "  show/VBP\n",
      "  up/IN\n",
      "  (NP decades/NNS)\n",
      "  later/JJ\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  said/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT asbestos/NN fiber/NN)\n",
      "  ,/,\n",
      "  (NP crocidolite/NN)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  unusually/RB\n",
      "  resilient/JJ\n",
      "  once/IN\n",
      "  (NP it/PRP)\n",
      "  enters/VBZ\n",
      "  (NP the/DT lungs/NNS)\n",
      "  ,/,\n",
      "  with/IN\n",
      "  (NP even/RB brief/JJ exposures/NNS)\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  causing/VBG\n",
      "  (NP symptoms/NNS)\n",
      "  (NP that/WDT)\n",
      "  show/VBP\n",
      "  up/IN\n",
      "  (NP decades/NNS)\n",
      "  later/JJ\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  said/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Lorillard/NNP Inc./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT unit/NN)\n",
      "  of/IN\n",
      "  (NP New/JJ York-based/JJ Loews/NNP Corp./NNP)\n",
      "  (NP that/WDT)\n",
      "  makes/VBZ\n",
      "  Kent/NNP\n",
      "  (NP cigarettes/NNS)\n",
      "  ,/,\n",
      "  stopped/VBD\n",
      "  using/VBG\n",
      "  (NP crocidolite/NN)\n",
      "  in/IN\n",
      "  (NP its/PRP$ Micronite/NN cigarette/NN filters/NNS)\n",
      "  in/IN\n",
      "  (NP 1956/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Lorillard/NNP Inc./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT unit/NN)\n",
      "  of/IN\n",
      "  (NP New/JJ York-based/JJ Loews/NNP Corp./NNP)\n",
      "  (NP that/WDT)\n",
      "  makes/VBZ\n",
      "  Kent/NNP\n",
      "  (NP cigarettes/NNS)\n",
      "  ,/,\n",
      "  stopped/VBD\n",
      "  using/VBG\n",
      "  (NP crocidolite/NN)\n",
      "  in/IN\n",
      "  (NP its/PRP$ Micronite/NN cigarette/NN filters/NNS)\n",
      "  in/IN\n",
      "  (NP 1956/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  Although/IN\n",
      "  (NP preliminary/JJ findings/NNS)\n",
      "  were/VBD\n",
      "  reported/VBN\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP a/DT year/NN)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP the/DT latest/JJS results/NNS)\n",
      "  appear/VBP\n",
      "  in/IN\n",
      "  (NP today/NN 's/POS New/NNP England/NNP Journal/NNP)\n",
      "  of/IN\n",
      "  (NP Medicine/NNP)\n",
      "  ,/,\n",
      "  (NP a/DT forum/NN)\n",
      "  likely/JJ\n",
      "  to/TO\n",
      "  bring/VB\n",
      "  (NP new/JJ attention/NN)\n",
      "  to/TO\n",
      "  (NP the/DT problem/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  Although/IN\n",
      "  (NP preliminary/JJ findings/NNS)\n",
      "  were/VBD\n",
      "  reported/VBN\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP a/DT year/NN)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP the/DT latest/JJS results/NNS)\n",
      "  appear/VBP\n",
      "  in/IN\n",
      "  (NP today/NN 's/POS New/NNP England/NNP Journal/NNP)\n",
      "  of/IN\n",
      "  (NP Medicine/NNP)\n",
      "  ,/,\n",
      "  (NP a/DT forum/NN)\n",
      "  likely/JJ\n",
      "  to/TO\n",
      "  bring/VB\n",
      "  (NP new/JJ attention/NN)\n",
      "  to/TO\n",
      "  (NP the/DT problem/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP We/PRP)\n",
      "  're/VBP\n",
      "  talking/VBG\n",
      "  about/IN\n",
      "  (NP years/NNS)\n",
      "  ago/IN\n",
      "  before/IN\n",
      "  (NP anyone/NN)\n",
      "  heard/VBD\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  having/VBG\n",
      "  (NP any/DT questionable/JJ properties/NNS)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP We/PRP)\n",
      "  're/VBP\n",
      "  talking/VBG\n",
      "  about/IN\n",
      "  (NP years/NNS)\n",
      "  ago/IN\n",
      "  before/IN\n",
      "  (NP anyone/NN)\n",
      "  heard/VBD\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  having/VBG\n",
      "  (NP any/DT questionable/JJ properties/NNS)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP There/EX)\n",
      "  is/VBZ\n",
      "  (NP no/DT asbestos/NN)\n",
      "  in/IN\n",
      "  (NP our/PRP$ products/NNS)\n",
      "  now/RB\n",
      "  ./.)\n",
      "(S\n",
      "  (NP There/EX)\n",
      "  is/VBZ\n",
      "  (NP no/DT asbestos/NN)\n",
      "  in/IN\n",
      "  (NP our/PRP$ products/NNS)\n",
      "  now/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for sent in nltk.corpus.treebank_chunk.chunked_sents()[:10]:\n",
    "    nltk.tree.Tree.pprint(sent)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre NNP B-NP\n",
      "Vinken NNP I-NP\n",
      ", , O\n",
      "61 CD B-NP\n",
      "years NNS I-NP\n",
      "old JJ O\n",
      ", , O\n",
      "will MD O\n",
      "join VB O\n",
      "the DT B-NP\n",
      "board NN I-NP\n",
      "as IN O\n",
      "a DT B-NP\n",
      "nonexecutive JJ I-NP\n",
      "director NN I-NP\n",
      "Nov. NNP I-NP\n",
      "29 CD I-NP\n",
      ". . O\n"
     ]
    }
   ],
   "source": [
    "print(nltk.chunk.tree2conllstr(nltk.corpus.treebank_chunk.chunked_sents()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "★ An n-gram chunker can use information other than the current part-of-speech tag and the $n-1$ previous chunk tags. Investigate other models of the context, such as the $n-1$ previous part-of-speech tags, or some combination of previous chunk tags along with previous and following part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. \n",
    "\n",
    "★ Consider the way an n-gram tagger uses recent tags to inform its tagging choice. Now observe how a chunker may re-use this sequence information. For example, both tasks will make use of the information that nouns tend to follow adjectives (in English). It would appear that the same information is being maintained in two places. Is this likely to become a problem as the size of the rule sets grows? If so, speculate about any ways that this problem might be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
