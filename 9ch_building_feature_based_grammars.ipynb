{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 9\n",
    "\n",
    "## Building Feature Based Grammars\n",
    "\n",
    "*The html version of this chapter in the book is available [here](https://www.nltk.org/book/ch09.html \"ch09\").*\n",
    "\n",
    "### 1   Grammatical Features\n",
    "\n",
    "We're now going to declare the features of words and phrases.  Here's an example of using dictionaries to store features and their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
    "chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature structures__ are the pairings of features and values.\n",
    "\n",
    "Adding more properties to the verb *chase*.  The subject plays the role of \"agent\" and the object the role of \"patient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "chase['AGT'] = 'sbj'\n",
    "chase['PAT'] = 'obj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rather convoluted code that follows processes the sentence *Kim chased Lee* and \"binds\" the verb's agent role to the subject (because it's to the left of the verb) and patient role to the object (because it's to the right of the verb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n"
     ]
    }
   ],
   "source": [
    "sent = \"Kim chased Lee\"\n",
    "tokens = sent.split()\n",
    "lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
    "\n",
    "def lex2fs(word):\n",
    "    for fs in [kim, lee, chase]:\n",
    "        if fs['ORTH'] == word:\n",
    "            return fs\n",
    "        \n",
    "subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n",
    "verb['AGT'] = subj['REF']\n",
    "verb['PAT'] = obj['REF']\n",
    "\n",
    "for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
    "    print(\"%-5s => %s\" % (k, verb[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1   Syntactic Agreement\n",
    "\n",
    "This simple grammar would permit *This dog runs*, but it would also permit sentences where the noun and verb don't agree, such as <i>*Theses dogs runs<i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "simple_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N \n",
    "VP -> V\n",
    "Det -> 'this' | 'these'\n",
    "N -> 'dog' | 'dogs'\n",
    "V -> 'runs'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more complex grammar would block sentences without agreement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP_SG VP_SG\n",
    "S -> NP_PL VP_PL\n",
    "NP_SG -> Det_SG N_SG\n",
    "NP_PL -> Det_PL N_PL\n",
    "VP_SG -> V_SG\n",
    "VP_PL -> V_PL\n",
    "\n",
    "Det_SG -> 'this'\n",
    "Det_PL -> 'these'\n",
    "N_SG -> 'dog'\n",
    "N_PL -> 'dogs'\n",
    "V_SG -> 'runs'\n",
    "V_PL -> 'run'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In effect, we've doubled the size of our grammar.  If we needed agreement for first, second and thrid person as well, this method would result in a grammar that would be six times the size of our original `simple` grammar.  We will look at ways of streamlining this.\n",
    "\n",
    "#### 1.2 Using Attributes and Constraints\n",
    "\n",
    "We could add __features__ to our notation, and use variables over values to reduce the number of required productions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "S[NUM = ?n] -> NP[NUM = ?n] VP[NUM = ?n]\n",
    "NP[NUM = ?n] -> Det[NUM = ?n] N[NUM = ?n]\n",
    "VP[NUM = ?n] -> V[NUM = ?n]\n",
    "Det[NUM = sg] -> 'this'\n",
    "Det[NUM = pl] -> 'these'\n",
    "N[NUM = sg] -> 'dog'\n",
    "N[NUM = pl] -> 'dogs'\n",
    "V[NUM = sg] -> 'runs'\n",
    "V[NUM = pl] -> 'run'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, there has to be agreement now, or the parse will fail.  \n",
    "\n",
    "For words that agree with all numbers (i.e., singular and plural), we can leave the `NUM` value __underspecified__ instead of declaring it twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "S[NUM = ?n] -> NP[NUM = ?n] VP[NUM = ?n]\n",
    "NP[NUM = ?n] -> Det[NUM = ?n] N[NUM = ?n]\n",
    "VP[NUM = ?n] -> V[NUM = ?n]\n",
    "\n",
    "Det[NUM = sg] -> 'this'\n",
    "Det[NUM = pl] -> 'these'\n",
    "N[NUM = sg] -> 'dog'\n",
    "N[NUM = pl] -> 'dogs'\n",
    "V[NUM = sg] -> 'runs'\n",
    "V[NUM = pl] -> 'run'\n",
    "Det[NUM = ?n] -> 'the' | 'some' | 'any'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the ideas from this chapter - and a few more - can be found in the grammar below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n] \n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl] \n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' \n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this grammar with a parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'Kim likes children'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat0.fcfg', trace = 2)\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3   Terminology\n",
    "\n",
    "Values like `sg` and `pl` are __atomic__ because they can't be decomposed into subparts.  We can use a __boolean__ value to distinguish __auxiliary__ verbs (e.g., modal verbs) with the boolean feature `AUX`.  For example, *can* could be represented with the production `V[TENSE=pres, AUX=+]`, though the convention is to put the `+/-` sign in front of the feature, like so: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "V[TENSE=pres, +AUX] -> 'can'\n",
    "V[TENSE=pres, +AUX] -> 'may'\n",
    "\n",
    "V[TENSE=pres, -AUX] -> 'walks'\n",
    "V[TENSE=pres, -AUX] -> 'likes'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also group together the features in an __attribute value matrix__ (AVM). `AGR` stands for *agreement*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[POS = N           ]\n",
    "[                  ]\n",
    "[AGR = [PER = 3   ]]\n",
    "[      [NUM = pl  ]]\n",
    "[      [GND = fem ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order does not matter in representation, so this would be equivalent:\n",
    "\n",
    "```\n",
    "[AGR = [NUM = pl  ]]\n",
    "[      [PER = 3   ]]\n",
    "[      [GND = fem ]]\n",
    "[                  ]\n",
    "[POS = N           ]\n",
    "```\n",
    "\n",
    "We could then refactor a grammar so that agreement features are bundled together like so:\n",
    "\n",
    "```\n",
    "S                    -> NP[AGR=?n] VP[AGR=?n]\n",
    "NP[AGR=?n]           -> PropN[AGR=?n]\n",
    "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
    "\n",
    "Cop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\n",
    "PropN[AGR=[NUM=sg, PER=3]]            -> 'Kim'\n",
    "Adj                                   -> 'happy'\n",
    "```\n",
    "\n",
    "__NOTE__: The book doesn't really do a great job of showing us how to set up Feature Grammar Parsing. We are required to do just this for the first exercise in the unit, but unfortunately there is not enough information in the chapter to carry this off.  Cf. [this nltk tutorial](http://www.nltk.org/howto/featgram.html \"Feature Grammar Parsing\") for more information, but be aware that I still needed to futz around a bit before I got something to work.\n",
    "\n",
    "### 2   Processing Feature Structures\n",
    "\n",
    "Use `FeatStruct()` to declare feature structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ NUM   = 'sg'   ]\n",
      "[ TENSE = 'past' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(TENSE = 'past', NUM = 'sg')\n",
    "print(fs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature structures are technically a kind of dictionary, so normal dictionary operations will work on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fem\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(PER = 3, NUM = 'pl', GND = 'fem')\n",
    "print(fs1['GND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1['CASE'] = 'acc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define structures that have complex values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ CASE = 'acc' ] ]\n",
      "[ AGR = [ GND  = 'fem' ] ]\n",
      "[       [ NUM  = 'pl'  ] ]\n",
      "[       [ PER  = 3     ] ]\n",
      "[                        ]\n",
      "[ POS = 'N'              ]\n"
     ]
    }
   ],
   "source": [
    "fs2 = nltk.FeatStruct(POS = 'N', AGR = fs1)\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CASE = 'acc' ]\n",
      "[ GND  = 'fem' ]\n",
      "[ NUM  = 'pl'  ]\n",
      "[ PER  = 3     ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2['AGR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(fs2['AGR']['PER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method is to use a brackted string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"[POS = 'N', AGR = [PER = 3, NUM = 'pl', GND = 'fem']]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature structures are not limited to linguistic objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ AGE   = 33               ]\n",
      "[ NAME  = 'Lee'            ]\n",
      "[ TELNO = '01 27 86 42 96' ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(NAME = 'Lee', TELNO = '01 27 86 42 96', AGE = 33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view feature structures as __directed acyclic graphs__ (DAGs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\dag01.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature values can be complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\dag02.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __feature path__ is a sequence of arcs that can be followed from the root node.  Ergo, `('ADDRESS', 'STREET')` is a feature path whose value is the node label `rue Pascal`.\n",
    "\n",
    "If Lee had a spouse named *Kim*, it would follow that they would have the same address.  We could represent that information like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\dag04.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could 'share' the sub-graph between different arcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\dag03.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These DAGs involve __structure sharing__ or __reentrancy__.  If two paths have the same value, we say they are __equivalent__.\n",
    "\n",
    "To indicate reentrancy, we will prefix the first occurrence of a shared feature stucture with an integer in parentheses.  Later references will use that notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"\"\"[NAME = 'Lee', \n",
    "                          ADDRESS = (1) [NUMBER = 74, \n",
    "                                         STREET = 'rue Pascal'],\n",
    "                          SPOUSE = [NAME = 'Kim', \n",
    "                                    ADDRESS -> (1)]]\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bracketed integers are called __tags__ or __coindices__.  We can use any number of tags within a single feature structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = 'a'             ]\n",
      "[                     ]\n",
      "[ B = (1) [ C = 'c' ] ]\n",
      "[                     ]\n",
      "[ D -> (1)            ]\n",
      "[ E -> (1)            ]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.FeatStruct(\"[A = 'a', B = (1)[C = 'c'], D -> (1), E -> (1)]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Subsumption and Unification\n",
    "\n",
    "We think of feature structures as providing __partial information__ about objects, and we can order these structures based on how much information they contain.  The following three structures are ordered by increasing amounts of information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.\n",
    "\n",
    "`[NUMBER = 74]`\n",
    "\n",
    "b.\t\t\n",
    "\n",
    "`[NUMBER = 74          ]\n",
    " [STREET = 'rue Pascal']`\n",
    "\n",
    "c.\t\t\n",
    "\n",
    "`[NUMBER = 74          ]\n",
    " [STREET = 'rue Pascal']\n",
    " [CITY = 'Paris'       ]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ordering is called __subsumption__. $FS_0$ subsumes $FS_1$ if all information in $FS_0$ is also in $FS_1$.  We use the symbol $\\sqsubseteq$ to represent subsumption.\n",
    "\n",
    "But we need to be more careful when we add the possibility of reentrancy.  If $FS_0 \\sqsubseteq FS_1$, then $FS_1$ must have all the paths and reentrancies of $FS_0$. \n",
    "\n",
    "Merging information from two feature structures is called __unification__, and is supported by the `unify()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(NUMBER = 74, STREET = 'rue Pascal')\n",
    "fs2 = nltk.FeatStruct(CITY = 'Paris')\n",
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unification is a (partial) binary operation: $FS_0 \\sqcup FS_1$.  Unification is binary, so $FS_0 \\sqcup FS_1 = FS_1 \\sqcup FS_0$.  The same is true in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2.unify(fs1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we unify two feature structures in a subsumption relationship, the result is the more informative of the two. E.g., if $FS_0 \\sqsubseteq FS_1$, then $FS_0 \\sqcup FS_1 = FS_1$\n",
    "\n",
    "If two feature structures share a path but have distinct values, the unification will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "fs0 = nltk.FeatStruct(A = 'a')\n",
    "fs1 = nltk.FeatStruct(A = 'b')\n",
    "fs2 = fs0.unify(fs1)\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider how unification interacts with structure sharing.  Let's define one of the DAGs from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n"
     ]
    }
   ],
   "source": [
    "fs0 = nltk.FeatStruct(\"\"\"[NAME = Lee,\n",
    "                          ADDRESS = [NUMBER = 74,\n",
    "                                     STREET = 'rue Pascal'],\n",
    "                          SPOUSE =  [NAME = Kim,\n",
    "                                     ADDRESS = [NUMBER = 74,\n",
    "                                                STREET = 'rue Pascal']]]\"\"\")\n",
    "print(fs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's augment Kim's address with a specification for `CITY`.  Notice that we need to include the whole path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [           [ CITY   = 'Paris'      ] ] ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(\"[SPOUSE = [ADDRESS = [CITY = Paris]]]\")\n",
    "print(fs1.unify(fs0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result would have been different if we had been sharing structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               [ CITY   = 'Paris'      ] ]\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ Name    = 'Kim' ]           ]\n"
     ]
    }
   ],
   "source": [
    "fs2 = nltk.FeatStruct(\"\"\"[NAME = Lee, ADDRESS = (1) [NUMBER = 74,\n",
    "                                                     STREET = 'rue Pascal'],\n",
    "                                      SPOUSE = [Name = Kim, ADDRESS ->(1)]]\"\"\")\n",
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use variables such as `?x` for structure sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS1 = ?x ]\n",
      "[ ADDRESS2 = ?x ]\n"
     ]
    }
   ],
   "source": [
    "fs1 = nltk.FeatStruct(\"[ADDRESS1 = [NUMBER = 74, STREET = 'rue Pascal']]\")\n",
    "fs2 = nltk.FeatStruct(\"[ADDRESS1 = ?x, ADDRESS2 = ?x]\")\n",
    "print(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ADDRESS1 = (1) [ NUMBER = 74           ] ]\n",
      "[                [ STREET = 'rue Pascal' ] ]\n",
      "[                                          ]\n",
      "[ ADDRESS2 -> (1)                          ]\n"
     ]
    }
   ],
   "source": [
    "print(fs2.unify(fs1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Extending a Feature based Grammar\n",
    "\n",
    "#### 3.1 Subcategorization\n",
    "\n",
    "Subcategorization allows us to make general distinctions across entire categories.  This was originally developed for a grammar framework called Generalized Phrase Structure Grammar (GPSG).\n",
    "\n",
    "```\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
    "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
    "\n",
    "V[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\n",
    "V[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
    "V[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\n",
    "\n",
    "V[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\n",
    "V[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
    "V[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\n",
    "\n",
    "V[SUBCAT=intrans, TENSE=past, NUM=?n] -> 'disappeared' | 'walked'\n",
    "V[SUBCAT=trans, TENSE=past, NUM=?n] -> 'saw' | 'liked'\n",
    "V[SUBCAT=clause, TENSE=past, NUM=?n] -> 'said' | 'claimed'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SBar` refers to subordinate clauses.  They require two further productions:\n",
    "\n",
    "```\n",
    "SBar -> Comp S\n",
    "Comp -> 'that'\n",
    "\n",
    "```\n",
    "\n",
    "Here is an example tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch09-tree-10.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another form of subcategorization directly encodes the valency of a head (i.e., the list of arguments that it can combine with).  E.g., *put* might be represented as `V[SUBCAT=<NP, NP, PP>]`, as in *put the book on the table*.\n",
    "\n",
    "#### 3.2 Heads Revisited\n",
    "\n",
    "X-bar Syntax represents the parent / head-child relation by abstracting out the notion of __phrasal level__.  If `N` represents the lexical level, `N'` represents the next level up, and `N\"` represents the phrasal level.  Here is a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch09-tree-12.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`N'` and `N\"` are called __(phrasal) projections__.  `N\"` is the __maximal projection__, and `N` is sometimes called the __zero projection__.  Here is an example of Python code:\n",
    "\n",
    "`\n",
    "S -> N[BAR=2] V[BAR=2]\n",
    "N[BAR=2] -> Det N[BAR=1]\n",
    "N[BAR=1] -> N[BAR=1] P[BAR=2]\n",
    "N[BAR=1] -> N[BAR=0] P[BAR=2]\n",
    "N[BAR=1] -> N[BAR=0]XS\n",
    "`\n",
    "\n",
    "#### 3.3   Auxiliary Verbs and Inversion\n",
    "\n",
    "Negative adverbs can begin sentences in English, but this requires an auxiliary verb.  E.g., \"Rarely do you see Kim.\"  This production is how we could capture such a structure:\n",
    "\n",
    "`S[+INV] -> V[+AUX] NP VP`\n",
    "\n",
    "#### 3.4 Unbounded Dependency Constructions\n",
    "\n",
    "Verbs such as like require an `NP` complement, but complements can be omitted in sentences like 'Kim knows who you like', because *who* serves as a __filler__.  We might say that sentences without these complements have __gaps__, and sometimes these gaps are made explicit with an underscore, e.g. 'Kim knows who you like \\__'.\n",
    "\n",
    "The mutual co-occurrence between filler and gap is sometimes termed a \"dependency\", and there is an upper bound on the distance between filler and gap.  But sentences can have indefinitely deep recursion of sentential complements, meaning that the gap can be embedded indefinitely far inside the whole sentence.  This leads to the notion of the __unbounded dependency construction__, where there is no upper bound on the distance between filler and gap.  \n",
    "\n",
    "One mechanism for handling unbounded dependencies involves __slash categories__ in the form `Y/XP`, where a pharse of category `Y` is missing a sub-constituent of category `XP`.  Here is an example:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch09-tree-16.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar below illustrates the main principles of slash categories, and also includes productions for inverted clauses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "S[-INV] -> NP VP\n",
      "S[-INV]/?x -> NP VP/?x\n",
      "S[-INV] -> NP S/NP\n",
      "S[-INV] -> Adv[+NEG] S[+INV]\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
      "SBar -> Comp S[-INV]\n",
      "SBar/?x -> Comp S[-INV]/?x\n",
      "VP -> V[SUBCAT=intrans, -AUX]\n",
      "VP -> V[SUBCAT=trans, -AUX] NP\n",
      "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
      "VP -> V[SUBCAT=clause, -AUX] SBar\n",
      "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
      "VP -> V[+AUX] VP\n",
      "VP/?x -> V[+AUX] VP/?x\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
      "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
      "V[+AUX] -> 'do' | 'can'\n",
      "NP[-WH] -> 'you' | 'cats'\n",
      "NP[+WH] -> 'who'\n",
      "Adv[+NEG] -> 'rarely' | 'never'\n",
      "NP/NP ->\n",
      "Comp -> 'that'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing 'who do you claim that you like':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[+WH] who)\n",
      "  (S[+INV]/NP[]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[]/NP[]\n",
      "      (V[-AUX, SUBCAT='clause'] claim)\n",
      "      (SBar[]/NP[]\n",
      "        (Comp[] that)\n",
      "        (S[-INV]/NP[]\n",
      "          (NP[-WH] you)\n",
      "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'who do you claim that you like'.split()\n",
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree above drawn out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\ch09-tree-17.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grammar will also parse sentences without gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (NP[-WH] you)\n",
      "  (VP[]\n",
      "    (V[-AUX, SUBCAT='clause'] claim)\n",
      "    (SBar[]\n",
      "      (Comp[] that)\n",
      "      (S[-INV]\n",
      "        (NP[-WH] you)\n",
      "        (VP[] (V[-AUX, SUBCAT='trans'] like) (NP[-WH] cats))))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'you claim that you like cats'.split()\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also admit inverted sentences which do not involve *wh* constructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[-INV]\n",
      "  (Adv[+NEG] rarely)\n",
      "  (S[+INV]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[] (V[-AUX, SUBCAT='intrans'] sing))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'rarely do you sing'.split()\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5   Case and Gender in German\n",
    "\n",
    "This grammar illustrates the interaction of agreement (comprising person, number and gender) with case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# Grammar Productions\n",
      "S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
      "NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
      "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
      "VP[AGR=?a] -> IV[AGR=?a]\n",
      "VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
      "# Lexical Productions\n",
      "# Singular determiners\n",
      "# masc\n",
      "Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der' \n",
      "Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
      "Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
      "# fem\n",
      "Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
      "Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
      "Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
      "# Plural determiners\n",
      "Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die' \n",
      "Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den' \n",
      "Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die' \n",
      "# Nouns\n",
      "N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
      "N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      "N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
      "N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      "N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
      "N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
      "# Pronouns\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
      "PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
      "PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
      "PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
      "PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
      "PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      "PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      "PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
      "PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
      "# Verbs\n",
      "IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n",
      "IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
      "IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
      "IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
      "IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
      "IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
      "TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
      "TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
      "TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/german.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a parse tree for a sentence with a verb which governs the dative case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n"
     ]
    }
   ],
   "source": [
    "tokens = 'ich folge den Katzen'.split()\n",
    "cp = load_parser('grammars/book_grammars/german.fcfg')\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `trace`, we can see how a sentence fails to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.ich.fol.den.Kat.|\n",
      "Leaf Init Rule:\n",
      "|[---]   .   .   .| [0:1] 'ich'\n",
      "|.   [---]   .   .| [1:2] 'folge'\n",
      "|.   .   [---]   .| [2:3] 'den'\n",
      "|.   .   .   [---]| [3:4] 'Katze'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---]   .   .   .| [0:1] PRO[AGR=[NUM='sg', PER=1], CASE='nom'] -> 'ich' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---]   .   .   .| [0:1] NP[AGR=[NUM='sg', PER=1], CASE='nom'] -> PRO[AGR=[NUM='sg', PER=1], CASE='nom'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[--->   .   .   .| [0:1] S[] -> NP[AGR=?a, CASE='nom'] * VP[AGR=?a] {?a: [NUM='sg', PER=1]}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   [---]   .   .| [1:2] TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] -> 'folge' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   [--->   .   .| [1:2] VP[AGR=?a] -> TV[AGR=?a, OBJCASE=?c] * NP[CASE=?c] {?a: [NUM='sg', PER=1], ?c: 'dat'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [---]   .| [2:3] Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] -> 'den' *\n",
      "|.   .   [---]   .| [2:3] Det[AGR=[NUM='pl', PER=3], CASE='dat'] -> 'den' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [--->   .| [2:3] NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [NUM='pl', PER=3], ?c: 'dat'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [--->   .| [2:3] NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [GND='masc', NUM='sg', PER=3], ?c: 'acc'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   .   [---]| [3:4] N[AGR=[GND='fem', NUM='sg', PER=3]] -> 'Katze' *\n"
     ]
    }
   ],
   "source": [
    "tokens = 'ich folge den Katze'.split()\n",
    "cp = load_parser('grammars/book_grammars/german.fcfg', trace = 2)\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Summary\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "### 5 Further Reading\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "### 6 Exercises\n",
    "\n",
    "##### 1. \n",
    "\n",
    "☼ What constraints are required to correctly parse word sequences like I am happy and she is happy but not *you is happy or *they am happy? Implement two solutions for the present tense paradigm of the verb be in English, first taking Grammar [(6)](https://www.nltk.org/book/ch09.html#ex-agcfg1) as your starting point, and then taking Grammar [(18)](https://www.nltk.org/book/ch09.html#ex-agr2) as the starting point.\n",
    "\n",
    "*The first grammar involves a __lot__ of redundancy:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP_SG_1 VP_SG_1\n",
    "S -> NP_SG_2 VP_SG_2\n",
    "S -> NP_SG_3 VP_SG_3\n",
    "S -> NP_PL VP_PL\n",
    "NP_SG_1 -> N_SG_1\n",
    "NP_SG_2 -> N_SG_2\n",
    "NP_SG_3 -> N_SG_3\n",
    "NP_PL -> N_PL_1 | N_PL_2 | N_PL_3\n",
    "VP_SG_1 -> V_SG_1 ADJ\n",
    "VP_SG_2 -> V_SG_2 ADJ\n",
    "VP_SG_3 -> V_SG_3 ADJ\n",
    "VP_PL -> V_PL ADJ\n",
    "N_SG_1 -> \"I\"\n",
    "N_SG_2 -> \"you\"\n",
    "N_SG_3 -> \"he\" | \"she\" | \"it\"\n",
    "N_PL_1 -> \"we\"\n",
    "N_PL_2 -> \"you\"\n",
    "N_PL_3 -> \"they\"\n",
    "V_SG_1 -> \"am\"\n",
    "V_SG_2 -> \"are\"\n",
    "V_SG_3 -> \"is\"\n",
    "V_PL   -> \"are\"\n",
    "ADJ -> \"happy\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP_SG_1 (N_SG_1 I)) (VP_SG_1 (V_SG_1 am) (ADJ happy)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"I am happy\".split()\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP_SG_3 (N_SG_3 she)) (VP_SG_3 (V_SG_3 is) (ADJ happy)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"she is happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If our grammar is correct, the next two sentences won't produce any parses:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"you is happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"they am happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second grammar was a bit difficult to set up. In my opinion, much more difficult than should be expected for part of the first question in an exercise set.  I couldn't find anything in the book to help me, and had to resort to [an external nltk tutorial](http://www.nltk.org/howto/featgram.html \"Feature Grammar Parsing\") in order to answer this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import grammar, parse\n",
    "\n",
    "g = \"\"\"\n",
    "% start S\n",
    "S -> NP[AGR = ?n] VP[AGR = ?n]\n",
    "NP[AGR = ?n] -> N[AGR = ?n]\n",
    "VP[AGR = ?n] -> V[AGR = ?n] ADJ\n",
    "N[AGR = [PER = 1, NUM = sg]] -> \"I\"\n",
    "N[AGR = [PER = 2, NUM = sg]] -> \"you\"\n",
    "N[AGR = [PER = 3, NUM = sg]] -> \"he\" | \"she\" | \"it\"\n",
    "N[AGR = [PER = 1, NUM = pl]] -> \"we\"\n",
    "N[AGR = [PER = 2, NUM = pl]] -> \"you\"\n",
    "N[AGR = [PER = 3, NUM = pl]]-> \"they\"\n",
    "V[AGR = [PER = 1, NUM = sg]] -> \"am\" \n",
    "V[AGR = [PER = 2, NUM = sg]] -> \"are\" \n",
    "V[AGR = [PER = 3, NUM = sg]] -> \"is\" \n",
    "V[AGR = [PER = ?n, NUM = pl]]  -> \"are\" \n",
    "ADJ -> \"happy\"\n",
    "\"\"\"\n",
    "\n",
    "gram = grammar.FeatureGrammar.fromstring(g)\n",
    "parser = parse.FeatureEarleyChartParser(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1]] (N[AGR=[NUM='sg', PER=1]] I))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (V[AGR=[NUM='sg', PER=1]] am)\n",
      "    (ADJ[] happy)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"I am happy\".split()\n",
    "\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=3]] (N[AGR=[NUM='sg', PER=3]] she))\n",
      "  (VP[AGR=[NUM='sg', PER=3]]\n",
      "    (V[AGR=[NUM='sg', PER=3]] is)\n",
      "    (ADJ[] happy)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"she is happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='pl', PER=3]] (N[AGR=[NUM='pl', PER=3]] they))\n",
      "  (VP[AGR=[NUM='pl', PER=?n2]]\n",
      "    (V[AGR=[NUM='pl', PER=?n]] are)\n",
      "    (ADJ[] happy)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"they are happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If our grammar is correct, the next two sentences won't produce any parses:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"you is happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"they am happy\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.\n",
    "\n",
    "☼ Develop a variant of grammar in [1.1](https://www.nltk.org/book/ch09.html#code-feat0cfg) that uses a feature count to make the distinctions shown below:\n",
    "\n",
    "(54)\n",
    "\n",
    "* a. The boy sings.\n",
    "\n",
    "* b. *Boy sings.\n",
    "\n",
    "(55)\n",
    "\n",
    "* a. The boys sings.\n",
    "\n",
    "* b. Boys sing.\n",
    "\n",
    "(56)\n",
    "\n",
    "* a. The water is precious.\n",
    "\n",
    "* b. Water is precious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this exercise didn't become clear to me until I started doing it.  Water is uncountable, so it can be used with and without an determiner.  However, countable nouns can only be used without a determiner if they are plural.\n",
    "\n",
    "*Sing* is a ambitransitive verb, meaning it can be transitive or intransitive, while *to be* is a copula. Since I wasn't going use this grammar for anything but the sentences in this exercise, I eliminated the productions for `TV` and `IV` and replaced them with `ambiV` and `Cop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start S\n",
    "# ###################\n",
    "# Grammar Productions\n",
    "# ###################\n",
    "# S expansion productions\n",
    "S -> NP[NUM=?n] VP[NUM=?n]\n",
    "# NP expansion productions\n",
    "NP[NUM=?n, Count = ?c] -> Det[NUM=?n] N[NUM=?n, Count = ?c]\n",
    "NP[NUM=pl, Count = ?c] -> N[NUM=pl, Count = ?c]\n",
    "NP[NUM=?n, Count = False] -> N[NUM=?n, Count = False]\n",
    "# VP expansion productions\n",
    "VP[TENSE=?t, NUM=?n] -> ambiV[TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> Cop[TENSE=?t, NUM=?n] ADJ\n",
    "# ###################\n",
    "# Lexical Productions\n",
    "# ###################\n",
    "Det -> 'The' | 'the' \n",
    "N[NUM=sg, Count = True] -> 'boy' | 'Boy' \n",
    "N[NUM=sg, Count = False] -> 'water' | 'Water'\n",
    "N[NUM=pl, Count = True] -> 'Boys' | 'boys'\n",
    "ambiV[TENSE=pres,  NUM=sg] -> 'sings'\n",
    "ambiV[TENSE=pres,  NUM=pl] -> 'sing'\n",
    "Cop[TENSE=pres,  NUM=sg] -> 'is'\n",
    "Cop[TENSE=pres,  NUM=pl] -> 'are'\n",
    "ADJ -> 'precious'\n",
    "\"\"\"\n",
    "\n",
    "gram = grammar.FeatureGrammar.fromstring(g)\n",
    "parser = parse.FeatureEarleyChartParser(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[+Count, NUM='sg'] (Det[] the) (N[+Count, NUM='sg'] boy))\n",
      "  (VP[NUM='sg', TENSE='pres'] (ambiV[NUM='sg', TENSE='pres'] sings)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sent = \"the boy sings\".split()\n",
    "\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't output anything, as it's ungrammatical\n",
    "\n",
    "sent = \"Boy sings\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[+Count, NUM='pl'] (Det[] The) (N[+Count, NUM='pl'] boys))\n",
      "  (VP[NUM='pl', TENSE='pres'] (ambiV[NUM='pl', TENSE='pres'] sing)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The boys sing\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[+Count, NUM='pl'] (N[+Count, NUM='pl'] Boys))\n",
      "  (VP[NUM='pl', TENSE='pres'] (ambiV[NUM='pl', TENSE='pres'] sing)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Boys sing\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[-Count, NUM='sg'] (Det[] The) (N[-Count, NUM='sg'] water))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (Cop[NUM='sg', TENSE='pres'] is)\n",
      "    (ADJ[] precious)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The water is precious\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[-Count, NUM='sg'] (N[-Count, NUM='sg'] Water))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (Cop[NUM='sg', TENSE='pres'] is)\n",
      "    (ADJ[] precious)))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Water is precious\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. \n",
    "\n",
    "☼ Write a function `subsumes()` which holds of two feature structures `fs1` and `fs2` just in case `fs1` subsumes `fs2`.\n",
    "\n",
    "*Seriously?  How am I supposed to know what this means?  This chapter is easily the worst written one I've seen yet.  Don't get me wrong: I'm thrilled the authors have dedicated so much time to making the NLTK library and placing this book under a CC license so that people can use it free of charge; but the editing is atrocious, which is all the more shocking considering the authors are experts in Linguistics.*\n",
    "\n",
    "*This leaves me no choice but to try to divine what the authors meant.  My guess is that we're supposed to write a function that takes two Feature Stuctures and determines if one subsumes the author.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsumes(f1, f2):\n",
    "    \"\"\"\n",
    "    Checks two features structures to see if one subsumes the other.\n",
    "    \n",
    "    Arguments:\n",
    "    f1, f2: two nltk.FeatStruct objects\n",
    "    \n",
    "    Returns:\n",
    "    0:   Neither object subsumes the other\n",
    "    1:   First object subsumes the second\n",
    "    2:   Second object subsumes the first\n",
    "    \"\"\"\n",
    "    \n",
    "    for f in [f1, f2]:\n",
    "        assert isinstance(f, nltk.FeatStruct), \\\n",
    "            \"Argument must be an nltk.FeatStruct\"\n",
    "    \n",
    "    if f1 == f1.unify(f2):\n",
    "        return 1\n",
    "    elif f2 == f2.unify(f1):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(NAME = \"Jack Torrance\", ROOM = '237', \n",
    "                      LODGING = 'Overlook Hotel')\n",
    "fs2 = nltk.FeatStruct(ROOM = '237')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsumes(fs1, fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(ROOM = '237')\n",
    "fs2 = nltk.FeatStruct(NAME = \"Jack Torrance\", ROOM = '237', \n",
    "                      LODGING = 'Overlook Hotel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsumes(fs1, fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(ROOM = '237')\n",
    "fs2 = nltk.FeatStruct(NAME = \"Jack Torrance\", LODGING = 'Overlook Hotel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsumes(fs1, fs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.\n",
    "\n",
    "☼ Modify the grammar illustrated in (28) to incorporate a *bar* feature for dealing with phrasal projections.\n",
    "\n",
    "*This question requires more linguistic knowledge than I currently have.*\n",
    "\n",
    "##### 5.\n",
    "\n",
    "☼ Modify the German grammar in [3.2](https://www.nltk.org/book/ch09.html#code-germancfg) to incorporate the treatment of subcategorization presented in [3](https://www.nltk.org/book/ch09.html#sec-extending-a-feature-based-grammar).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start S\n",
    "# Grammar Productions\n",
    "S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
    "NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
    "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
    "VP[AGR=?a] -> V[SUBCAT = intrans, AGR=?a]\n",
    "VP[AGR=?a] -> V[SUBCAT = trans, OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
    "# Lexical Productions\n",
    "# Singular determiners\n",
    "# masc\n",
    "Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der' \n",
    "Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
    "Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
    "# fem\n",
    "Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
    "Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
    "Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
    "# Plural determiners\n",
    "Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die' \n",
    "Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den' \n",
    "Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die' \n",
    "# Nouns\n",
    "N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
    "N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
    "N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
    "N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
    "# Pronouns\n",
    "PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
    "PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
    "PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
    "PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
    "PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
    "PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
    "PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
    "PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
    "# Verbs\n",
    "V[SUBCAT = intrans, AGR=[NUM=sg,PER=1]] -> 'komme'\n",
    "V[SUBCAT = intrans, AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
    "V[SUBCAT = intrans, AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
    "V[SUBCAT = intrans, AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
    "V[SUBCAT = intrans, AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
    "V[SUBCAT = intrans, AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
    "\"\"\"\n",
    "\n",
    "gram = grammar.FeatureGrammar.fromstring(g)\n",
    "parser = parse.FeatureEarleyChartParser(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (V[AGR=[NUM='sg', PER=1], OBJCASE='dat', SUBCAT='trans'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n"
     ]
    }
   ],
   "source": [
    "Satz = \"ich folge den Katzen\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[GND='fem', NUM='sg', PER=3], CASE='nom']\n",
      "    (Det[AGR=[GND='fem', NUM='sg', PER=3], CASE='nom'] die)\n",
      "    (N[AGR=[GND='fem', NUM='sg', PER=3]] Katze))\n",
      "  (VP[AGR=[NUM='sg', PER=3]]\n",
      "    (V[AGR=[NUM='sg', PER=3], OBJCASE='acc', SUBCAT='trans'] sieht)\n",
      "    (NP[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc']\n",
      "      (Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] den)\n",
      "      (N[AGR=[GND='masc', NUM='sg', PER=3]] Hund))))\n"
     ]
    }
   ],
   "source": [
    "Satz = \"die Katze sieht den Hund\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail\n",
    "\n",
    "Satz = \"die Katze sieht dem Hund\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[GND='fem', NUM='sg', PER=3], CASE='nom']\n",
      "    (Det[AGR=[GND='fem', NUM='sg', PER=3], CASE='nom'] die)\n",
      "    (N[AGR=[GND='fem', NUM='sg', PER=3]] Katze))\n",
      "  (VP[AGR=[NUM='sg', PER=3]]\n",
      "    (V[AGR=[NUM='sg', PER=3], OBJCASE='dat', SUBCAT='trans'] hilft)\n",
      "    (NP[AGR=[GND='masc', NUM='sg', PER=3], CASE='dat']\n",
      "      (Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='dat'] dem)\n",
      "      (N[AGR=[GND='masc', NUM='sg', PER=3]] Hund))))\n"
     ]
    }
   ],
   "source": [
    "Satz = \"die Katze hilft dem Hund\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail\n",
    "\n",
    "Satz = \"die Katze hilft den Hund\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail\n",
    "\n",
    "Satz = \"ich folge den Katze\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.\n",
    "\n",
    "◑ Develop a feature based grammar that will correctly describe the following Spanish noun phrases:\n",
    "\n",
    "*__N.B.__: The sentences do not render correctly in the HTML version of this chapter, so I had to copy the sentences from the printed book.*\n",
    "\n",
    "(59) \n",
    "\n",
    "* un cuadro hermos-o\n",
    "* INDEF.SG.MASC picture beautiful-SG.MASC\n",
    "* 'a beautiful picture'\n",
    "\n",
    "(60) \n",
    "\n",
    "* un-os cuadro-s hermos-os\n",
    "* INDEF-PL.MASC picture-PL beautiful-PL.MASC\n",
    "* 'beautiful pictures'\n",
    "\n",
    "(61) \n",
    "\n",
    "* un-a cortina hermos-a\n",
    "* INDEF-SG.FEM curtain beautiful-SG.FEM\n",
    "* 'a beautiful curtain'\n",
    "\n",
    "(62) \n",
    "\n",
    "* un-as cortina-s hermos-as\n",
    "* INDEF-PL.FEM curtain beautiful-PL.FEM\n",
    "* 'beautiful curtains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start NP\n",
    "# Grammar Productions\n",
    "\n",
    "NP[AGR=?a] -> INDEF[AGR=?a] N[AGR=?a] ADJ[AGR=?a]\n",
    "\n",
    "# Lexical Productions\n",
    "\n",
    "# Singular determiners\n",
    "INDEF[AGR=[GND=masc, NUM=sg]] -> 'un' \n",
    "INDEF[AGR=[GND=masc, NUM=pl]] -> 'unos' \n",
    "INDEF[AGR=[GND=fem, NUM=sg]] -> 'una' \n",
    "INDEF[AGR=[GND=fem, NUM=pl]] -> 'unas'  \n",
    "\n",
    "# Nouns\n",
    "N[AGR=[GND=masc, NUM=sg]] -> 'cuadro'\n",
    "N[AGR=[GND=masc, NUM=pl]] -> 'cuadros'\n",
    "N[AGR=[GND=fem, NUM=sg]] -> 'cortina'\n",
    "N[AGR=[GND=fem, NUM=pl]] -> 'cortinas'\n",
    "\n",
    "# Adjectives\n",
    "ADJ[AGR=[GND=masc, NUM=sg]] -> 'hermoso'\n",
    "ADJ[AGR=[GND=masc, NUM=pl]] -> 'hermosos'\n",
    "ADJ[AGR=[GND=fem, NUM=sg]] -> 'hermosa'\n",
    "ADJ[AGR=[GND=fem, NUM=pl]] -> 'hermosas'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "gram = grammar.FeatureGrammar.fromstring(g)\n",
    "parser = parse.FeatureEarleyChartParser(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP[AGR=[GND='masc', NUM='sg']]\n",
      "  (INDEF[AGR=[GND='masc', NUM='sg']] un)\n",
      "  (N[AGR=[GND='masc', NUM='sg']] cuadro)\n",
      "  (ADJ[AGR=[GND='masc', NUM='sg']] hermoso))\n"
     ]
    }
   ],
   "source": [
    "frase = \"un cuadro hermoso\".split()\n",
    "for tree in parser.parse(frase):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP[AGR=[GND='masc', NUM='pl']]\n",
      "  (INDEF[AGR=[GND='masc', NUM='pl']] unos)\n",
      "  (N[AGR=[GND='masc', NUM='pl']] cuadros)\n",
      "  (ADJ[AGR=[GND='masc', NUM='pl']] hermosos))\n"
     ]
    }
   ],
   "source": [
    "frase = \"unos cuadros hermosos\".split()\n",
    "for tree in parser.parse(frase):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP[AGR=[GND='fem', NUM='sg']]\n",
      "  (INDEF[AGR=[GND='fem', NUM='sg']] una)\n",
      "  (N[AGR=[GND='fem', NUM='sg']] cortina)\n",
      "  (ADJ[AGR=[GND='fem', NUM='sg']] hermosa))\n"
     ]
    }
   ],
   "source": [
    "frase = \"una cortina hermosa\".split()\n",
    "for tree in parser.parse(frase):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP[AGR=[GND='fem', NUM='pl']]\n",
      "  (INDEF[AGR=[GND='fem', NUM='pl']] unas)\n",
      "  (N[AGR=[GND='fem', NUM='pl']] cortinas)\n",
      "  (ADJ[AGR=[GND='fem', NUM='pl']] hermosas))\n"
     ]
    }
   ],
   "source": [
    "frase = \"unas cortinas hermosas\".split()\n",
    "for tree in parser.parse(frase):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are more impossible parses than possible ones, so I'll just try a couple that should fail:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail\n",
    "frase = \"una cuadro hermosa\".split()\n",
    "for tree in parser.parse(frase):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail\n",
    "frase = \"unos cuadro hermosos\".split()\n",
    "for tree in parser.parse(frase):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "◑ Develop your own version of the `EarleyChartParser` which only prints a trace if the input sequence fails to parse.\n",
    "\n",
    "*The printed book (i.e., not the online version) says we only need to create a wrapper, which is much easier to implement.  That said, the function wasn't that simple to create.  I tried building both the grammar and the parser inside of the function, but this threw errors which I couldn't decipher because the code for the Earley Chart parser is a bit opaque.  As a workaround, I just decided that the grammar and parser had to be pre-defined, and the parser would be passed as one of the arguments for the function:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def check_Earley(seq, parser):\n",
    "    \"\"\"\n",
    "    Checks seq to determine if it would fail to parse.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    seq:    The string to be checked in the parsed grammar\n",
    "    parser: A pre-defined parser\n",
    "    \"\"\"\n",
    "    phrase = seq.split()\n",
    "    if not tree in parser.parse(phrase):\n",
    "        print(\"This sequence fails to parse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sequence fails to parse.\n"
     ]
    }
   ],
   "source": [
    "# This will parse, so nothing is printed\n",
    "check_Earley(\"un cuadro hermoso\", parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sequence fails to parse.\n"
     ]
    }
   ],
   "source": [
    "# This will fail to parse\n",
    "check_Earley(\"un cuadro hermosa\", parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. \n",
    "\n",
    "Consider the feature structures shown below:\n",
    "\n",
    "```\n",
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
    "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
    "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
    "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
    "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")\n",
    "```\n",
    "\n",
    "Work out on paper what the result is of the following unifications. (Hint: you might find it useful to draw the graph structures.)\n",
    "\n",
    "\n",
    "* 1.    `fs1` and `fs2`\n",
    "* 2.    `fs1` and `fs3`\n",
    "* 3.    `fs4` and `fs5`\n",
    "* 4.    `fs5` and `fs6`\n",
    "* 5.    `fs5` and `fs7`\n",
    "* 6.    `fs8` and `fs9`\n",
    "* 7.    `fs8` and `fs10`\n",
    "\n",
    "Check your answers using Python.\n",
    "\n",
    "*Calculating answers with Python is trivial.  However, I'm still fairly unclear regarding Feature Structures and their unifications.  This was the first time I've seen the topic, and the book didn't really dive very deeply into the nuances.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = ?x          ]\n",
      "[                 ]\n",
      "[ B = [ C = ?x  ] ]\n",
      "[     [ D = 'd' ] ]\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "\n",
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
    "\n",
    "print(fs1.unify(fs2))                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = 'd'         ]\n",
      "[                 ]\n",
      "[ B = [ C = 'd' ] ]\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "\n",
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
    "\n",
    "print(fs1.unify(fs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b'  ] ]\n",
      "[ A = (1) [ D = ?x   ] ]\n",
      "[         [ E -> (1) ] ]\n",
      "[         [ F = ?x   ] ]\n",
      "[                      ]\n",
      "[ C -> (1)             ]\n"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "\n",
    "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "\n",
    "print(fs4.unify(fs5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = (1) [ D = 'd' ] ]\n",
      "[                     ]\n",
      "[ C = [ E -> (1) ]    ]\n",
      "[     [ F = 'd'  ]    ]\n"
     ]
    }
   ],
   "source": [
    "# 4.\n",
    "\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
    "\n",
    "print(fs5.unify(fs6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5. \n",
    "\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
    "\n",
    "print(fs5.unify(fs7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b' ] ]\n",
      "[ A = (1) [ D = 'e' ] ]\n",
      "[         [ G = 'e' ] ]\n",
      "[                     ]\n",
      "[ C = [ B = 'e'  ]    ]\n",
      "[     [ E -> (1) ]    ]\n"
     ]
    }
   ],
   "source": [
    "# 6.\n",
    "\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
    "\n",
    "print(fs8.unify(fs9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b'  ] ]\n",
      "[ A = (1) [ D = 'b'  ] ]\n",
      "[         [ E -> (1) ] ]\n",
      "[         [ G = 'b'  ] ]\n",
      "[                      ]\n",
      "[ C -> (1)             ]\n"
     ]
    }
   ],
   "source": [
    "# 7. \n",
    "\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")\n",
    "\n",
    "print(fs8.unify(fs10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.\n",
    "\n",
    "◑ List two feature structures that subsume `[A=?x, B=?x].`\n",
    "\n",
    "##### 10.\n",
    "\n",
    "◑ Ignoring structure sharing, give an informal algorithm for unifying two feature structures.\n",
    "\n",
    "*I'm indefinitely skipping these questions.  I'm more interested in general Python applications that can be used in other domains than in linguistic theory, and these questions seem too theoretical to be useful for me now.  Furthermore, as I mentioned earlier, I'm not totally satisfied with the book's presentation of the topic, and I don't feel I'm able to answer question without doing outside research that I don't have time for now.*\n",
    "\n",
    "\n",
    "##### 11.\n",
    "\n",
    "◑ Extend the German grammar in [3.2](https://www.nltk.org/book/ch09.html#code-germancfg) so that it can handle so-called verb-second structures like the following:\n",
    "\n",
    "(58)\t\t\n",
    "\n",
    "Heute sieht der Hund die Katze.\n",
    "\n",
    "*This took a bit of futzing around to get right.  I was able to get the parser to parse the sentence by adding a new `VP` production at line 13.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start S\n",
    "# Grammar Productions\n",
    "S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
    "S -> ADV VP[AGR=?a] \n",
    "\n",
    "NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
    "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
    "VP[AGR=?a] -> V[SUBCAT = intrans, AGR=?a]\n",
    "VP[AGR=?a] -> V[SUBCAT = trans, OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
    "\n",
    "# New VP production\n",
    "VP[AGR=?a] -> V[SUBCAT = trans, OBJCASE=?c, AGR=?a] NP[CASE=nom, AGR = ?a] NP[CASE=?c]\n",
    "\n",
    "# Lexical Productions\n",
    "# Singular determiners\n",
    "# masc\n",
    "Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der' \n",
    "Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
    "Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
    "# fem\n",
    "Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
    "Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
    "Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die' \n",
    "# Plural determiners\n",
    "Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die' \n",
    "Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den' \n",
    "Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die' \n",
    "# Nouns\n",
    "N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
    "N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
    "N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
    "N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
    "# Pronouns\n",
    "PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
    "PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
    "PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
    "PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
    "PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
    "PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
    "PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
    "PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
    "# Verbs\n",
    "V[SUBCAT = intrans, AGR=[NUM=sg,PER=1]] -> 'komme'\n",
    "V[SUBCAT = intrans, AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
    "V[SUBCAT = intrans, AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
    "V[SUBCAT = intrans, AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
    "V[SUBCAT = intrans, AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
    "V[SUBCAT = intrans, AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
    "V[SUBCAT = trans, OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
    "V[SUBCAT = trans, OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
    "# Adverbs\n",
    "ADV -> 'Heute'\n",
    "\"\"\"\n",
    "\n",
    "gram = grammar.FeatureGrammar.fromstring(g)\n",
    "parser = parse.FeatureEarleyChartParser(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (ADV[] Heute)\n",
      "  (VP[AGR=[GND='masc', NUM='sg', PER=3]]\n",
      "    (V[AGR=[NUM='sg', PER=3], OBJCASE='acc', SUBCAT='trans'] sieht)\n",
      "    (NP[AGR=[GND='masc', NUM='sg', PER=3], CASE='nom']\n",
      "      (Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='nom'] der)\n",
      "      (N[AGR=[GND='masc', NUM='sg', PER=3]] Hund))\n",
      "    (NP[AGR=[GND='fem', NUM='sg', PER=3], CASE='acc']\n",
      "      (Det[AGR=[GND='fem', NUM='sg', PER=3], CASE='acc'] die)\n",
      "      (N[AGR=[GND='fem', NUM='sg', PER=3]] Katze))))\n"
     ]
    }
   ],
   "source": [
    "Satz = \"Heute sieht der Hund die Katze\".split()\n",
    "for tree in parser.parse(Satz):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. \n",
    "\n",
    "◑ Seemingly synonymous verbs have slightly different syntactic properties (Levin, 1993). Consider the patterns of grammaticality for the verbs *loaded*, *filled*, and *dumped* below. Can you write grammar productions to handle such data?\n",
    "\n",
    "(59)\t\t\n",
    "\n",
    "* a. The farmer *loaded* the cart with sand\n",
    "\n",
    "* b. The farmer *loaded* sand into the cart\n",
    "\n",
    "* c. The farmer *filled* the cart with sand\n",
    "\n",
    "* d. \\*The farmer *filled* sand into the cart\n",
    "\n",
    "* e. \\*The farmer *dumped* the cart with sand\n",
    "\n",
    "* f. The farmer *dumped* sand into the cart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To the verbs I'll add feature structures indicating which prepositions go with which verbs.  I'll also create one production of `VP`s for each preposition:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start S\n",
    "# Grammar Productions\n",
    "S -> NP VP\n",
    "NP -> N[Count = False]\n",
    "NP -> Det N[Count = ?c]\n",
    "VP -> V[Prep = With] NP 'with' NP\n",
    "VP -> V[Prep = Into] NP 'into' NP\n",
    "\n",
    "# Lexical Productions\n",
    "# Determiner\n",
    "Det -> 'The' | 'the' \n",
    "# Nouns\n",
    "N[Count = True] -> 'farmer' | 'cart'\n",
    "N[Count = False] -> 'sand'\n",
    "# Verbs\n",
    "V[Prep = With] -> 'loaded' | 'filled'\n",
    "V[Prep = Into] -> 'loaded' | 'dumped'\n",
    "\"\"\"\n",
    "\n",
    "gram = grammar.FeatureGrammar.fromstring(g)\n",
    "parser = parse.FeatureEarleyChartParser(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[] (Det[] The) (N[+Count] farmer))\n",
      "  (VP[]\n",
      "    (V[Prep='With'] loaded)\n",
      "    (NP[] (Det[] the) (N[+Count] cart))\n",
      "    with\n",
      "    (NP[] (N[-Count] sand))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The farmer loaded the cart with sand\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[] (Det[] The) (N[+Count] farmer))\n",
      "  (VP[]\n",
      "    (V[Prep='Into'] loaded)\n",
      "    (NP[] (N[-Count] sand))\n",
      "    into\n",
      "    (NP[] (Det[] the) (N[+Count] cart))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The farmer loaded sand into the cart\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[] (Det[] The) (N[+Count] farmer))\n",
      "  (VP[]\n",
      "    (V[Prep='With'] filled)\n",
      "    (NP[] (Det[] the) (N[+Count] cart))\n",
      "    with\n",
      "    (NP[] (N[-Count] sand))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The farmer filled the cart with sand\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should fail\n",
    "\n",
    "sent = \"The farmer filled sand into the cart\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should fail\n",
    "\n",
    "sent = \"The farmer dumped the cart with sand\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[] (Det[] The) (N[+Count] farmer))\n",
      "  (VP[]\n",
      "    (V[Prep='Into'] dumped)\n",
      "    (NP[] (N[-Count] sand))\n",
      "    into\n",
      "    (NP[] (Det[] the) (N[+Count] cart))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The farmer dumped sand into the cart\".split()\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I'll come back to the remaining questions, time permitting...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
