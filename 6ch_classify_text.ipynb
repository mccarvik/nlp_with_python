{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 6\n",
    "\n",
    "## Learning to Classify Text\n",
    "\n",
    "*The html version of this chapter in the book is available [here](https://www.nltk.org/book/ch06.html \"ch06\").*\n",
    "\n",
    "### 1 Supervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\supervised-classification.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Gender Identification\n",
    "\n",
    "*Guessing the gender of a name based on the last letter:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, random\n",
    "from nltk.corpus import names\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating feature, training, and test sets:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Neo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Trinity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finding accuracy:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most informative features:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.7 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.1 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.8 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.8 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__: Modify the `gender_features()` function to provide the classifier with features encoding the length of the name, its first letter, and any other features that seem like they might be informative. Retrain the classifier with these new features, and test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1],\n",
    "            'length'     : len(word)}\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter' : word[-1],\n",
    "            'length'      : len(word),\n",
    "            \"first_letter\": word[0]}\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            \"first_letter\" : word[0],\n",
    "            \"number_vowels\": sum([1 for ch in word if ch in 'AEIOUaeiouy'])}\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It appears that counting the number of vowels confuses the classifier slightly...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.7 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.1 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.8 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.8 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.1 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.7 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.3 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.2 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.6 : 1.0\n",
      "             last_letter = 'g'              male : female =      5.4 : 1.0\n",
      "             last_letter = 'w'              male : female =      4.7 : 1.0\n",
      "            first_letter = 'W'              male : female =      4.7 : 1.0\n",
      "             last_letter = 's'              male : female =      4.4 : 1.0\n",
      "             last_letter = 't'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'z'              male : female =      3.9 : 1.0\n",
      "           number_vowels = 1                male : female =      3.9 : 1.0\n",
      "             last_letter = 'b'              male : female =      3.8 : 1.0\n",
      "             last_letter = 'i'            female : male   =      3.6 : 1.0\n",
      "            first_letter = 'Q'              male : female =      3.3 : 1.0\n",
      "             last_letter = 'u'              male : female =      3.2 : 1.0\n",
      "           number_vowels = 5              female : male   =      3.2 : 1.0\n",
      "                  length = 2                male : female =      2.6 : 1.0\n",
      "            first_letter = 'U'              male : female =      2.6 : 1.0\n",
      "           number_vowels = 4              female : male   =      2.5 : 1.0\n",
      "            first_letter = 'K'            female : male   =      2.5 : 1.0\n",
      "            first_letter = 'H'              male : female =      2.4 : 1.0\n",
      "            first_letter = 'X'              male : female =      2.3 : 1.0\n",
      "             last_letter = 'n'              male : female =      2.1 : 1.0\n",
      "                  length = 3                male : female =      1.9 : 1.0\n",
      "             last_letter = 'e'            female : male   =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:]}\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2]}\n",
    "            \n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2],\n",
    "            'first_vowel'  : [i for i in range(len(word)) \n",
    "                              if word[i] in 'AEIOUaeiouy'][0]}\n",
    "            \n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'second_letter': word[1],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2],\n",
    "            'first_vowel'  : [i for i in range(len(word)) \n",
    "                              if word[i] in 'AEIOUaeiouy'][0]}\n",
    "            \n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The position of the first vowel also seems to confuse the classifier. Saving a list with all the features may take up a large amount of memory.  `nltk.classify.apply_features` returns an object that acts like a list but does not sotre all the feature sets in memory:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import apply_features\n",
    "train_set = apply_features(gender_features, labeled_names[500:])\n",
    "test_set = apply_features(gender_features, labeled_names[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Choosing the Right Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_letter': 'j', 'last_letter': 'n', 'count(a)': 0, 'has(a)': False, 'count(b)': 0, 'has(b)': False, 'count(c)': 0, 'has(c)': False, 'count(d)': 0, 'has(d)': False, 'count(e)': 0, 'has(e)': False, 'count(f)': 0, 'has(f)': False, 'count(g)': 0, 'has(g)': False, 'count(h)': 1, 'has(h)': True, 'count(i)': 0, 'has(i)': False, 'count(j)': 1, 'has(j)': True, 'count(k)': 0, 'has(k)': False, 'count(l)': 0, 'has(l)': False, 'count(m)': 0, 'has(m)': False, 'count(n)': 1, 'has(n)': True, 'count(o)': 1, 'has(o)': True, 'count(p)': 0, 'has(p)': False, 'count(q)': 0, 'has(q)': False, 'count(r)': 0, 'has(r)': False, 'count(s)': 0, 'has(s)': False, 'count(t)': 0, 'has(t)': False, 'count(u)': 0, 'has(u)': False, 'count(v)': 0, 'has(v)': False, 'count(w)': 0, 'has(w)': False, 'count(x)': 0, 'has(x)': False, 'count(y)': 0, 'has(y)': False, 'count(z)': 0, 'has(z)': False}"
     ]
    }
   ],
   "source": [
    "print(gender_features2('John'), end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This classifier actually performs worse.  Having too many features will cause the classifier to __overfit__ to the training set, and it will thus have problems generalizing to the observations in the test set:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One way to refine the feature is __error analysis__.  We'll make a __development set__, which contains the __training set__ and the __dev-test__ set.  The __dev-test__ has to be kept separate from the test set:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\corpus-org.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Figure 1.3:__ Organization of corpus data for training supervised classifiers. The corpus data is divided into two sets: the development set, and the test set. The development set is often further subdivided into a training set and a dev-test set.*\n",
    "\n",
    "*Compiling the errors:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can then examine the errors:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female   guess=male     name=Abagael                       \n",
      "correct=female   guess=male     name=Abby                          \n",
      "correct=female   guess=male     name=Ajay                          \n",
      "correct=female   guess=male     name=Alisun                        \n",
      "correct=female   guess=male     name=Allsun                        \n",
      "correct=female   guess=male     name=Amargo                        \n",
      "correct=female   guess=male     name=Arden                         \n",
      "correct=female   guess=male     name=Bamby                         \n",
      "correct=female   guess=male     name=Beilul                        \n",
      "correct=female   guess=male     name=Bobby                         \n",
      "correct=female   guess=male     name=Bren                          \n",
      "correct=female   guess=male     name=Cam                           \n",
      "correct=female   guess=male     name=Charin                        \n",
      "correct=female   guess=male     name=Chloris                       \n",
      "correct=female   guess=male     name=Chris                         \n",
      "correct=female   guess=male     name=Christan                      \n",
      "correct=female   guess=male     name=Clo                           \n",
      "correct=female   guess=male     name=Cloris                        \n",
      "correct=female   guess=male     name=Cybil                         \n",
      "correct=female   guess=male     name=Danell                        \n",
      "correct=female   guess=male     name=Demeter                       \n",
      "correct=female   guess=male     name=Farand                        \n",
      "correct=female   guess=male     name=Floris                        \n",
      "correct=female   guess=male     name=France                        \n",
      "correct=female   guess=male     name=Gabriell                      \n",
      "correct=female   guess=male     name=Gayleen                       \n",
      "correct=female   guess=male     name=Gaynor                        \n",
      "correct=female   guess=male     name=George                        \n",
      "correct=female   guess=male     name=Gray                          \n",
      "correct=female   guess=male     name=Grethel                       \n",
      "correct=female   guess=male     name=Guendolen                     \n",
      "correct=female   guess=male     name=Guenevere                     \n",
      "correct=female   guess=male     name=Gunvor                        \n",
      "correct=female   guess=male     name=Gusty                         \n",
      "correct=female   guess=male     name=Halie                         \n",
      "correct=female   guess=male     name=Halley                        \n",
      "correct=female   guess=male     name=Harmony                       \n",
      "correct=female   guess=male     name=Harrie                        \n",
      "correct=female   guess=male     name=Harriet                       \n",
      "correct=female   guess=male     name=Hedvig                        \n",
      "correct=female   guess=male     name=Helen                         \n",
      "correct=female   guess=male     name=Helyn                         \n",
      "correct=female   guess=male     name=Hephzibah                     \n",
      "correct=female   guess=male     name=Hester                        \n",
      "correct=female   guess=male     name=Hyacinth                      \n",
      "correct=female   guess=male     name=Imojean                       \n",
      "correct=female   guess=male     name=Jen                           \n",
      "correct=female   guess=male     name=Jess                          \n",
      "correct=female   guess=male     name=Jewel                         \n",
      "correct=female   guess=male     name=Jewell                        \n",
      "correct=female   guess=male     name=Jonell                        \n",
      "correct=female   guess=male     name=Kerrill                       \n",
      "correct=female   guess=male     name=Kit                           \n",
      "correct=female   guess=male     name=Kym                           \n",
      "correct=female   guess=male     name=Lark                          \n",
      "correct=female   guess=male     name=Leanor                        \n",
      "correct=female   guess=male     name=Marget                        \n",
      "correct=female   guess=male     name=Margot                        \n",
      "correct=female   guess=male     name=Marlo                         \n",
      "correct=female   guess=male     name=Meagan                        \n",
      "correct=female   guess=male     name=Meghan                        \n",
      "correct=female   guess=male     name=Melicent                      \n",
      "correct=female   guess=male     name=Melisent                      \n",
      "correct=female   guess=male     name=Miriam                        \n",
      "correct=female   guess=male     name=Mureil                        \n",
      "correct=female   guess=male     name=Myriam                        \n",
      "correct=female   guess=male     name=Nat                           \n",
      "correct=female   guess=male     name=Nert                          \n",
      "correct=female   guess=male     name=Nichol                        \n",
      "correct=female   guess=male     name=Nicky                         \n",
      "correct=female   guess=male     name=Oliy                          \n",
      "correct=female   guess=male     name=Orel                          \n",
      "correct=female   guess=male     name=Page                          \n",
      "correct=female   guess=male     name=Pen                           \n",
      "correct=female   guess=male     name=Perl                          \n",
      "correct=female   guess=male     name=Prue                          \n",
      "correct=female   guess=male     name=Quinn                         \n",
      "correct=female   guess=male     name=Ricky                         \n",
      "correct=female   guess=male     name=Rivkah                        \n",
      "correct=female   guess=male     name=Rory                          \n",
      "correct=female   guess=male     name=Rubie                         \n",
      "correct=female   guess=male     name=Ruby                          \n",
      "correct=female   guess=male     name=Sal                           \n",
      "correct=female   guess=male     name=Shay                          \n",
      "correct=female   guess=male     name=Shelby                        \n",
      "correct=female   guess=male     name=Sibeal                        \n",
      "correct=female   guess=male     name=Terri-Jo                      \n",
      "correct=female   guess=male     name=Tierney                       \n",
      "correct=female   guess=male     name=Tim                           \n",
      "correct=female   guess=male     name=Tomiko                        \n",
      "correct=female   guess=male     name=Trudy                         \n",
      "correct=female   guess=male     name=Tuesday                       \n",
      "correct=female   guess=male     name=Veradis                       \n",
      "correct=female   guess=male     name=Wendie                        \n",
      "correct=female   guess=male     name=Willyt                        \n",
      "correct=female   guess=male     name=Wilow                         \n",
      "correct=female   guess=male     name=Windy                         \n",
      "correct=female   guess=male     name=Winnie                        \n",
      "correct=female   guess=male     name=Wynne                         \n",
      "correct=female   guess=male     name=Yehudit                       \n",
      "correct=male     guess=female   name=Antonin                       \n",
      "correct=male     guess=female   name=Arne                          \n",
      "correct=male     guess=female   name=Ashby                         \n",
      "correct=male     guess=female   name=Aubrey                        \n",
      "correct=male     guess=female   name=Barnie                        \n",
      "correct=male     guess=female   name=Barrie                        \n",
      "correct=male     guess=female   name=Berkie                        \n",
      "correct=male     guess=female   name=Berkley                       \n",
      "correct=male     guess=female   name=Boniface                      \n",
      "correct=male     guess=female   name=Bradly                        \n",
      "correct=male     guess=female   name=Carey                         \n",
      "correct=male     guess=female   name=Chane                         \n",
      "correct=male     guess=female   name=Christie                      \n",
      "correct=male     guess=female   name=Clayborne                     \n",
      "correct=male     guess=female   name=Corey                         \n",
      "correct=male     guess=female   name=Corky                         \n",
      "correct=male     guess=female   name=Costa                         \n",
      "correct=male     guess=female   name=Courtney                      \n",
      "correct=male     guess=female   name=Dani                          \n",
      "correct=male     guess=female   name=Danny                         \n",
      "correct=male     guess=female   name=Daren                         \n",
      "correct=male     guess=female   name=Daryl                         \n",
      "correct=male     guess=female   name=Deryl                         \n",
      "correct=male     guess=female   name=Dickey                        \n",
      "correct=male     guess=female   name=Donny                         \n",
      "correct=male     guess=female   name=Douglass                      \n",
      "correct=male     guess=female   name=Dwaine                        \n",
      "correct=male     guess=female   name=Dwayne                        \n",
      "correct=male     guess=female   name=Ellis                         \n",
      "correct=male     guess=female   name=Elvin                         \n",
      "correct=male     guess=female   name=Evan                          \n",
      "correct=male     guess=female   name=Felix                         \n",
      "correct=male     guess=female   name=Filmore                       \n",
      "correct=male     guess=female   name=Finley                        \n",
      "correct=male     guess=female   name=Gayle                         \n",
      "correct=male     guess=female   name=Jean-Christophe               \n",
      "correct=male     guess=female   name=Jeramie                       \n",
      "correct=male     guess=female   name=Jere                          \n",
      "correct=male     guess=female   name=Jody                          \n",
      "correct=male     guess=female   name=Joe                           \n",
      "correct=male     guess=female   name=Jonah                         \n",
      "correct=male     guess=female   name=Karsten                       \n",
      "correct=male     guess=female   name=Keene                         \n",
      "correct=male     guess=female   name=Kelly                         \n",
      "correct=male     guess=female   name=Klee                          \n",
      "correct=male     guess=female   name=Lawrence                      \n",
      "correct=male     guess=female   name=Lemuel                        \n",
      "correct=male     guess=female   name=Leslie                        \n",
      "correct=male     guess=female   name=Levi                          \n",
      "correct=male     guess=female   name=Lonnie                        \n",
      "correct=male     guess=female   name=Lorrie                        \n",
      "correct=male     guess=female   name=Manuel                        \n",
      "correct=male     guess=female   name=Marshal                       \n",
      "correct=male     guess=female   name=Martainn                      \n",
      "correct=male     guess=female   name=Marten                        \n",
      "correct=male     guess=female   name=Martie                        \n",
      "correct=male     guess=female   name=Matthieu                      \n",
      "correct=male     guess=female   name=Michale                       \n",
      "correct=male     guess=female   name=Mischa                        \n",
      "correct=male     guess=female   name=Moise                         \n",
      "correct=male     guess=female   name=Monty                         \n",
      "correct=male     guess=female   name=Morly                         \n",
      "correct=male     guess=female   name=Morrie                        \n",
      "correct=male     guess=female   name=Morse                         \n",
      "correct=male     guess=female   name=Morty                         \n",
      "correct=male     guess=female   name=Nate                          \n",
      "correct=male     guess=female   name=Nathanial                     \n",
      "correct=male     guess=female   name=Nikolai                       \n",
      "correct=male     guess=female   name=Partha                        \n",
      "correct=male     guess=female   name=Patrice                       \n",
      "correct=male     guess=female   name=Patty                         \n",
      "correct=male     guess=female   name=Randi                         \n",
      "correct=male     guess=female   name=Reube                         \n",
      "correct=male     guess=female   name=Rodrique                      \n",
      "correct=male     guess=female   name=Roscoe                        \n",
      "correct=male     guess=female   name=Roth                          \n",
      "correct=male     guess=female   name=Salomone                      \n",
      "correct=male     guess=female   name=Sammie                        \n",
      "correct=male     guess=female   name=Sansone                       \n",
      "correct=male     guess=female   name=Sascha                        \n",
      "correct=male     guess=female   name=Scotti                        \n",
      "correct=male     guess=female   name=Shelley                       \n",
      "correct=male     guess=female   name=Sidnee                        \n",
      "correct=male     guess=female   name=Simone                        \n",
      "correct=male     guess=female   name=Sonnie                        \n",
      "correct=male     guess=female   name=Sully                         \n",
      "correct=male     guess=female   name=Tabbie                        \n",
      "correct=male     guess=female   name=Terrence                      \n",
      "correct=male     guess=female   name=Terri                         \n",
      "correct=male     guess=female   name=Toddie                        \n",
      "correct=male     guess=female   name=Uli                           \n",
      "correct=male     guess=female   name=Vasili                        \n",
      "correct=male     guess=female   name=Vasily                        \n",
      "correct=male     guess=female   name=Verne                         \n"
     ]
    }
   ],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct={:<8} guess={:<8} name={:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I noticed doubled letters are quite common in the mistaken names, so I decided to make that a feature:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2],\n",
    "            'first_vowel'  : [i for i in range(len(word)) \n",
    "                              if word[i] in 'AEIOUaeiouy'][0],\n",
    "            'double_letters': sum([1 for ch in range(len(word) - 1)\n",
    "                                  if word[ch] == word[ch + 1]])}\n",
    "            \n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Document Classification\n",
    "\n",
    "*Looking at movie reviews to build a classifier to determine if a review is positive or negative:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Making a feature extractor.  We'll make a list of the 2,000 most common words in the corpus, and then check for the presence of these words in a given document:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)]  = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(,)': True, 'contains(the)': True, 'contains(.)': True, 'contains(a)': True, 'contains(and)': True, 'contains(of)': True, 'contains(to)': True, \"contains(')\": True, 'contains(is)': True, 'contains(in)': True, 'contains(s)': True, 'contains(\")': True, 'contains(it)': True, 'contains(that)': True, 'contains(-)': True, 'contains())': True, 'contains(()': True, 'contains(as)': True, 'contains(with)': True, 'contains(for)': True, 'contains(his)': True, 'contains(this)': True, 'contains(film)': False, 'contains(i)': False, 'contains(he)': True, 'contains(but)': True, 'contains(on)': True, 'contains(are)': True, 'contains(t)': False, 'contains(by)': True, 'contains(be)': True, 'contains(one)': True, 'contains(movie)': True, 'contains(an)': True, 'contains(who)': True, 'contains(not)': True, 'contains(you)': True, 'contains(from)': True, 'contains(at)': False, 'contains(was)': False, 'contains(have)': True, 'contains(they)': True, 'contains(has)': True, 'contains(her)': False, 'contains(all)': True, 'contains(?)': False, 'contains(there)': True, 'contains(like)': True, 'contains(so)': False, 'contains(out)': True, 'contains(about)': True, 'contains(up)': False, 'contains(more)': False, 'contains(what)': True, 'contains(when)': True, 'contains(which)': True, 'contains(or)': False, 'contains(she)': True, 'contains(their)': False, 'contains(:)': True, 'contains(some)': False, 'contains(just)': True, 'contains(can)': False, 'contains(if)': True, 'contains(we)': False, 'contains(him)': True, 'contains(into)': True, 'contains(even)': False, 'contains(only)': True, 'contains(than)': False, 'contains(no)': False, 'contains(good)': False, 'contains(time)': False, 'contains(most)': True, 'contains(its)': False, 'contains(will)': True, 'contains(story)': False, 'contains(would)': False, 'contains(been)': False, 'contains(much)': False, 'contains(character)': False, 'contains(also)': True, 'contains(get)': True, 'contains(other)': True, 'contains(do)': True, 'contains(two)': True, 'contains(well)': True, 'contains(them)': True, 'contains(very)': True, 'contains(characters)': False, 'contains(;)': False, 'contains(first)': False, 'contains(--)': True, 'contains(after)': False, 'contains(see)': False, 'contains(!)': True, 'contains(way)': True, 'contains(because)': False, 'contains(make)': True, 'contains(life)': False, 'contains(off)': False, 'contains(too)': False, 'contains(any)': False, 'contains(does)': False, 'contains(really)': False, 'contains(had)': False, 'contains(while)': True, 'contains(films)': False, 'contains(how)': True, 'contains(plot)': True, 'contains(little)': True, 'contains(where)': True, 'contains(people)': False, 'contains(over)': False, 'contains(could)': False, 'contains(then)': True, 'contains(me)': True, 'contains(scene)': True, 'contains(man)': False, 'contains(bad)': False, 'contains(my)': False, 'contains(never)': True, 'contains(being)': False, 'contains(best)': True, 'contains(these)': False, 'contains(don)': False, 'contains(new)': False, 'contains(doesn)': False, 'contains(scenes)': False, 'contains(many)': True, 'contains(director)': False, 'contains(such)': False, 'contains(know)': False, 'contains(were)': False, 'contains(movies)': True, 'contains(through)': False, 'contains(here)': True, 'contains(action)': True, 'contains(great)': True, 'contains(re)': True, 'contains(another)': False, 'contains(love)': False, 'contains(go)': False, 'contains(made)': False, 'contains(us)': True, 'contains(big)': False, 'contains(end)': False, 'contains(something)': False, 'contains(back)': False, 'contains(*)': True, 'contains(still)': False, 'contains(world)': True, 'contains(seems)': False, 'contains(work)': False, 'contains(those)': False, 'contains(makes)': False, 'contains(now)': False, 'contains(before)': False, 'contains(however)': True, 'contains(between)': True, 'contains(few)': False, 'contains(/)': False, 'contains(down)': False, 'contains(every)': False, 'contains(though)': False, 'contains(better)': False, 'contains(real)': False, 'contains(audience)': False, 'contains(enough)': False, 'contains(seen)': False, 'contains(take)': False, 'contains(around)': False, 'contains(both)': False, 'contains(going)': False, 'contains(year)': False, 'contains(performance)': False, 'contains(why)': False, 'contains(should)': False, 'contains(role)': False, 'contains(isn)': False, 'contains(same)': True, 'contains(old)': False, 'contains(gets)': True, 'contains(your)': False, 'contains(may)': False, 'contains(things)': True, 'contains(think)': False, 'contains(years)': False, 'contains(last)': False, 'contains(comedy)': True, 'contains(funny)': True, 'contains(actually)': True, 'contains(ve)': False, 'contains(long)': False, 'contains(look)': True, 'contains(almost)': False, 'contains(own)': True, 'contains(thing)': False, 'contains(fact)': False, 'contains(nothing)': False, 'contains(say)': False, 'contains(right)': False, 'contains(john)': False, 'contains(although)': False, 'contains(played)': True, 'contains(find)': False, 'contains(script)': False, 'contains(come)': False, 'contains(ever)': True, 'contains(cast)': False, 'contains(since)': False, 'contains(did)': False, 'contains(star)': False, 'contains(plays)': False, 'contains(young)': False, 'contains(show)': False, 'contains(comes)': False, 'contains(m)': False, 'contains(part)': False, 'contains(original)': False, 'contains(actors)': False, 'contains(screen)': True, 'contains(without)': False, 'contains(again)': False, 'contains(acting)': False, 'contains(three)': False, 'contains(day)': True, 'contains(each)': True, 'contains(point)': False, 'contains(lot)': False, 'contains(least)': True, 'contains(takes)': False, 'contains(guy)': True, 'contains(quite)': False, 'contains(himself)': False, 'contains(away)': False, 'contains(during)': False, 'contains(family)': False, 'contains(effects)': False, 'contains(course)': True, 'contains(goes)': False, 'contains(minutes)': False, 'contains(interesting)': False, 'contains(might)': False, 'contains(far)': False, 'contains(high)': False, 'contains(rather)': False, 'contains(once)': True, 'contains(must)': False, 'contains(anything)': False, 'contains(place)': True, 'contains(set)': False, 'contains(yet)': False, 'contains(watch)': True, 'contains(d)': False, 'contains(making)': True, 'contains(our)': False, 'contains(wife)': True, 'contains(hard)': False, 'contains(always)': False, 'contains(fun)': True, 'contains(didn)': False, 'contains(ll)': False, 'contains(seem)': False, 'contains(special)': False, 'contains(bit)': False, 'contains(times)': False, 'contains(trying)': False, 'contains(hollywood)': False, 'contains(instead)': False, 'contains(give)': False, 'contains(want)': False, 'contains(picture)': False, 'contains(kind)': True, 'contains(american)': False, 'contains(job)': False, 'contains(sense)': False, 'contains(woman)': True, 'contains(home)': False, 'contains(having)': False, 'contains(series)': True, 'contains(actor)': False, 'contains(probably)': False, 'contains(help)': True, 'contains(half)': False, 'contains(along)': True, 'contains(men)': False, 'contains(everything)': True, 'contains(pretty)': False, 'contains(becomes)': False, 'contains(sure)': False, 'contains(black)': False, 'contains(together)': False, 'contains(dialogue)': False, 'contains(money)': False, 'contains(become)': False, 'contains(gives)': False, 'contains(given)': False, 'contains(looking)': False, 'contains(whole)': False, 'contains(watching)': False, 'contains(father)': False, 'contains(`)': False, 'contains(feel)': False, 'contains(everyone)': False, 'contains(music)': False, 'contains(wants)': False, 'contains(sex)': False, 'contains(less)': False, 'contains(done)': False, 'contains(horror)': False, 'contains(got)': True, 'contains(death)': False, 'contains(perhaps)': False, 'contains(city)': False, 'contains(next)': False, 'contains(especially)': True, 'contains(play)': False, 'contains(girl)': False, 'contains(mind)': False, 'contains(10)': False, 'contains(moments)': False, 'contains(looks)': True, 'contains(completely)': False, 'contains(2)': False, 'contains(reason)': False, 'contains(mother)': False, 'contains(whose)': False, 'contains(line)': False, 'contains(night)': False, 'contains(human)': False, 'contains(until)': False, 'contains(rest)': False, 'contains(performances)': False, 'contains(different)': False, 'contains(evil)': False, 'contains(small)': False, 'contains(james)': False, 'contains(simply)': False, 'contains(couple)': False, 'contains(put)': False, 'contains(let)': False, 'contains(anyone)': False, 'contains(ending)': False, 'contains(case)': False, 'contains(several)': False, 'contains(dead)': False, 'contains(michael)': False, 'contains(left)': False, 'contains(thought)': False, 'contains(school)': False, 'contains(shows)': False, 'contains(humor)': False, 'contains(true)': False, 'contains(lost)': False, 'contains(written)': False, 'contains(itself)': False, 'contains(friend)': False, 'contains(entire)': False, 'contains(getting)': True, 'contains(town)': False, 'contains(turns)': False, 'contains(soon)': False, 'contains(someone)': False, 'contains(second)': False, 'contains(main)': False, 'contains(stars)': False, 'contains(found)': False, 'contains(use)': False, 'contains(problem)': False, 'contains(friends)': True, 'contains(tv)': False, 'contains(top)': True, 'contains(name)': False, 'contains(begins)': False, 'contains(called)': False, 'contains(based)': False, 'contains(comic)': False, 'contains(david)': False, 'contains(head)': False, 'contains(else)': False, 'contains(idea)': True, 'contains(either)': False, 'contains(wrong)': True, 'contains(unfortunately)': False, 'contains(later)': False, 'contains(final)': False, 'contains(hand)': False, 'contains(alien)': False, 'contains(house)': False, 'contains(group)': False, 'contains(full)': False, 'contains(used)': True, 'contains(tries)': True, 'contains(often)': True, 'contains(against)': False, 'contains(war)': False, 'contains(sequence)': False, 'contains(keep)': False, 'contains(turn)': False, 'contains(playing)': True, 'contains(boy)': False, 'contains(behind)': False, 'contains(named)': False, 'contains(certainly)': False, 'contains(live)': False, 'contains(believe)': False, 'contains(under)': False, 'contains(works)': False, 'contains(relationship)': False, 'contains(face)': False, 'contains(hour)': False, 'contains(run)': False, 'contains(style)': False, 'contains(said)': False, 'contains(despite)': False, 'contains(person)': False, 'contains(finally)': False, 'contains(shot)': False, 'contains(book)': False, 'contains(doing)': False, 'contains(tell)': False, 'contains(maybe)': False, 'contains(nice)': False, 'contains(son)': False, 'contains(perfect)': False, 'contains(side)': False, 'contains(seeing)': True, 'contains(able)': False, 'contains(finds)': False, 'contains(children)': False, 'contains(days)': False, 'contains(past)': False, 'contains(summer)': False, 'contains(camera)': False, 'contains(won)': False, 'contains(including)': False, 'contains(mr)': False, 'contains(kids)': False, 'contains(lives)': False, 'contains(directed)': False, 'contains(moment)': False, 'contains(game)': False, 'contains(running)': False, 'contains(fight)': True, 'contains(supposed)': False, 'contains(video)': False, 'contains(car)': False, 'contains(matter)': False, 'contains(kevin)': True, 'contains(joe)': False, 'contains(lines)': False, 'contains(worth)': True, 'contains(=)': False, 'contains(daughter)': False, 'contains(earth)': False, 'contains(starts)': False, 'contains(need)': False, 'contains(entertaining)': False, 'contains(white)': False, 'contains(start)': True, 'contains(writer)': False, 'contains(dark)': False, 'contains(short)': False, 'contains(self)': False, 'contains(worst)': False, 'contains(nearly)': False, 'contains(opening)': False, 'contains(try)': False, 'contains(upon)': False, 'contains(care)': False, 'contains(early)': True, 'contains(violence)': False, 'contains(throughout)': False, 'contains(team)': False, 'contains(production)': False, 'contains(example)': False, 'contains(beautiful)': False, 'contains(title)': False, 'contains(exactly)': False, 'contains(jack)': False, 'contains(review)': False, 'contains(major)': False, 'contains(drama)': False, 'contains(&)': False, 'contains(problems)': True, 'contains(sequences)': False, 'contains(obvious)': False, 'contains(version)': False, 'contains(screenplay)': False, 'contains(known)': True, 'contains(killer)': False, 'contains(wasn)': False, 'contains(robert)': False, 'contains(disney)': False, 'contains(already)': False, 'contains(close)': False, 'contains(classic)': False, 'contains(others)': True, 'contains(hit)': False, 'contains(kill)': False, 'contains(deep)': True, 'contains(five)': False, 'contains(order)': False, 'contains(act)': False, 'contains(simple)': False, 'contains(fine)': False, 'contains(themselves)': False, 'contains(heart)': False, 'contains(roles)': False, 'contains(jackie)': True, 'contains(direction)': False, 'contains(eyes)': False, 'contains(four)': False, 'contains(question)': False, 'contains(sort)': False, 'contains(sometimes)': False, 'contains(knows)': False, 'contains(supporting)': False, 'contains(coming)': False, 'contains(voice)': False, 'contains(women)': False, 'contains(truly)': False, 'contains(save)': False, 'contains(jokes)': False, 'contains(computer)': False, 'contains(child)': False, 'contains(o)': False, 'contains(boring)': False, 'contains(tom)': False, 'contains(level)': False, 'contains(1)': False, 'contains(body)': False, 'contains(guys)': False, 'contains(genre)': False, 'contains(brother)': False, 'contains(strong)': False, 'contains(stop)': True, 'contains(room)': False, 'contains(space)': False, 'contains(lee)': False, 'contains(ends)': False, 'contains(beginning)': False, 'contains(ship)': False, 'contains(york)': False, 'contains(attempt)': False, 'contains(thriller)': False, 'contains(scream)': True, 'contains(peter)': False, 'contains(aren)': False, 'contains(husband)': False, 'contains(fiction)': False, 'contains(happens)': False, 'contains(hero)': False, 'contains(novel)': False, 'contains(note)': False, 'contains(hope)': False, 'contains(king)': False, 'contains(yes)': False, 'contains(says)': False, 'contains(tells)': False, 'contains(quickly)': False, 'contains(romantic)': False, 'contains(dog)': False, 'contains(oscar)': False, 'contains(stupid)': False, 'contains(possible)': False, 'contains(saw)': False, 'contains(lead)': True, 'contains(career)': False, 'contains(murder)': False, 'contains(extremely)': False, 'contains(manages)': False, 'contains(god)': False, 'contains(mostly)': False, 'contains(wonder)': False, 'contains(particularly)': False, 'contains(future)': False, 'contains(fans)': False, 'contains(sound)': False, 'contains(worse)': False, 'contains(piece)': False, 'contains(involving)': False, 'contains(de)': False, 'contains(appears)': False, 'contains(planet)': False, 'contains(paul)': False, 'contains(involved)': False, 'contains(mean)': False, 'contains(none)': False, 'contains(taking)': False, 'contains(hours)': False, 'contains(laugh)': True, 'contains(police)': False, 'contains(sets)': False, 'contains(attention)': False, 'contains(co)': False, 'contains(hell)': False, 'contains(eventually)': False, 'contains(single)': False, 'contains(fall)': False, 'contains(falls)': False, 'contains(material)': False, 'contains(emotional)': False, 'contains(power)': False, 'contains(late)': False, 'contains(lack)': False, 'contains(dr)': False, 'contains(van)': False, 'contains(result)': False, 'contains(elements)': False, 'contains(meet)': False, 'contains(smith)': False, 'contains(science)': False, 'contains(experience)': False, 'contains(bring)': False, 'contains(wild)': False, 'contains(living)': False, 'contains(theater)': False, 'contains(interest)': False, 'contains(leads)': False, 'contains(word)': False, 'contains(feature)': False, 'contains(battle)': False, 'contains(girls)': False, 'contains(alone)': False, 'contains(obviously)': False, 'contains(george)': False, 'contains(within)': False, 'contains(usually)': False, 'contains(enjoy)': False, 'contains(guess)': False, 'contains(among)': True, 'contains(taken)': False, 'contains(feeling)': False, 'contains(laughs)': False, 'contains(aliens)': False, 'contains(talk)': True, 'contains(chance)': False, 'contains(talent)': False, 'contains(3)': False, 'contains(middle)': False, 'contains(number)': False, 'contains(easy)': False, 'contains(across)': False, 'contains(needs)': False, 'contains(attempts)': False, 'contains(happen)': False, 'contains(television)': False, 'contains(chris)': False, 'contains(deal)': False, 'contains(poor)': False, 'contains(form)': False, 'contains(girlfriend)': True, 'contains(viewer)': False, 'contains(release)': False, 'contains(killed)': False, 'contains(forced)': False, 'contains(whether)': False, 'contains(wonderful)': False, 'contains(feels)': False, 'contains(oh)': False, 'contains(tale)': False, 'contains(serious)': False, 'contains(expect)': False, 'contains(except)': False, 'contains(light)': False, 'contains(success)': False, 'contains(features)': True, 'contains(premise)': False, 'contains(happy)': False, 'contains(words)': False, 'contains(leave)': False, 'contains(important)': False, 'contains(meets)': False, 'contains(history)': False, 'contains(giving)': False, 'contains(crew)': False, 'contains(type)': False, 'contains(call)': False, 'contains(turned)': False, 'contains(released)': False, 'contains(parents)': False, 'contains(art)': False, 'contains(impressive)': False, 'contains(mission)': False, 'contains(working)': False, 'contains(seemed)': False, 'contains(score)': False, 'contains(told)': False, 'contains(recent)': False, 'contains(robin)': False, 'contains(basically)': False, 'contains(entertainment)': False, 'contains(america)': False, 'contains($)': False, 'contains(surprise)': False, 'contains(apparently)': False, 'contains(easily)': False, 'contains(ryan)': False, 'contains(cool)': False, 'contains(stuff)': False, 'contains(cop)': False, 'contains(change)': False, 'contains(williams)': False, 'contains(crime)': False, 'contains(office)': False, 'contains(parts)': False, 'contains(somehow)': False, 'contains(sequel)': False, 'contains(william)': False, 'contains(cut)': False, 'contains(die)': False, 'contains(jones)': False, 'contains(credits)': False, 'contains(batman)': False, 'contains(suspense)': False, 'contains(brings)': False, 'contains(events)': False, 'contains(reality)': False, 'contains(whom)': False, 'contains(local)': False, 'contains(talking)': False, 'contains(difficult)': True, 'contains(using)': False, 'contains(went)': False, 'contains(writing)': False, 'contains(remember)': False, 'contains(near)': False, 'contains(straight)': False, 'contains(hilarious)': True, 'contains(ago)': False, 'contains(certain)': False, 'contains(ben)': False, 'contains(kid)': False, 'contains(wouldn)': False, 'contains(slow)': True, 'contains(blood)': False, 'contains(mystery)': False, 'contains(complete)': False, 'contains(red)': False, 'contains(popular)': False, 'contains(effective)': False, 'contains(am)': False, 'contains(fast)': True, 'contains(flick)': False, 'contains(due)': False, 'contains(runs)': False, 'contains(gone)': False, 'contains(return)': False, 'contains(presence)': False, 'contains(quality)': False, 'contains(dramatic)': False, 'contains(filmmakers)': False, 'contains(age)': False, 'contains(brothers)': False, 'contains(business)': False, 'contains(general)': False, 'contains(rock)': False, 'contains(sexual)': False, 'contains(present)': False, 'contains(surprisingly)': False, 'contains(anyway)': False, 'contains(uses)': False, 'contains(4)': False, 'contains(personal)': False, 'contains(figure)': False, 'contains(smart)': False, 'contains(ways)': False, 'contains(decides)': False, 'contains(annoying)': False, 'contains(begin)': False, 'contains(couldn)': False, 'contains(somewhat)': False, 'contains(shots)': False, 'contains(rich)': False, 'contains(minute)': False, 'contains(law)': False, 'contains(previous)': False, 'contains(jim)': False, 'contains(successful)': False, 'contains(harry)': False, 'contains(water)': False, 'contains(similar)': False, 'contains(absolutely)': False, 'contains(motion)': False, 'contains(former)': False, 'contains(strange)': False, 'contains(came)': False, 'contains(follow)': False, 'contains(read)': False, 'contains(project)': False, 'contains(million)': True, 'contains(secret)': False, 'contains(starring)': False, 'contains(clear)': False, 'contains(familiar)': False, 'contains(romance)': False, 'contains(intelligent)': False, 'contains(third)': True, 'contains(excellent)': False, 'contains(amazing)': False, 'contains(party)': False, 'contains(budget)': False, 'contains(eye)': False, 'contains(actress)': False, 'contains(prison)': False, 'contains(latest)': False, 'contains(means)': True, 'contains(company)': False, 'contains(towards)': False, 'contains(predictable)': False, 'contains(powerful)': False, 'contains(nor)': False, 'contains(bob)': False, 'contains(beyond)': False, 'contains(visual)': False, 'contains(leaves)': False, 'contains(r)': False, 'contains(nature)': False, 'contains(following)': False, 'contains(villain)': False, 'contains(leaving)': False, 'contains(animated)': False, 'contains(low)': False, 'contains(myself)': False, 'contains(b)': False, 'contains(bill)': False, 'contains(sam)': False, 'contains(filled)': False, 'contains(wars)': False, 'contains(questions)': False, 'contains(cinema)': False, 'contains(message)': False, 'contains(box)': False, 'contains(moving)': True, 'contains(herself)': False, 'contains(country)': False, 'contains(usual)': False, 'contains(martin)': False, 'contains(definitely)': False, 'contains(add)': False, 'contains(large)': False, 'contains(clever)': False, 'contains(create)': False, 'contains(felt)': False, 'contains(stories)': False, 'contains(brilliant)': False, 'contains(ones)': False, 'contains(giant)': False, 'contains(situation)': False, 'contains(murphy)': False, 'contains(break)': False, 'contains(opens)': False, 'contains(scary)': False, 'contains(doubt)': False, 'contains(drug)': True, 'contains(bunch)': False, 'contains(thinking)': False, 'contains(solid)': False, 'contains(effect)': False, 'contains(learn)': False, 'contains(move)': False, 'contains(force)': False, 'contains(potential)': False, 'contains(seriously)': False, 'contains(follows)': False, 'contains(above)': False, 'contains(saying)': False, 'contains(huge)': False, 'contains(class)': False, 'contains(plan)': False, 'contains(agent)': False, 'contains(created)': False, 'contains(unlike)': False, 'contains(pay)': False, 'contains(non)': True, 'contains(married)': False, 'contains(mark)': False, 'contains(sweet)': False, 'contains(perfectly)': False, 'contains(ex)': False, 'contains(realize)': False, 'contains(audiences)': False, 'contains(took)': False, 'contains(decent)': False, 'contains(likely)': False, 'contains(dream)': False, 'contains(view)': False, 'contains(scott)': False, 'contains(subject)': False, 'contains(understand)': False, 'contains(happened)': False, 'contains(enjoyable)': True, 'contains(studio)': False, 'contains(immediately)': False, 'contains(open)': False, 'contains(e)': False, 'contains(points)': False, 'contains(heard)': False, 'contains(viewers)': False, 'contains(cameron)': False, 'contains(truman)': False, 'contains(bruce)': False, 'contains(frank)': False, 'contains(private)': False, 'contains(stay)': False, 'contains(fails)': False, 'contains(impossible)': False, 'contains(cold)': False, 'contains(richard)': False, 'contains(overall)': False, 'contains(merely)': False, 'contains(exciting)': False, 'contains(mess)': False, 'contains(chase)': True, 'contains(free)': False, 'contains(ten)': False, 'contains(neither)': False, 'contains(wanted)': False, 'contains(gun)': True, 'contains(appear)': False, 'contains(carter)': False, 'contains(escape)': False, 'contains(ultimately)': False, 'contains(+)': False, 'contains(fan)': False, 'contains(inside)': False, 'contains(favorite)': False, 'contains(haven)': False, 'contains(modern)': False, 'contains(l)': False, 'contains(wedding)': False, 'contains(stone)': False, 'contains(trek)': False, 'contains(brought)': False, 'contains(trouble)': True, 'contains(otherwise)': False, 'contains(tim)': False, 'contains(5)': False, 'contains(allen)': False, 'contains(bond)': False, 'contains(society)': False, 'contains(liked)': False, 'contains(dumb)': False, 'contains(musical)': False, 'contains(stand)': False, 'contains(political)': False, 'contains(various)': False, 'contains(talented)': False, 'contains(particular)': False, 'contains(west)': False, 'contains(state)': False, 'contains(keeps)': True, 'contains(english)': False, 'contains(silly)': False, 'contains(u)': False, 'contains(situations)': False, 'contains(park)': False, 'contains(teen)': False, 'contains(rating)': False, 'contains(slightly)': False, 'contains(steve)': False, 'contains(truth)': False, 'contains(air)': False, 'contains(element)': False, 'contains(joke)': False, 'contains(spend)': False, 'contains(key)': True, 'contains(biggest)': False, 'contains(members)': False, 'contains(effort)': False, 'contains(government)': False, 'contains(focus)': False, 'contains(eddie)': False, 'contains(soundtrack)': False, 'contains(hands)': False, 'contains(earlier)': False, 'contains(chan)': True, 'contains(purpose)': False, 'contains(today)': True, 'contains(showing)': False, 'contains(memorable)': False, 'contains(six)': False, 'contains(cannot)': False, 'contains(max)': False, 'contains(offers)': False, 'contains(rated)': False, 'contains(mars)': False, 'contains(heavy)': False, 'contains(totally)': False, 'contains(control)': False, 'contains(credit)': False, 'contains(fi)': False, 'contains(woody)': False, 'contains(ideas)': False, 'contains(sci)': False, 'contains(wait)': False, 'contains(sit)': False, 'contains(female)': False, 'contains(ask)': False, 'contains(waste)': False, 'contains(terrible)': False, 'contains(depth)': False, 'contains(simon)': False, 'contains(aspect)': False, 'contains(list)': False, 'contains(mary)': False, 'contains(sister)': False, 'contains(animation)': False, 'contains(entirely)': False, 'contains(fear)': False, 'contains(steven)': False, 'contains(moves)': False, 'contains(actual)': False, 'contains(army)': False, 'contains(british)': False, 'contains(constantly)': False, 'contains(fire)': False, 'contains(convincing)': False, 'contains(setting)': False, 'contains(gave)': False, 'contains(tension)': False, 'contains(street)': False, 'contains(8)': False, 'contains(brief)': True, 'contains(ridiculous)': False, 'contains(cinematography)': False, 'contains(typical)': False, 'contains(nick)': False, 'contains(screenwriter)': False, 'contains(ability)': False, 'contains(spent)': False, 'contains(quick)': True, 'contains(violent)': False, 'contains(atmosphere)': False, 'contains(subtle)': False, 'contains(expected)': False, 'contains(fairly)': True, 'contains(seven)': False, 'contains(killing)': False, 'contains(tone)': False, 'contains(master)': False, 'contains(disaster)': False, 'contains(lots)': False, 'contains(thinks)': False, 'contains(song)': False, 'contains(cheap)': False, 'contains(suddenly)': False, 'contains(background)': False, 'contains(club)': False, 'contains(willis)': False, 'contains(whatever)': False, 'contains(highly)': False, 'contains(sees)': True, 'contains(complex)': False, 'contains(greatest)': False, 'contains(impact)': False, 'contains(beauty)': False, 'contains(front)': False, 'contains(humans)': False, 'contains(indeed)': False, 'contains(flat)': False, 'contains(grace)': False, 'contains(wrote)': False, 'contains(amusing)': False, 'contains(ii)': False, 'contains(mike)': False, 'contains(further)': False, 'contains(cute)': False, 'contains(dull)': False, 'contains(minor)': False, 'contains(recently)': False, 'contains(hate)': False, 'contains(outside)': False, 'contains(plenty)': False, 'contains(wish)': False, 'contains(godzilla)': False, 'contains(college)': False, 'contains(titanic)': False, 'contains(sounds)': False, 'contains(telling)': False, 'contains(sight)': False, 'contains(double)': False, 'contains(cinematic)': False, 'contains(queen)': False, 'contains(hold)': False, 'contains(meanwhile)': False, 'contains(awful)': False, 'contains(clearly)': False, 'contains(theme)': False, 'contains(hear)': False, 'contains(x)': False, 'contains(amount)': False, 'contains(baby)': False, 'contains(approach)': False, 'contains(dreams)': False, 'contains(shown)': False, 'contains(island)': False, 'contains(reasons)': False, 'contains(charm)': False, 'contains(miss)': True, 'contains(longer)': False, 'contains(common)': False, 'contains(sean)': False, 'contains(carry)': False, 'contains(believable)': False, 'contains(realistic)': False, 'contains(chemistry)': True, 'contains(possibly)': False, 'contains(casting)': False, 'contains(carrey)': False, 'contains(french)': False, 'contains(trailer)': False, 'contains(tough)': False, 'contains(produced)': False, 'contains(imagine)': False, 'contains(choice)': False, 'contains(ride)': False, 'contains(somewhere)': False, 'contains(hot)': False, 'contains(race)': False, 'contains(road)': False, 'contains(leader)': False, 'contains(thin)': False, 'contains(jerry)': False, 'contains(slowly)': False, 'contains(delivers)': False, 'contains(detective)': False, 'contains(brown)': False, 'contains(jackson)': False, 'contains(member)': False, 'contains(provide)': False, 'contains(president)': False, 'contains(puts)': False, 'contains(asks)': False, 'contains(critics)': False, 'contains(appearance)': False, 'contains(famous)': False, 'contains(okay)': False, 'contains(intelligence)': False, 'contains(energy)': False, 'contains(sent)': False, 'contains(spielberg)': False, 'contains(development)': False, 'contains(etc)': False, 'contains(language)': False, 'contains(blue)': False, 'contains(proves)': False, 'contains(vampire)': False, 'contains(seemingly)': False, 'contains(basic)': False, 'contains(caught)': False, 'contains(decide)': False, 'contains(opportunity)': False, 'contains(incredibly)': False, 'contains(images)': False, 'contains(band)': False, 'contains(j)': False, 'contains(writers)': False, 'contains(knew)': False, 'contains(interested)': False, 'contains(considering)': False, 'contains(boys)': False, 'contains(thanks)': False, 'contains(remains)': False, 'contains(climax)': True, 'contains(event)': False, 'contains(directing)': False, 'contains(conclusion)': False, 'contains(leading)': False, 'contains(ground)': False, 'contains(lies)': False, 'contains(forget)': False, 'contains(alive)': False, 'contains(tarzan)': False, 'contains(century)': False, 'contains(provides)': False, 'contains(trip)': False, 'contains(partner)': False, 'contains(central)': False, 'contains(tarantino)': False, 'contains(period)': False, 'contains(pace)': False, 'contains(yourself)': False, 'contains(worked)': False, 'contains(ready)': False, 'contains(date)': False, 'contains(thus)': False, 'contains(1998)': False, 'contains(terrific)': False, 'contains(write)': False, 'contains(average)': False, 'contains(onto)': False, 'contains(songs)': False, 'contains(occasionally)': False, 'contains(doctor)': False, 'contains(stands)': False, 'contains(hardly)': False, 'contains(monster)': False, 'contains(led)': False, 'contains(mysterious)': False, 'contains(details)': False, 'contains(wasted)': False, 'contains(apart)': False, 'contains(aside)': False, 'contains(store)': False, 'contains(billy)': False, 'contains(boss)': True, 'contains(travolta)': False, 'contains(producer)': False, 'contains(pull)': False, 'contains(consider)': False, 'contains(pictures)': False, 'contains(becoming)': False, 'contains(cage)': False, 'contains(loud)': False, 'contains(looked)': False, 'contains(officer)': False, 'contains(twenty)': False, 'contains(system)': False, 'contains(contains)': False, 'contains(julia)': False, 'contains(subplot)': False, 'contains(missing)': False, 'contains(personality)': False, 'contains(building)': False, 'contains(learns)': False, 'contains(hong)': True, 'contains(la)': False, 'contains(apartment)': False, 'contains(7)': False, 'contains(bizarre)': False, 'contains(powers)': False, 'contains(flaws)': False, 'contains(catch)': False, 'contains(lawyer)': False, 'contains(shoot)': False, 'contains(student)': False, 'contains(unique)': True, 'contains(000)': False, 'contains(admit)': False, 'contains(concept)': False, 'contains(needed)': False, 'contains(thrown)': False, 'contains(christopher)': False, 'contains(laughing)': False, 'contains(green)': False, 'contains(twists)': False, 'contains(matthew)': False, 'contains(touch)': False, 'contains(waiting)': False, 'contains(victim)': False, 'contains(cover)': False, 'contains(machine)': False, 'contains(danny)': False, 'contains(mention)': False, 'contains(search)': False, 'contains(1997)': False, 'contains(win)': False, 'contains(door)': False, 'contains(manner)': False, 'contains(train)': True, 'contains(saving)': False, 'contains(share)': False, 'contains(image)': False, 'contains(discovers)': False, 'contains(normal)': False, 'contains(cross)': False, 'contains(fox)': False, 'contains(returns)': False, 'contains(adult)': False, 'contains(adds)': False, 'contains(answer)': False, 'contains(adventure)': False, 'contains(lame)': False, 'contains(male)': False, 'contains(odd)': False, 'contains(singer)': False, 'contains(deserves)': False, 'contains(gore)': False, 'contains(states)': False, 'contains(include)': False, 'contains(equally)': False, 'contains(months)': False, 'contains(barely)': False, 'contains(directors)': False, 'contains(introduced)': False, 'contains(fashion)': False, 'contains(social)': False, 'contains(1999)': False, 'contains(news)': False, 'contains(hair)': False, 'contains(dance)': False, 'contains(innocent)': False, 'contains(camp)': False, 'contains(teacher)': False, 'contains(became)': False, 'contains(sad)': False, 'contains(witch)': False, 'contains(includes)': False, 'contains(nights)': False, 'contains(jason)': False, 'contains(julie)': False, 'contains(latter)': False, 'contains(food)': True, 'contains(jennifer)': False, 'contains(land)': False, 'contains(menace)': False, 'contains(rate)': False, 'contains(storyline)': False, 'contains(contact)': False, 'contains(jean)': False, 'contains(elizabeth)': False, 'contains(fellow)': False, 'contains(changes)': False, 'contains(henry)': False, 'contains(hill)': False, 'contains(pulp)': False, 'contains(gay)': False, 'contains(tried)': False, 'contains(surprised)': False, 'contains(literally)': False, 'contains(walk)': False, 'contains(standard)': False, 'contains(90)': False, 'contains(forward)': False, 'contains(wise)': False, 'contains(enjoyed)': False, 'contains(discover)': False, 'contains(pop)': False, 'contains(anderson)': False, 'contains(offer)': False, 'contains(recommend)': False, 'contains(public)': False, 'contains(drive)': False, 'contains(c)': False, 'contains(toy)': False, 'contains(charming)': False, 'contains(fair)': False, 'contains(chinese)': True, 'contains(rescue)': False, 'contains(terms)': False, 'contains(mouth)': False, 'contains(lucas)': False, 'contains(accident)': False, 'contains(dies)': False, 'contains(decided)': False, 'contains(edge)': False, 'contains(footage)': False, 'contains(culture)': False, 'contains(weak)': False, 'contains(presented)': False, 'contains(blade)': False, 'contains(younger)': False, 'contains(douglas)': False, 'contains(natural)': False, 'contains(born)': False, 'contains(generally)': False, 'contains(teenage)': False, 'contains(older)': False, 'contains(horrible)': False, 'contains(addition)': False, 'contains(sadly)': False, 'contains(creates)': False, 'contains(disturbing)': False, 'contains(roger)': False, 'contains(detail)': False, 'contains(devil)': False, 'contains(debut)': False, 'contains(track)': False, 'contains(developed)': False, 'contains(week)': False, 'contains(russell)': False, 'contains(attack)': False, 'contains(explain)': False, 'contains(rarely)': False, 'contains(fully)': False, 'contains(prove)': False, 'contains(exception)': False, 'contains(jeff)': False, 'contains(twist)': False, 'contains(gang)': False, 'contains(winning)': False, 'contains(jr)': False, 'contains(species)': False, 'contains(issues)': False, 'contains(fresh)': False, 'contains(rules)': False, 'contains(meaning)': False, 'contains(inspired)': False, 'contains(heroes)': False, 'contains(desperate)': False, 'contains(fighting)': False, 'contains(filmed)': False, 'contains(faces)': False, 'contains(alan)': False, 'contains(bright)': False, 'contains(ass)': True, 'contains(flying)': False, 'contains(kong)': True, 'contains(rush)': False, 'contains(forces)': False, 'contains(charles)': False, 'contains(numerous)': False, 'contains(emotions)': False, 'contains(involves)': True, 'contains(patrick)': False, 'contains(weird)': False, 'contains(apparent)': False, 'contains(information)': False, 'contains(revenge)': False, 'contains(jay)': False, 'contains(toward)': False, 'contains(surprising)': False, 'contains(twice)': False, 'contains(editing)': False, 'contains(calls)': False, 'contains(lose)': False, 'contains(vegas)': False, 'contains(stage)': False, 'contains(intended)': False, 'contains(gags)': False, 'contains(opinion)': False, 'contains(likes)': False, 'contains(crazy)': False, 'contains(owner)': False, 'contains(places)': False, 'contains(pair)': False, 'contains(genuine)': False, 'contains(epic)': False, 'contains(speak)': False, 'contains(throw)': False, 'contains(appeal)': False, 'contains(gibson)': False, 'contains(captain)': False, 'contains(military)': False, 'contains(20)': False, 'contains(blair)': False, 'contains(nowhere)': False, 'contains(length)': False, 'contains(nicely)': False, 'contains(cause)': False, 'contains(pass)': False, 'contains(episode)': False, 'contains(kiss)': False, 'contains(arnold)': True, 'contains(please)': False, 'contains(hasn)': False, 'contains(phone)': False, 'contains(filmmaking)': False, 'contains(formula)': False, 'contains(boyfriend)': False, 'contains(talents)': False, 'contains(creating)': False, 'contains(kelly)': False, 'contains(buy)': False, 'contains(wide)': False, 'contains(fantasy)': False, 'contains(mood)': False, 'contains(heads)': False, 'contains(pathetic)': False, 'contains(lacks)': False, 'contains(loved)': False, 'contains(asked)': False, 'contains(mrs)': False, 'contains(witty)': False, 'contains(shakespeare)': False, 'contains(mulan)': False, 'contains(generation)': False, 'contains(affair)': False, 'contains(pieces)': False, 'contains(task)': False, 'contains(rare)': False, 'contains(kept)': False, 'contains(cameo)': False, 'contains(fascinating)': False, 'contains(ed)': False, 'contains(fbi)': False, 'contains(burton)': False, 'contains(incredible)': False, 'contains(accent)': False, 'contains(artist)': False, 'contains(superior)': False, 'contains(academy)': False, 'contains(thomas)': False, 'contains(spirit)': False, 'contains(technical)': False, 'contains(confusing)': False, 'contains(poorly)': False, 'contains(target)': False, 'contains(lover)': False, 'contains(woo)': False, 'contains(mentioned)': False, 'contains(theaters)': False, 'contains(plane)': False, 'contains(confused)': False, 'contains(dennis)': False, 'contains(rob)': False, 'contains(appropriate)': False, 'contains(christmas)': False, 'contains(considered)': False, 'contains(legend)': False, 'contains(shame)': False, 'contains(soul)': False, 'contains(matt)': False, 'contains(campbell)': False, 'contains(process)': False, 'contains(bottom)': False, 'contains(sitting)': False, 'contains(brain)': False, 'contains(creepy)': False, 'contains(13)': False, 'contains(forever)': False, 'contains(dude)': False, 'contains(crap)': False, 'contains(superb)': False, 'contains(speech)': False, 'contains(ice)': False, 'contains(journey)': False, 'contains(masterpiece)': False, 'contains(intriguing)': False, 'contains(names)': False, 'contains(pick)': False, 'contains(speaking)': False, 'contains(virtually)': False, 'contains(award)': False, 'contains(worthy)': False, 'contains(marriage)': False, 'contains(deliver)': False, 'contains(cash)': False, 'contains(magic)': False, 'contains(respect)': False, 'contains(product)': False, 'contains(necessary)': False, 'contains(suppose)': False, 'contains(silent)': False, 'contains(pointless)': False, 'contains(station)': False, 'contains(affleck)': False, 'contains(dimensional)': False, 'contains(charlie)': False, 'contains(allows)': False, 'contains(avoid)': False, 'contains(meant)': False, 'contains(cops)': False, 'contains(attitude)': False, 'contains(relationships)': False, 'contains(hits)': False, 'contains(stephen)': False, 'contains(spends)': False, 'contains(relief)': False, 'contains(physical)': True, 'contains(count)': False, 'contains(reviews)': False, 'contains(appreciate)': False, 'contains(cliches)': False, 'contains(holds)': False, 'contains(pure)': False, 'contains(plans)': False, 'contains(limited)': False, 'contains(failed)': False, 'contains(pain)': False, 'contains(impression)': False, 'contains(unless)': False, 'contains(sub)': False, 'contains([)': False, 'contains(total)': False, 'contains(creature)': False, 'contains(viewing)': False, 'contains(loves)': False, 'contains(princess)': False, 'contains(kate)': False, 'contains(rising)': False, 'contains(woods)': False, 'contains(baldwin)': False, 'contains(angry)': False, 'contains(drawn)': False, 'contains(step)': False, 'contains(matrix)': False, 'contains(themes)': False, 'contains(satire)': False, 'contains(arts)': False, 'contains(])': False, 'contains(remake)': False, 'contains(wall)': False, 'contains(moral)': False, 'contains(color)': False, 'contains(ray)': False, 'contains(stuck)': False, 'contains(touching)': False, 'contains(wit)': False, 'contains(tony)': False, 'contains(hanks)': False, 'contains(continues)': False, 'contains(damn)': False, 'contains(nobody)': False, 'contains(cartoon)': False, 'contains(keeping)': False, 'contains(realized)': False, 'contains(criminal)': False, 'contains(unfunny)': False, 'contains(comedic)': False, 'contains(martial)': False, 'contains(disappointing)': False, 'contains(anti)': False, 'contains(graphic)': False, 'contains(stunning)': False, 'contains(actions)': False, 'contains(floor)': False, 'contains(emotion)': False, 'contains(soldiers)': False, 'contains(edward)': False, 'contains(comedies)': False, 'contains(driver)': False, 'contains(expectations)': False, 'contains(added)': False, 'contains(mad)': False, 'contains(angels)': False, 'contains(shallow)': False, 'contains(suspect)': False, 'contains(humorous)': False, 'contains(phantom)': False, 'contains(appealing)': False, 'contains(device)': False, 'contains(design)': False, 'contains(industry)': False, 'contains(reach)': False, 'contains(fat)': False, 'contains(blame)': False, 'contains(united)': False, 'contains(sign)': False, 'contains(portrayal)': False, 'contains(rocky)': False, 'contains(finale)': False, 'contains(grand)': False, 'contains(opposite)': False, 'contains(hotel)': False, 'contains(match)': False, 'contains(damme)': False, 'contains(speed)': False, 'contains(ok)': False, 'contains(loving)': False, 'contains(field)': True, 'contains(larry)': False, 'contains(urban)': False, 'contains(troopers)': False, 'contains(compared)': False, 'contains(apes)': False, 'contains(rose)': False, 'contains(falling)': False, 'contains(era)': False, 'contains(loses)': False, 'contains(adults)': False, 'contains(managed)': False, 'contains(dad)': False, 'contains(therefore)': False, 'contains(pg)': False, 'contains(results)': False, 'contains(guns)': False, 'contains(radio)': False, 'contains(lady)': False, 'contains(manage)': False, 'contains(spice)': False, 'contains(naked)': False, 'contains(started)': False, 'contains(intense)': False, 'contains(humanity)': False, 'contains(wonderfully)': False, 'contains(slasher)': False, 'contains(bland)': False, 'contains(imagination)': False, 'contains(walking)': False, 'contains(willing)': False, 'contains(horse)': False, 'contains(rent)': False, 'contains(mix)': False, 'contains(generated)': False, 'contains(g)': False, 'contains(utterly)': False, 'contains(scientist)': False, 'contains(washington)': False, 'contains(notice)': False, 'contains(players)': False, 'contains(teenagers)': False, 'contains(moore)': False, 'contains(board)': False, 'contains(price)': False, 'contains(frightening)': False, 'contains(tommy)': False, 'contains(spectacular)': False, 'contains(bored)': False, 'contains(jane)': False, 'contains(join)': False, 'contains(producers)': False, 'contains(johnny)': False, 'contains(zero)': False, 'contains(vampires)': False, 'contains(adaptation)': False, 'contains(dollars)': False, 'contains(parody)': False, 'contains(documentary)': False, 'contains(dvd)': False, 'contains(wayne)': False, 'contains(post)': False, 'contains(exist)': False, 'contains(matters)': False, 'contains(chosen)': False, 'contains(mel)': False, 'contains(attractive)': True, 'contains(plain)': False, 'contains(trust)': False, 'contains(safe)': False, 'contains(reading)': False, 'contains(hoping)': False, 'contains(protagonist)': False, 'contains(feelings)': False, 'contains(fate)': False, 'contains(finding)': False, 'contains(feet)': False, 'contains(visuals)': False, 'contains(spawn)': False, 'contains(compelling)': False, 'contains(hall)': False, 'contains(sympathetic)': False, 'contains(featuring)': False, 'contains(difference)': False, 'contains(professional)': False, 'contains(drugs)': False, 'contains(ford)': False, 'contains(shooting)': False, 'contains(gold)': False, 'contains(patch)': False, 'contains(build)': False, 'contains(boat)': False, 'contains(cruise)': False, 'contains(honest)': False, 'contains(media)': False, 'contains(flicks)': False, 'contains(bug)': False, 'contains(bringing)': False, 'contains(dangerous)': True, 'contains(watched)': False, 'contains(grant)': False, 'contains(smile)': False, 'contains(plus)': False, 'contains(shouldn)': False, 'contains(decision)': False, 'contains(visually)': False, 'contains(allow)': False, 'contains(starship)': False, 'contains(roberts)': False, 'contains(dying)': False, 'contains(portrayed)': False, 'contains(turning)': False, 'contains(believes)': False, 'contains(changed)': False, 'contains(shock)': False, 'contains(destroy)': False, 'contains(30)': False, 'contains(crowd)': False, 'contains(broken)': False, 'contains(tired)': False, 'contains(fail)': False, 'contains(south)': False, 'contains(died)': False, 'contains(cult)': False, 'contains(fake)': False, 'contains(vincent)': False, 'contains(identity)': False, 'contains(sexy)': False, 'contains(hunt)': False, 'contains(jedi)': False, 'contains(flynt)': False, 'contains(alex)': False, 'contains(engaging)': False, 'contains(serve)': False, 'contains(snake)': False, 'contains(yeah)': False, 'contains(expecting)': False, 'contains(100)': False, 'contains(decade)': False, 'contains(ups)': False, 'contains(constant)': False, 'contains(current)': False, 'contains(survive)': False, 'contains(jimmy)': False, 'contains(buddy)': False, 'contains(send)': False, 'contains(brooks)': False, 'contains(goofy)': False, 'contains(likable)': False, 'contains(humour)': False, 'contains(technology)': False, 'contains(files)': False, 'contains(babe)': False, 'contains(aspects)': False, 'contains(presents)': False, 'contains(kills)': False, 'contains(supposedly)': False, 'contains(eight)': True, 'contains(sandler)': False, 'contains(hospital)': False, 'contains(test)': False, 'contains(hidden)': False, 'contains(brian)': False, 'contains(books)': False, 'contains(promise)': False, 'contains(determined)': False, 'contains(professor)': False, 'contains(welcome)': False, 'contains(pleasure)': False, 'contains(succeeds)': False, 'contains(individual)': False, 'contains(annie)': False, 'contains(mob)': False, 'contains(ted)': False, 'contains(virus)': False, 'contains(content)': False, 'contains(gary)': False, 'contains(direct)': False, 'contains(contrived)': False, 'contains(carpenter)': False, 'contains(scale)': False, 'contains(sick)': False, 'contains(nasty)': False, 'contains(conflict)': False, 'contains(haunting)': False, 'contains(ghost)': False, 'contains(filmmaker)': False, 'contains(japanese)': False, 'contains(helps)': False, 'contains(fare)': False, 'contains(lucky)': False, 'contains(ultimate)': False, 'contains(window)': False, 'contains(support)': False, 'contains(goal)': False, 'contains(provided)': False, 'contains(genius)': False, 'contains(winner)': False, 'contains(taylor)': False, 'contains(fantastic)': False, 'contains(faith)': False, 'contains(lynch)': False, 'contains(fit)': False, 'contains(catherine)': False, 'contains(ms)': False, 'contains(paced)': False, 'contains(breaks)': False, 'contains(al)': False, 'contains(frame)': False, 'contains(travel)': False, 'contains(badly)': False, 'contains(available)': False, 'contains(cares)': False, 'contains(reeves)': False, 'contains(crash)': False, 'contains(driving)': False, 'contains(press)': False, 'contains(seagal)': False, 'contains(amy)': False, 'contains(9)': False, 'contains(headed)': False, 'contains(instance)': False, 'contains(excuse)': False, 'contains(offensive)': False, 'contains(narrative)': False, 'contains(fault)': False, 'contains(bus)': False, 'contains(f)': False, 'contains(extreme)': False, 'contains(miller)': False, 'contains(guilty)': False, 'contains(grows)': False, 'contains(overly)': False, 'contains(liners)': False, 'contains(forgotten)': False, 'contains(ahead)': False, 'contains(accept)': False, 'contains(porn)': False, 'contains(directly)': False, 'contains(helen)': False, 'contains(began)': False, 'contains(lord)': False, 'contains(folks)': False, 'contains(mediocre)': False, 'contains(bar)': False, 'contains(surface)': False, 'contains(super)': False, 'contains(failure)': False, 'contains(6)': False, 'contains(acted)': False, 'contains(quiet)': False, 'contains(laughable)': False, 'contains(sheer)': False, 'contains(security)': True, 'contains(emotionally)': False, 'contains(season)': False, 'contains(stuart)': False, 'contains(jail)': True, 'contains(deals)': False, 'contains(cheesy)': False, 'contains(court)': False, 'contains(beach)': False, 'contains(austin)': False, 'contains(model)': False, 'contains(outstanding)': False, 'contains(substance)': False, 'contains(nudity)': False, 'contains(slapstick)': False, 'contains(joan)': False, 'contains(reveal)': False, 'contains(placed)': False, 'contains(check)': False, 'contains(beast)': False, 'contains(hurt)': False, 'contains(bloody)': False, 'contains(acts)': False, 'contains(fame)': False, 'contains(meeting)': False, 'contains(nuclear)': False, 'contains(1996)': False, 'contains(strength)': False, 'contains(center)': False, 'contains(funniest)': False, 'contains(standing)': True, 'contains(damon)': False, 'contains(clich)': False, 'contains(position)': False, 'contains(desire)': False, 'contains(driven)': False, 'contains(seat)': False, 'contains(stock)': False, 'contains(wondering)': True, 'contains(realizes)': False, 'contains(dealing)': False, 'contains(taste)': False, 'contains(routine)': False, 'contains(comparison)': False, 'contains(cinematographer)': False, 'contains(seconds)': False, 'contains(singing)': False, 'contains(gangster)': True, 'contains(responsible)': False, 'contains(football)': False, 'contains(remarkable)': False, 'contains(hunting)': False, 'contains(adams)': False, 'contains(fly)': False, 'contains(suspects)': False, 'contains(treat)': False, 'contains(hopes)': False, 'contains(heaven)': False, 'contains(myers)': False, 'contains(struggle)': False, 'contains(costumes)': False, 'contains(beat)': False, 'contains(happening)': False, 'contains(skills)': False, 'contains(ugly)': False, 'contains(figures)': False, 'contains(thoroughly)': False, 'contains(ill)': False, 'contains(surprises)': False, 'contains(player)': False, 'contains(rival)': False, 'contains(guard)': True, 'contains(anthony)': False, 'contains(strike)': False, 'contains(community)': False, 'contains(streets)': False, 'contains(hopkins)': False, 'contains(ended)': False, 'contains(originally)': False, 'contains(sarah)': False, 'contains(creative)': False, 'contains(characterization)': False, 'contains(thankfully)': False, 'contains(growing)': False, 'contains(sharp)': False, 'contains(williamson)': False, 'contains(eccentric)': False, 'contains(explained)': False, 'contains(hey)': False, 'contains(claire)': False, 'contains(steal)': False, 'contains(inevitable)': False, 'contains(joel)': False, 'contains(core)': False, 'contains(weren)': False, 'contains(sorry)': False, 'contains(built)': False, 'contains(anne)': False, 'contains(breaking)': False, 'contains(villains)': False, 'contains(critic)': False, 'contains(lets)': False, 'contains(visit)': False, 'contains(followed)': False}\n"
     ]
    }
   ],
   "source": [
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we can train a classifier, and then check its accuracy:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most informative features.  As was the case with the accuracy, the figures will be slightly different from the book, since our test set was randomly selected from the entire corpus, and thus is different from the one that was used in the book.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     13.2 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      9.0 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      7.8 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      7.5 : 1.0\n",
      "          contains(lame) = True              neg : pos    =      6.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Part-of-Speech Tagging\n",
    "\n",
    "*Working out what the most informative suffixes for a part-of-speech tagger would be:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we'll define a feature extractor function that checks a word for these:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endwsith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training a classifier.  Runs quite slowly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6270512182993535"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LONG RUN\n",
    "tagged_words = brown.tagged_words(categories = 'news')\n",
    "featuresets = [(pos_features(n), g) for (n, g) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can instruct NLTK to print the decision tree as pseudocode:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endwsith(the) == False: \n",
      "  if endwsith(,) == False: \n",
      "    if endwsith(s) == False: \n",
      "      if endwsith(.) == False: \n",
      "        if endwsith(of) == False: \n",
      "          if endwsith(and) == False: return '.'\n",
      "          if endwsith(and) == True: return 'CC'\n",
      "        if endwsith(of) == True: return 'IN'\n",
      "      if endwsith(.) == True: return '.'\n",
      "    if endwsith(s) == True: \n",
      "      if endwsith(is) == False: \n",
      "        if endwsith(was) == False: \n",
      "          if endwsith(as) == False: return 'PP$'\n",
      "          if endwsith(as) == True: return 'CS'\n",
      "        if endwsith(was) == True: return 'BEDZ'\n",
      "      if endwsith(is) == True: \n",
      "        if endwsith(his) == False: return 'BEZ'\n",
      "        if endwsith(his) == True: return 'PP$'\n",
      "  if endwsith(,) == True: return ','\n",
      "if endwsith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth = 6))\n",
    "# END LONG RUN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Exploiting Context\n",
    "\n",
    "*Of course, the tagger will be even more accurate if it can take advantage of a word's context:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i - 1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = []\n",
    "tagged_sents = brown.tagged_sents(categories = 'news')\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Sequence Classification \n",
    "\n",
    "__Joint classifier__ models choose an appropriate labeling for a collection of related inputs.  With part-of-speech tagging, a number of different __sequence classifier__ models can be utilized to jointly choose part-of-speech tags for words in a given sentence.\n",
    "\n",
    "__Consecutive classification__ (a.k.a. __greedy sequence classification__) finds the most likely class label for the first input, and then uses this to find the best label for the next input.  This process is repeated until all the inputs have been labeled.  \n",
    "\n",
    "*We can update `pos_features` to store the tags of the previous word, and from there build our sequence classifier:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i - 1]\n",
    "        features[\"prev-tag\"] = history[i - 1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin McCarville\\AppData\\Local\\Temp\\ipykernel_11848\\4082246279.py:5: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tagger.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980528511821975\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories = 'news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Other Methods for Sequence Classification\n",
    "\n",
    "One shortcoming of this approach is that the labels cannot be revised once they are assigned.  A solution is to adopt a transformational strategy instead.  In such a strategy, labels are iteratively refined to repair inconsistencies between related inputs.  The Brill Tagger is an example of this.\n",
    "\n",
    "Another solution is assign scores to all possible sequences of POS tags, and then choose the sequence with the highest overall score.  This is the approach used by __Hidden Markov Models__.  One problem is that the number of possible tag sequences can be very large, so normally we'll only look at the most recent $n$ tags.  \n",
    "\n",
    "### 2 Further Examples of Supervised Classification\n",
    "\n",
    "#### 2.1 Sentence Segmentation\n",
    "\n",
    "Trying to extract features from sentences that have already been segmented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the features of the data that will be used in order to decide whether punctuation indicates a sentence-boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i + 1][0].isupper(),\n",
    "            'prev-word': tokens[i - 1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i - 1]) == 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all punctuation tokens and tagger whether they are boundary tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "                for i in range(1, len(tokens) - 1)\n",
    "                if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluating a punctuation classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936026936026936"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(word, i)) == True:\n",
    "            sents.append(words[start:i + 1])\n",
    "            start = i + 1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Identifying Dialogue Act Types\n",
    "\n",
    "Recognizing the __dialogue acts__ behind the utterances in a dialogue can be an important initial step in understanding a conversation.\n",
    "\n",
    "The posts in NPS Chat Corpus have been labeled with one of 15 dialogue act types.  We can build a classifier that can identify the dialogue act types for new messages.  We can extract the basic messaging data by calling `xml_posts()` to get a data structure representing the XML annotation for each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature extractor checks what words the post contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.667\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "                for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Recognizing Textual Entailment (RTE)\n",
    "\n",
    "RTE could be thought of as making logical inferences about a text.  RTE can achieve fairly good results on shallow analysis alone (i.e., the similarity between the text and hypothesis at the word level).  \n",
    "\n",
    "The RTE feature detector below measures the amount of word overlap between the text and the hypothesis.  Named Entity mentions are likely to be more important than words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rte_features(rtepair):\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'central', 'Asia', 'Russia', 'operation', 'four', 'fight', 'Co', 'meeting', 'that', 'former', 'Organisation', 'representing', 'was', 'Shanghai', 'fledgling', 'Davudi', 'republics', 'SCO', 'China', 'together', 'Parviz', 'Soviet', 'association', 'Iran', 'at', 'binds', 'terrorism.'}\n"
     ]
    }
   ],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "print(extractor.text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'China', 'SCO.', 'member'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.hyp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(extractor.overlap('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'China'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.overlap('ne'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.hyp_extra('word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Scaling Up to Large Datasets\n",
    "\n",
    "Python isn't as efficient with large datasets.  NLTK has facilities for interfacing with external machine learning packages.  \n",
    "\n",
    "*__N.B.__ I searched for these but was unable to find anything...*\n",
    "\n",
    "### 3 Evaluation\n",
    "\n",
    "#### 3.1 The Test Set\n",
    "\n",
    "The size of the test set should be chosen so that the least frequent label occurs at least 50 times.\n",
    "\n",
    "The book recommends that when using the Brown Corpus, we should __not__ shuffle the sentences.  The reason is that shuffling the corpus almost guarantees that sentences from the same documents will be in both the training and the test sets, and therefore the resulting classifier will not generalize well to other documents.  The book even goes so far as to recommend using different categories from the Brown Corpus for the training and the test sets.\n",
    "\n",
    "#### 3.2 Accuracy\n",
    "\n",
    "Accuracy is not a reliable metric when dealing with very rare phenomena.  E.g., if we tested a disease that occurred only once in 10,000 instances, we could label all the tests 'negative' and still achieve 99.99% accuracy; but the test would be meaningless, as it fails to identify what it's supposed to test.\n",
    "\n",
    "#### 3.3 Precision and Recall\n",
    "\n",
    "* __False positives__ are __Type I errors__\n",
    "* __False negatives__ are __Type II errors__\n",
    "\n",
    "* __Precision__ (the proportion of relevant items amongst the items we identified) is $\\frac{TP}{(TP + FP)}$\n",
    "* __Recall__ (the proportion of relevant items we identified over all the relevant items) is $\\frac{TP}{(TP + FN)}$\n",
    "* __F-Measure__ (or __F-Score__) is the harmonic mean of precision and recall: $\\frac{(2 \\times Precision \\times Recall)}{(Precision + Recall)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\precision-recall.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test from same doc\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]\n",
    "\n",
    "# better for training and test from diff docs\n",
    "file_ids = brown.fileids(categories='news')\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_set = brown.tagged_sents(file_ids[size:])\n",
    "test_set = brown.tagged_sents(file_ids[:size])\n",
    "\n",
    "# docs of diff genres\n",
    "# train_set = brown.tagged_sents(categories='news')\n",
    "# test_set = brown.tagged_sents(categories='fiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wont run, train_set directly above unlabeled\n",
    "# classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# print('Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Confusion Matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |                                         N                      |\n",
      "    |      N      I      A      J             N             V      N |\n",
      "    |      N      N      T      J      .      S      ,      B      P |\n",
      "----+----------------------------------------------------------------+\n",
      " NN | <11.9%>  0.0%      .   0.2%      .   0.0%      .   0.2%   0.0% |\n",
      " IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |\n",
      " AT |      .      .  <8.6%>     .      .      .      .      .      . |\n",
      " JJ |   1.7%      .      .  <4.0%>     .      .      .   0.0%   0.0% |\n",
      "  . |      .      .      .      .  <4.8%>     .      .      .      . |\n",
      "NNS |   1.4%      .      .      .      .  <3.3%>     .      .   0.0% |\n",
      "  , |      .      .      .      .      .      .  <4.4%>     .      . |\n",
      " VB |   1.0%      .      .   0.0%      .      .      .  <2.4%>     . |\n",
      " NP |   1.0%      .      .   0.0%      .      .      .      .  <1.9%>|\n",
      "----+----------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\n",
    "\n",
    "# this was left out of the book, however, it's clear in line 13\n",
    "# that we need a tagger\n",
    "tagged_sents = brown.tagged_sents(categories = 'news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff = t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff = t1)\n",
    "\n",
    "gold = tag_list(brown.tagged_sents(categories = 'editorial'))\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories = 'editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(cm.pretty_format(sort_by_count = True, show_percents = True, truncate = 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows represent the reference tags (i.e., what they should be), while the columns represent the test tags (i.e., what they were actually labelled.  So, in this example, 1.7% of the words were labeled `NN` when they were actually `JJ`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Cross-Validation\n",
    "\n",
    "Cross Validation (such as $k$-folds validation) is very valuable when we only have a limited amount of annotated data.  It also allows us to see how consistent our training data is.\n",
    "\n",
    "### 4 Decision Trees\n",
    "\n",
    "Decision trees consist of __decision nodes__, which check feature values, and __leaf nodes__, which assign labels, and the __root node__, which is where the decision tree begins.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\decision-tree.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Entropy and Information Gain\n",
    "\n",
    "__Information gain__ measures how much more organized the input values become when we divide them up using a given feature.  To measure how disorganized the original set of input values are, we calculate entropy of their labels, which will be high if the input values have highly varied labels, and low if many input values all have the same label.  I.e., entropy is defined as the sum of the probability of each label times the log probability of that same label:\n",
    "\n",
    "$$(1)\\quad H = -\\Sigma_{l \\text{ in } labels} P(l) \\times log_2P(l)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\Binary_entropy_plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p, 2) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy(['male', 'male', 'male', 'male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "print(entropy(['male', 'female', 'male', 'male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy(['female', 'male', 'female', 'male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "print(entropy(['female', 'female', 'male', 'female']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy(['female', 'female', 'female', 'female']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can calculate the entropy of the values, we can determine how much more organized the labels would be at a certain __decision stump__, which is a single node that decides how to classify inputs based on a single feature.  We would calculate the entropy for each of the stump's leaves, take the average of those leaf entropy values, and then determine the information gain by subtracting the new entropy value from the original.  The higher the information gain, the better job the decision stump does of dividing the input values into coherent groups.\n",
    "\n",
    "While decision trees are easy to understand and interpret, they also split the training data at each branch.  Consequently, lower decision nodes may overfit the training set.  This can be avoided by not dividing nodes once the amount of training data is too small; another approach involves pruning decision nodes that do not improve performance on a dev-test.\n",
    "\n",
    "Another problem is that decision trees may force features to be checked in a certain order, even if such features are independent of each other.  This may lead to certain features being checked multiple times in different parts of the tree.  \n",
    "\n",
    "A final problem is that decision trees do not efficiently use features that are weak predictors of the correct label. Since these features make relatively small incremental improvements, they tend to occur very low in the decision tree, at which point there is very little training data.  \n",
    "\n",
    "### 5 Naive Bayes Classifiers\n",
    "\n",
    "In __naive Bayes__ classifiers, every feature gets a say into determining the label.  The classifier calculates the __prior probability__ of each label by checking its frequency in the training set.  Contributions from each feature are combined with the prior probability to arrive at a likelihood estimate for each label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\naive-bayes-triangle.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Figure 5.1__: An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document. In the training corpus, most documents are automotive, so the classifier starts out at a point closer to the \"automotive\" label. But it then considers the effect of each feature. In this example, the input document contains the word \"dark,\" which is a weak indicator for murder mysteries, but it also contains the word \"football,\" which is a strong indicator for sports documents. After every feature has made its contribution, the classifier checks which label it is closest to, and assigns that label to the input.*\n",
    "\n",
    "Individual features make their contributions to the overall decision by \"voting against\" labels that don't occur with that feature very often.  The likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature.  E.g., if *run* occurs in 12% of the sports docs, 10% of the murder mystery docs, and 2% of the automotive docs, then the likelihood scores would respectively be multiplied by .12, .1, and .02 for the labels sports, murder mystery, and automotive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\naive_bayes_bargraph.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1   Underlying Probabilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive Bayes classifier assumes the features are independent of each other, which is an unrealistic assumption.  But it makes it easier to combine the contributions of the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image(filename = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\naive_bayes_graph.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Figure 5.3__: A __Bayesian Network Graph__ illustrating the generative process that is assumed by the naive Bayes classifier. To generate a labeled input, the model first chooses a label for the input, then it generates each of the input's features based on that label. Every feature is assumed to be entirely independent of every other feature, given the label.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this assumption to calculate $P(label|features)$, the probability an input will have a particular label given it has a particular set of features.  We will simply pick the label $l$ that maximizes $P(l|features)$.  We can use Bayes' law to calculate this:\n",
    "\n",
    "$$(2) \\quad P(label|features) = \\frac{P(features, label)}{P(features)}$$\n",
    "\n",
    "Since $P(features)$ will be the same for every choice of label, it will suffice to calculate $P(features, label) if we just want to find the most likely label.\n",
    "\n",
    "To generate a probability estimate for each label, we should calculate the sum of labels of $P(features, label)$:\n",
    "\n",
    "$$(3) \\quad P(features) = \\Sigma_{l \\text{  in  } labels} P(features, label)$$\n",
    "\n",
    "We can expand the label likelihood thusly:\n",
    "\n",
    "$$(4) \\quad P(features, label) = P(label) \\times P(features|label)$$\n",
    "\n",
    "We can also separate the probability of each individual feature:\n",
    "\n",
    "$$(5) \\quad P(features, label) = P(label) \\times P_{f \\text {  in  } features} P(f|label)$$\n",
    "\n",
    "\n",
    "#### 5.2 Zero Counts and Smoothing\n",
    "\n",
    "The simplest way to calculate the contribution of a feature toward the label likelihood ($P(f|label)$) is to take the percentage of training instances with the given label that also have the given feature:\n",
    "\n",
    "$$(6) \\quad P(f|label) = \\frac{count(f, label)}{count(label)}$$\n",
    "\n",
    "But this won't work if a feature *never* occurs with a given label in the training set, since the figure will be zero.  We would need to use a __smoothing__ technique to deal with this, such as using the __Expected Likelihood Estimation__, which adds 0.5 to each $count(f, label)$ valued, and the __Heldout Estimation__ uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities.  \n",
    "\n",
    "#### 5.3 Non-Binary Features\n",
    "\n",
    "Label-valued features (e.g., colors) can be converted to binary by replacing them with binary features (e.g., \"is-red\"). Numeric features can be binned.  Another possibility is to use regression models.  \n",
    "\n",
    "#### 5.4 The Naivete of Independence\n",
    "\n",
    "Since features are usually interconnected, by ignoring their dependence we run the risk of double counting the effect of highly correlated features.  \n",
    "\n",
    "#### 5.5 The Cause of Double Counting\n",
    "\n",
    "To account for double counting, we could consider the possible interactions between feature contributions during training.  The following equation separates out the contribution made by each feature:\n",
    "\n",
    "$$(7) \\quad P(features, label) = w[label] \\times Prod_{f \\text{ in } features} w[f, label]$$\n",
    "\n",
    "The values $w[label]$ and $w[f, label]$ are the __parameters__ (or __weights__) for the model.  We can set each of these parameters independently with the naive Bayes algorithm.\n",
    "\n",
    "$$(8) \\quad w[label] = P(label)$$\n",
    "\n",
    "$$(9) \\quad w[f, label] = P(f|label)$$\n",
    "\n",
    "### 6 Maximum Entropy Classifiers\n",
    "\n",
    "The __Maximum Entropy__ classifier uses a model that utilizes search techniques to find a set of parameters that will maximize the performance of the classifier.  Specifically, it looks for the set of parameters that maximizes the __total likelihood__ of the training corpus, which is defined as:\n",
    "\n",
    "$$(10) \\quad P(features) = \\Sigma_{x \\text{  in  } corpus} P(label(x)|features(x))$$\n",
    "\n",
    "The probability that an input whose features are `features` will have class label `label` is \n",
    "\n",
    "$$(11) \\quad P(label|features) = \\frac{P(label, features)}{\\Sigma_{label}P(label, features)}$$\n",
    "\n",
    "Because of the potentially complex interactions between the effects of related features, there is no way to directly calculate the model parameters that maximize the likelihood of the training set.  ME classifiers use __iterative optimization__ techniques, which start by using random values for the model's parameters and then repeatedly refine those parameters to bring them closer to the optimal solution.  Although each step brings the parameters closer to the optimal values, it is difficult to determine when those optimal values have been reached.  Therefore, it can take a long time to learn parameters, especially if the size of the training set, the number of features, or the number of labels is large.\n",
    "\n",
    "#### 6.1 The Maximum Entropy Model\n",
    "\n",
    "The ME classifier model lets the user decide what combination of labels and features should receive their own parameters.  It is possible to use a single parameter to associate a feature with more than one label, or to associate more than one feature with a given lable.  This will sometimes allow the model to \"generalize\" over some of the differences between related labels or features.\n",
    "\n",
    "Each combination of labels and features is called a __joint-feature__.  A join-feature is defined for each label, corresponding to $w[label]$, and for each combination of feature and lable, corresponding to $w[f, label]$.  The score assibned to a label for a given input is simply the product of the parameters associated with the joint-features that apply to that input and label:\n",
    "\n",
    "$$P(input, label) = Prod_{joint-features(input, label)}w[joint-features]$$\n",
    "\n",
    "#### 6.2 Maximizing Entropy\n",
    "\n",
    "The intuition that motivates ME classification is that we should build a model that captures the frequences of individual joint-features without making any unwarranted assumptions.  E.g., suppose we have to choose from a list of ten possible senses (labeled A-J), but we don't know anything more about the word or the senses.  If we have a number of possible probability distributions such as these:\n",
    "\n",
    "\n",
    "|       |  A  |   B  |  C  |  D  |  E  |  F  |  G  |  H  |  I  |\n",
    "|:-----:|:---:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|  (i)  | 10% |  10% | 10% | 10% | 10% | 10% | 10% | 10% | 10% |\n",
    "|  (ii) |  5% |  15% |  0% | 30% |  0% |  8% | 12% |  0% |  6% |\n",
    "| (iii) |  0% | 100% |  0% |  0% |  0% |  0% |  0% |  0% |  0% |\n",
    "\n",
    "We are more likely to choose $(i)$, because its entropy is higher than the others.  The __Maximum Entropy principle__ states that among the distributions that are consistent with what we know, we should choose the distribution whose entropy is highest.\n",
    "\n",
    "Another example: Suppose that we know that sense A appears 55\\% of the time.  Again, we have a number of distributions to choose from:\n",
    "\n",
    "|      |  A  |  B  |  C |  D |  E |  F |  G |  H  |  I |\n",
    "|:----:|:---:|:---:|:--:|:--:|:--:|:--:|:--:|:---:|:--:|\n",
    "| (iv) | 55% | 45% | 0% | 0% | 0% | 0% | 0% |  0% | 0% |\n",
    "|  (v) | 55% |  5% | 5% | 5% | 5% | 5% | 5% |  5% | 5% |\n",
    "| (vi) | 55% |  3% | 1% | 2% | 9% | 5% | 0% | 25% | 0% |\n",
    "\n",
    "In this instance, $(v)$ has the highest entropy, and in the absence of more information we should choose this one. \n",
    "\n",
    "In the final example: suppose we know that the word \"up\" appears in the nearby context 10\\% of the time, and when it does appear there's an 80\\% chance that sense A or C will be used.  In this case, the following distributions look appropriate:\n",
    "\n",
    "|       |     |    A   |   B   |   C   |   D   |   E   |   F   |   G   |   H   |   I   |   J   |\n",
    "|:-----:|:---:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| (vii) | +up |  5.10% | 0.25% | 2.90% | 0.25% | 0.25% | 0.25% | 0.25% | 0.25% | 0.25% | 0.25% |\n",
    "|  ` `  | -up | 49.90% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Generative vs Conditional Classifiers\n",
    "\n",
    "The naive Bayes classifier is a __generative__ classifier which builds a model that predicts $P(input, label)$, the joint probability of a $(input, label)$ pair.  Generative modesl can be used to answer the following questions:\n",
    "\n",
    "* 1. What is the most likely label for a given input?\n",
    "* 2. How likely is a given label for a given input?\n",
    "* 3. What is the most likely input value?\n",
    "* 4. How likely is a given input value?\n",
    "* 5. How likely is a given input value with a given label?\n",
    "* 6. What is the most likely label for an input that might have one of two values (but we don't know which)?\n",
    "\n",
    "The ME classifier is a __conditional__ classifier.  These build models that predict $P(label|input)$, i.e., the probability of a label *given* the input value. Conditional models can be used to answer the first two questions above, but not the remaining four.\n",
    "\n",
    "Generative models are more powerful, sicne we can predict $P(label|input)$ from $P(input, label)$ but not vice versa.  But these models have more \"free parameters\" that need to be learned, even though the size of the training set is fixed.  As a result, we end up with less data that can be used to train each parameter's value, making it harder to find the best parameter values.  Consequently, the generative model may not do as well at answering questions 1 and 2 as a conditional model.  \n",
    "\n",
    "### 7 Modeling Linguistic Patterns\n",
    "\n",
    "#### 7.1 What do models tell us?\n",
    "\n",
    "Descriptive models can capture patterns in data, but can't provide any information about *why* the data contain these patterns.  That's the job of explanatory models.\n",
    "\n",
    "### 8 Summary\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "### 9 Further Reading\n",
    "\n",
    "*__No notes.__*\n",
    "\n",
    "### 10 Exercises\n",
    "\n",
    "##### 1. \n",
    "\n",
    "☼ Read up on one of the language technologies mentioned in this section, such as word sense disambiguation, semantic role labeling, question answering, machine translation, named entity detection. Find out what type and quantity of annotated data is required for developing such systems. Why do you think a large amount of data is required?\n",
    "\n",
    "*Since this book was released there has been a tectonic shift in labeled models, and instead of using large corpora of annotated data the trend now is to use massive corpora of unlabeled data.  Simply put, we now have the computing power and data stores to analyze these massive amounts of data.  With models such as Word2Vec, machines can discover the relationships between semantic ideas with no need for human annotations.*\n",
    "\n",
    "*To answer the question as it was posed: As was pointed out multiple times in this book, models can only discover phenomena that are present in their training data, and large corpora are required to guarantee that less frequent phenomena are represented often enough to be registered by the model.  Some models, such as decision trees, cannot learn phenomena unless they are represented a certain number of times.*\n",
    "\n",
    "##### 2.\n",
    "\n",
    "☼ Using any of the three classifiers described in this chapter, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "\n",
    "*Since we've practiced using Naive Bayes and Decision Trees, I decided I would try a Maximum Entropy Classifier, as we have only discussed this theoretically.  It wasn't so easy to find examples of ME in use, but I did find a brief one [here](http://www.nltk.org/howto/classify.html \"MaxEnt Example\").  With MaxEnt we have to designate the number of iterations.*\n",
    "\n",
    "*I also spent a lot of time refining this classifier when I was working throught the notes in the chapter, so for this exercise I mostly just pulled out the code I used for that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.372\n",
      "             2          -0.37420        0.761\n",
      "             3          -0.37380        0.761\n",
      "             4          -0.37356        0.761\n",
      "             5          -0.37340        0.761\n",
      "             6          -0.37329        0.761\n",
      "             7          -0.37320        0.761\n",
      "             8          -0.37313        0.761\n",
      "             9          -0.37308        0.761\n",
      "         Final          -0.37304        0.761\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, devtest_set, test_set = featuresets[1000:], featuresets[500:1000], featuresets[:500]\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, max_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_names = labeled_names[500:1000]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = female   guess = male     name = Abigail                       \n",
      "correct = female   guess = male     name = Allis                         \n",
      "correct = female   guess = male     name = Amber                         \n",
      "correct = female   guess = male     name = Annabel                       \n",
      "correct = female   guess = male     name = Avrit                         \n",
      "correct = female   guess = male     name = Beatriz                       \n",
      "correct = female   guess = male     name = Bell                          \n",
      "correct = female   guess = male     name = Berget                        \n",
      "correct = female   guess = male     name = Bird                          \n",
      "correct = female   guess = male     name = Brier                         \n",
      "correct = female   guess = male     name = Brigid                        \n",
      "correct = female   guess = male     name = Carlin                        \n",
      "correct = female   guess = male     name = Caryn                         \n",
      "correct = female   guess = male     name = Cathleen                      \n",
      "correct = female   guess = male     name = Charlean                      \n",
      "correct = female   guess = male     name = Christen                      \n",
      "correct = female   guess = male     name = Consuelo                      \n",
      "correct = female   guess = male     name = Corliss                       \n",
      "correct = female   guess = male     name = Cristen                       \n",
      "correct = female   guess = male     name = Dael                          \n",
      "correct = female   guess = male     name = Dion                          \n",
      "correct = female   guess = male     name = Dyan                          \n",
      "correct = female   guess = male     name = Ellen                         \n",
      "correct = female   guess = male     name = Emlyn                         \n",
      "correct = female   guess = male     name = Emmalyn                       \n",
      "correct = female   guess = male     name = Erin                          \n",
      "correct = female   guess = male     name = Felicdad                      \n",
      "correct = female   guess = male     name = Gennifer                      \n",
      "correct = female   guess = male     name = Grethel                       \n",
      "correct = female   guess = male     name = Janel                         \n",
      "correct = female   guess = male     name = Janot                         \n",
      "correct = female   guess = male     name = Jaynell                       \n",
      "correct = female   guess = male     name = Jillian                       \n",
      "correct = female   guess = male     name = Joleen                        \n",
      "correct = female   guess = male     name = Kameko                        \n",
      "correct = female   guess = male     name = Karlyn                        \n",
      "correct = female   guess = male     name = Kaylil                        \n",
      "correct = female   guess = male     name = Kerrin                        \n",
      "correct = female   guess = male     name = Linnet                        \n",
      "correct = female   guess = male     name = Liz                           \n",
      "correct = female   guess = male     name = Loralyn                       \n",
      "correct = female   guess = male     name = Lyndell                       \n",
      "correct = female   guess = male     name = Lynnell                       \n",
      "correct = female   guess = male     name = Mab                           \n",
      "correct = female   guess = male     name = Mariam                        \n",
      "correct = female   guess = male     name = Meghann                       \n",
      "correct = female   guess = male     name = Moreen                        \n",
      "correct = female   guess = male     name = Muffin                        \n",
      "correct = female   guess = male     name = Phil                          \n",
      "correct = female   guess = male     name = Quentin                       \n",
      "correct = female   guess = male     name = Row                           \n",
      "correct = female   guess = male     name = Ryann                         \n",
      "correct = female   guess = male     name = Sal                           \n",
      "correct = female   guess = male     name = Sharleen                      \n",
      "correct = female   guess = male     name = Sharyl                        \n",
      "correct = female   guess = male     name = Sheril                        \n",
      "correct = female   guess = male     name = Shir                          \n",
      "correct = female   guess = male     name = Tatum                         \n",
      "correct = female   guess = male     name = Teriann                       \n",
      "correct = female   guess = male     name = Violet                        \n",
      "correct = female   guess = male     name = Wandis                        \n",
      "correct = female   guess = male     name = Yoshiko                       \n",
      "correct = male     guess = female   name = Alaa                          \n",
      "correct = male     guess = female   name = Andrea                        \n",
      "correct = male     guess = female   name = Artie                         \n",
      "correct = male     guess = female   name = Barnabe                       \n",
      "correct = male     guess = female   name = Berkley                       \n",
      "correct = male     guess = female   name = Broddie                       \n",
      "correct = male     guess = female   name = Carleigh                      \n",
      "correct = male     guess = female   name = Cary                          \n",
      "correct = male     guess = female   name = Charlie                       \n",
      "correct = male     guess = female   name = Chauncey                      \n",
      "correct = male     guess = female   name = Clayborne                     \n",
      "correct = male     guess = female   name = Clemente                      \n",
      "correct = male     guess = female   name = Corey                         \n",
      "correct = male     guess = female   name = Corrie                        \n",
      "correct = male     guess = female   name = Davey                         \n",
      "correct = male     guess = female   name = Davidde                       \n",
      "correct = male     guess = female   name = Demetre                       \n",
      "correct = male     guess = female   name = Ellsworth                     \n",
      "correct = male     guess = female   name = Elmore                        \n",
      "correct = male     guess = female   name = Ezra                          \n",
      "correct = male     guess = female   name = Felipe                        \n",
      "correct = male     guess = female   name = Freddy                        \n",
      "correct = male     guess = female   name = Gale                          \n",
      "correct = male     guess = female   name = Garey                         \n",
      "correct = male     guess = female   name = Germaine                      \n",
      "correct = male     guess = female   name = Gerome                        \n",
      "correct = male     guess = female   name = Gregory                       \n",
      "correct = male     guess = female   name = Hamish                        \n",
      "correct = male     guess = female   name = Harley                        \n",
      "correct = male     guess = female   name = Ike                           \n",
      "correct = male     guess = female   name = Jae                           \n",
      "correct = male     guess = female   name = Jessey                        \n",
      "correct = male     guess = female   name = Jordy                         \n",
      "correct = male     guess = female   name = Judah                         \n",
      "correct = male     guess = female   name = Kerry                         \n",
      "correct = male     guess = female   name = Knox                          \n",
      "correct = male     guess = female   name = Mahesh                        \n",
      "correct = male     guess = female   name = Marsh                         \n",
      "correct = male     guess = female   name = Mattie                        \n",
      "correct = male     guess = female   name = Meade                         \n",
      "correct = male     guess = female   name = Mikey                         \n",
      "correct = male     guess = female   name = Natale                        \n",
      "correct = male     guess = female   name = Nate                          \n",
      "correct = male     guess = female   name = Noe                           \n",
      "correct = male     guess = female   name = Ollie                         \n",
      "correct = male     guess = female   name = Pearce                        \n",
      "correct = male     guess = female   name = Price                         \n",
      "correct = male     guess = female   name = Ramsay                        \n",
      "correct = male     guess = female   name = Reza                          \n",
      "correct = male     guess = female   name = Roni                          \n",
      "correct = male     guess = female   name = Ronnie                        \n",
      "correct = male     guess = female   name = Roth                          \n",
      "correct = male     guess = female   name = Shelby                        \n",
      "correct = male     guess = female   name = Sonnie                        \n",
      "correct = male     guess = female   name = Stacy                         \n",
      "correct = male     guess = female   name = Tammy                         \n",
      "correct = male     guess = female   name = Timmy                         \n",
      "correct = male     guess = female   name = Tracie                        \n",
      "correct = male     guess = female   name = Tremayne                      \n",
      "correct = male     guess = female   name = Woodie                        \n",
      "correct = male     guess = female   name = Yale                          \n",
      "correct = male     guess = female   name = Yuri                          \n",
      "correct = male     guess = female   name = Zacharie                      \n"
     ]
    }
   ],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct = {:<8} guess = {:<8} name = {:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1],\n",
    "            'length'     : len(word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (6 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.372\n",
      "             2          -0.44632        0.751\n",
      "             3          -0.40277        0.771\n",
      "             4          -0.38633        0.767\n",
      "             5          -0.37900        0.767\n",
      "         Final          -0.37543        0.765\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, devtest_set, test_set = featuresets[1000:], featuresets[500:1000], featuresets[:500]\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, max_iter = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = female   guess = male     name = Allis                         \n",
      "correct = female   guess = male     name = Amber                         \n",
      "correct = female   guess = male     name = Avrit                         \n",
      "correct = female   guess = male     name = Beatriz                       \n",
      "correct = female   guess = male     name = Bell                          \n",
      "correct = female   guess = male     name = Berget                        \n",
      "correct = female   guess = male     name = Bird                          \n",
      "correct = female   guess = male     name = Brier                         \n",
      "correct = female   guess = male     name = Brigid                        \n",
      "correct = female   guess = male     name = Carlin                        \n",
      "correct = female   guess = male     name = Caryn                         \n",
      "correct = female   guess = male     name = Consuelo                      \n",
      "correct = female   guess = male     name = Corliss                       \n",
      "correct = female   guess = male     name = Cristen                       \n",
      "correct = female   guess = male     name = Dael                          \n",
      "correct = female   guess = male     name = Dion                          \n",
      "correct = female   guess = male     name = Dyan                          \n",
      "correct = female   guess = male     name = Ellen                         \n",
      "correct = female   guess = male     name = Emlyn                         \n",
      "correct = female   guess = male     name = Emmalyn                       \n",
      "correct = female   guess = male     name = Erin                          \n",
      "correct = female   guess = male     name = Felicdad                      \n",
      "correct = female   guess = male     name = Gennifer                      \n",
      "correct = female   guess = male     name = Janel                         \n",
      "correct = female   guess = male     name = Janot                         \n",
      "correct = female   guess = male     name = Jillian                       \n",
      "correct = female   guess = male     name = Joleen                        \n",
      "correct = female   guess = male     name = Kameko                        \n",
      "correct = female   guess = male     name = Karlyn                        \n",
      "correct = female   guess = male     name = Kaylil                        \n",
      "correct = female   guess = male     name = Kerrin                        \n",
      "correct = female   guess = male     name = Linnet                        \n",
      "correct = female   guess = male     name = Liz                           \n",
      "correct = female   guess = male     name = Loralyn                       \n",
      "correct = female   guess = male     name = Mab                           \n",
      "correct = female   guess = male     name = Mariam                        \n",
      "correct = female   guess = male     name = Meghann                       \n",
      "correct = female   guess = male     name = Moreen                        \n",
      "correct = female   guess = male     name = Muffin                        \n",
      "correct = female   guess = male     name = Phil                          \n",
      "correct = female   guess = male     name = Quentin                       \n",
      "correct = female   guess = male     name = Row                           \n",
      "correct = female   guess = male     name = Ryann                         \n",
      "correct = female   guess = male     name = Sal                           \n",
      "correct = female   guess = male     name = Sharyl                        \n",
      "correct = female   guess = male     name = Sheril                        \n",
      "correct = female   guess = male     name = Shir                          \n",
      "correct = female   guess = male     name = Tatum                         \n",
      "correct = female   guess = male     name = Teriann                       \n",
      "correct = female   guess = male     name = Violet                        \n",
      "correct = female   guess = male     name = Wandis                        \n",
      "correct = female   guess = male     name = Yoshiko                       \n",
      "correct = male     guess = female   name = Alaa                          \n",
      "correct = male     guess = female   name = Andrea                        \n",
      "correct = male     guess = female   name = Artie                         \n",
      "correct = male     guess = female   name = Barnabe                       \n",
      "correct = male     guess = female   name = Benjamen                      \n",
      "correct = male     guess = female   name = Benjamin                      \n",
      "correct = male     guess = female   name = Berkley                       \n",
      "correct = male     guess = female   name = Broddie                       \n",
      "correct = male     guess = female   name = Carleigh                      \n",
      "correct = male     guess = female   name = Cary                          \n",
      "correct = male     guess = female   name = Charlie                       \n",
      "correct = male     guess = female   name = Chauncey                      \n",
      "correct = male     guess = female   name = Clayborne                     \n",
      "correct = male     guess = female   name = Clemente                      \n",
      "correct = male     guess = female   name = Corey                         \n",
      "correct = male     guess = female   name = Corrie                        \n",
      "correct = male     guess = female   name = Davey                         \n",
      "correct = male     guess = female   name = Davidde                       \n",
      "correct = male     guess = female   name = Demetre                       \n",
      "correct = male     guess = female   name = Ellsworth                     \n",
      "correct = male     guess = female   name = Elmore                        \n",
      "correct = male     guess = female   name = Ezechiel                      \n",
      "correct = male     guess = female   name = Ezra                          \n",
      "correct = male     guess = female   name = Felipe                        \n",
      "correct = male     guess = female   name = Freddy                        \n",
      "correct = male     guess = female   name = Gale                          \n",
      "correct = male     guess = female   name = Garey                         \n",
      "correct = male     guess = female   name = Garfinkel                     \n",
      "correct = male     guess = female   name = Germaine                      \n",
      "correct = male     guess = female   name = Gerome                        \n",
      "correct = male     guess = female   name = Gregory                       \n",
      "correct = male     guess = female   name = Gretchen                      \n",
      "correct = male     guess = female   name = Hamish                        \n",
      "correct = male     guess = female   name = Hannibal                      \n",
      "correct = male     guess = female   name = Harley                        \n",
      "correct = male     guess = female   name = Hartwell                      \n",
      "correct = male     guess = female   name = Ike                           \n",
      "correct = male     guess = female   name = Immanuel                      \n",
      "correct = male     guess = female   name = Jae                           \n",
      "correct = male     guess = female   name = Jessey                        \n",
      "correct = male     guess = female   name = Johnathan                     \n",
      "correct = male     guess = female   name = Jordy                         \n",
      "correct = male     guess = female   name = Judah                         \n",
      "correct = male     guess = female   name = Kerry                         \n",
      "correct = male     guess = female   name = Kingston                      \n",
      "correct = male     guess = female   name = Knox                          \n",
      "correct = male     guess = female   name = Mahesh                        \n",
      "correct = male     guess = female   name = Marsh                         \n",
      "correct = male     guess = female   name = Mattie                        \n",
      "correct = male     guess = female   name = Meade                         \n",
      "correct = male     guess = female   name = Mikey                         \n",
      "correct = male     guess = female   name = Natale                        \n",
      "correct = male     guess = female   name = Nate                          \n",
      "correct = male     guess = female   name = Noe                           \n",
      "correct = male     guess = female   name = Ollie                         \n",
      "correct = male     guess = female   name = Pearce                        \n",
      "correct = male     guess = female   name = Price                         \n",
      "correct = male     guess = female   name = Ramsay                        \n",
      "correct = male     guess = female   name = Ransell                       \n",
      "correct = male     guess = female   name = Reza                          \n",
      "correct = male     guess = female   name = Roni                          \n",
      "correct = male     guess = female   name = Ronnie                        \n",
      "correct = male     guess = female   name = Roth                          \n",
      "correct = male     guess = female   name = Shelby                        \n",
      "correct = male     guess = female   name = Sonnie                        \n",
      "correct = male     guess = female   name = Stacy                         \n",
      "correct = male     guess = female   name = Tammy                         \n",
      "correct = male     guess = female   name = Thurston                      \n",
      "correct = male     guess = female   name = Timmy                         \n",
      "correct = male     guess = female   name = Tracie                        \n",
      "correct = male     guess = female   name = Tremayne                      \n",
      "correct = male     guess = female   name = Woodie                        \n",
      "correct = male     guess = female   name = Yale                          \n",
      "correct = male     guess = female   name = Yuri                          \n",
      "correct = male     guess = female   name = Zacharie                      \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name))\n",
    "\n",
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct = {:<8} guess = {:<8} name = {:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2],\n",
    "            'first_vowel'  : [i for i in range(len(word)) \n",
    "                              if word[i] in 'AEIOUaeiouy'][0],\n",
    "            'double_letters': sum([1 for ch in range(len(word) - 1)\n",
    "                                  if word[ch] == word[ch + 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (7 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.372\n",
      "             2          -0.47554        0.746\n",
      "             3          -0.40732        0.810\n",
      "             4          -0.36944        0.818\n",
      "             5          -0.34610        0.824\n",
      "             6          -0.33044        0.827\n",
      "         Final          -0.31925        0.828\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, devtest_set, test_set = featuresets[1000:], featuresets[500:1000], featuresets[:500]\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, max_iter = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = female   guess = male     name = Abigail                       \n",
      "correct = female   guess = male     name = Amber                         \n",
      "correct = female   guess = male     name = Avrit                         \n",
      "correct = female   guess = male     name = Barbey                        \n",
      "correct = female   guess = male     name = Barby                         \n",
      "correct = female   guess = male     name = Beatriz                       \n",
      "correct = female   guess = male     name = Bell                          \n",
      "correct = female   guess = male     name = Bird                          \n",
      "correct = female   guess = male     name = Brier                         \n",
      "correct = female   guess = male     name = Brigid                        \n",
      "correct = female   guess = male     name = Charlean                      \n",
      "correct = female   guess = male     name = Consuelo                      \n",
      "correct = female   guess = male     name = Dael                          \n",
      "correct = female   guess = male     name = Dion                          \n",
      "correct = female   guess = male     name = Dyan                          \n",
      "correct = female   guess = male     name = Erin                          \n",
      "correct = female   guess = male     name = Felicdad                      \n",
      "correct = female   guess = male     name = Fey                           \n",
      "correct = female   guess = male     name = Gennifer                      \n",
      "correct = female   guess = male     name = Georgiamay                    \n",
      "correct = female   guess = male     name = Grethel                       \n",
      "correct = female   guess = male     name = Harley                        \n",
      "correct = female   guess = male     name = Liz                           \n",
      "correct = female   guess = male     name = Mab                           \n",
      "correct = female   guess = male     name = Mariam                        \n",
      "correct = female   guess = male     name = Modesty                       \n",
      "correct = female   guess = male     name = Moreen                        \n",
      "correct = female   guess = male     name = Muffin                        \n",
      "correct = female   guess = male     name = Phil                          \n",
      "correct = female   guess = male     name = Quentin                       \n",
      "correct = female   guess = male     name = Randy                         \n",
      "correct = female   guess = male     name = Ranice                        \n",
      "correct = female   guess = male     name = Row                           \n",
      "correct = female   guess = male     name = Sal                           \n",
      "correct = female   guess = male     name = Scotty                        \n",
      "correct = female   guess = male     name = Sheril                        \n",
      "correct = female   guess = male     name = Shir                          \n",
      "correct = female   guess = male     name = Tandy                         \n",
      "correct = female   guess = male     name = Tatum                         \n",
      "correct = female   guess = male     name = Tony                          \n",
      "correct = female   guess = male     name = Wandis                        \n",
      "correct = female   guess = male     name = Wendie                        \n",
      "correct = female   guess = male     name = Wynny                         \n",
      "correct = male     guess = female   name = Alaa                          \n",
      "correct = male     guess = female   name = Allyn                         \n",
      "correct = male     guess = female   name = Anatol                        \n",
      "correct = male     guess = female   name = Andrea                        \n",
      "correct = male     guess = female   name = Artie                         \n",
      "correct = male     guess = female   name = Benjamen                      \n",
      "correct = male     guess = female   name = Berkley                       \n",
      "correct = male     guess = female   name = Broddie                       \n",
      "correct = male     guess = female   name = Carleigh                      \n",
      "correct = male     guess = female   name = Cary                          \n",
      "correct = male     guess = female   name = Cecil                         \n",
      "correct = male     guess = female   name = Charlie                       \n",
      "correct = male     guess = female   name = Chauncey                      \n",
      "correct = male     guess = female   name = Chris                         \n",
      "correct = male     guess = female   name = Clayborne                     \n",
      "correct = male     guess = female   name = Clemente                      \n",
      "correct = male     guess = female   name = Corey                         \n",
      "correct = male     guess = female   name = Corrie                        \n",
      "correct = male     guess = female   name = Davey                         \n",
      "correct = male     guess = female   name = Davidde                       \n",
      "correct = male     guess = female   name = Demetre                       \n",
      "correct = male     guess = female   name = Demosthenis                   \n",
      "correct = male     guess = female   name = Elliott                       \n",
      "correct = male     guess = female   name = Ellsworth                     \n",
      "correct = male     guess = female   name = Elmore                        \n",
      "correct = male     guess = female   name = Ezra                          \n",
      "correct = male     guess = female   name = Fazeel                        \n",
      "correct = male     guess = female   name = Felipe                        \n",
      "correct = male     guess = female   name = Freddy                        \n",
      "correct = male     guess = female   name = Gale                          \n",
      "correct = male     guess = female   name = Germaine                      \n",
      "correct = male     guess = female   name = Immanuel                      \n",
      "correct = male     guess = female   name = Jabez                         \n",
      "correct = male     guess = female   name = Jae                           \n",
      "correct = male     guess = female   name = Jessey                        \n",
      "correct = male     guess = female   name = Jordy                         \n",
      "correct = male     guess = female   name = Judah                         \n",
      "correct = male     guess = female   name = Juergen                       \n",
      "correct = male     guess = female   name = Kalvin                        \n",
      "correct = male     guess = female   name = Kerry                         \n",
      "correct = male     guess = female   name = Knox                          \n",
      "correct = male     guess = female   name = Mahesh                        \n",
      "correct = male     guess = female   name = Marsh                         \n",
      "correct = male     guess = female   name = Mattie                        \n",
      "correct = male     guess = female   name = Meade                         \n",
      "correct = male     guess = female   name = Merrel                        \n",
      "correct = male     guess = female   name = Mikey                         \n",
      "correct = male     guess = female   name = Natale                        \n",
      "correct = male     guess = female   name = Nate                          \n",
      "correct = male     guess = female   name = Ollie                         \n",
      "correct = male     guess = female   name = Pearce                        \n",
      "correct = male     guess = female   name = Price                         \n",
      "correct = male     guess = female   name = Reza                          \n",
      "correct = male     guess = female   name = Roni                          \n",
      "correct = male     guess = female   name = Ronnie                        \n",
      "correct = male     guess = female   name = Roth                          \n",
      "correct = male     guess = female   name = Sonnie                        \n",
      "correct = male     guess = female   name = Stacy                         \n",
      "correct = male     guess = female   name = Tammy                         \n",
      "correct = male     guess = female   name = Timmy                         \n",
      "correct = male     guess = female   name = Tracie                        \n",
      "correct = male     guess = female   name = Tremayne                      \n",
      "correct = male     guess = female   name = Yale                          \n",
      "correct = male     guess = female   name = Yuri                          \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name))\n",
    "\n",
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct = {:<8} guess = {:<8} name = {:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2],\n",
    "            'number_vowels': sum([1 for ch in word if ch in 'AEIOUaeiouy']),\n",
    "            'first_vowel'  : [i for i in range(len(word)) \n",
    "                              if word[i] in 'AEIOUaeiouy'][0],\n",
    "            'double_letters': sum([1 for ch in range(len(word) - 1)\n",
    "                                  if word[ch] == word[ch + 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.372\n",
      "             2          -0.48467        0.731\n",
      "             3          -0.41699        0.807\n",
      "             4          -0.37809        0.816\n",
      "         Final          -0.35360        0.824\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, devtest_set, test_set = featuresets[1000:], featuresets[500:1000], featuresets[:500]\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, max_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = female   guess = male     name = Abigail                       \n",
      "correct = female   guess = male     name = Amber                         \n",
      "correct = female   guess = male     name = Avrit                         \n",
      "correct = female   guess = male     name = Barbey                        \n",
      "correct = female   guess = male     name = Barby                         \n",
      "correct = female   guess = male     name = Beatriz                       \n",
      "correct = female   guess = male     name = Bell                          \n",
      "correct = female   guess = male     name = Bird                          \n",
      "correct = female   guess = male     name = Brier                         \n",
      "correct = female   guess = male     name = Brigid                        \n",
      "correct = female   guess = male     name = Consuelo                      \n",
      "correct = female   guess = male     name = Dion                          \n",
      "correct = female   guess = male     name = Dyan                          \n",
      "correct = female   guess = male     name = Erin                          \n",
      "correct = female   guess = male     name = Felicdad                      \n",
      "correct = female   guess = male     name = Fey                           \n",
      "correct = female   guess = male     name = Gennifer                      \n",
      "correct = female   guess = male     name = Grethel                       \n",
      "correct = female   guess = male     name = Harley                        \n",
      "correct = female   guess = male     name = Janot                         \n",
      "correct = female   guess = male     name = Liz                           \n",
      "correct = female   guess = male     name = Mab                           \n",
      "correct = female   guess = male     name = Mariam                        \n",
      "correct = female   guess = male     name = Muffin                        \n",
      "correct = female   guess = male     name = Phil                          \n",
      "correct = female   guess = male     name = Quentin                       \n",
      "correct = female   guess = male     name = Randy                         \n",
      "correct = female   guess = male     name = Row                           \n",
      "correct = female   guess = male     name = Sal                           \n",
      "correct = female   guess = male     name = Scotty                        \n",
      "correct = female   guess = male     name = Sheril                        \n",
      "correct = female   guess = male     name = Shir                          \n",
      "correct = female   guess = male     name = Tatum                         \n",
      "correct = female   guess = male     name = Tony                          \n",
      "correct = female   guess = male     name = Wandis                        \n",
      "correct = female   guess = male     name = Wendie                        \n",
      "correct = female   guess = male     name = Wynny                         \n",
      "correct = male     guess = female   name = Alaa                          \n",
      "correct = male     guess = female   name = Allyn                         \n",
      "correct = male     guess = female   name = Anatol                        \n",
      "correct = male     guess = female   name = Andrea                        \n",
      "correct = male     guess = female   name = Artie                         \n",
      "correct = male     guess = female   name = Benjamen                      \n",
      "correct = male     guess = female   name = Benjamin                      \n",
      "correct = male     guess = female   name = Berkley                       \n",
      "correct = male     guess = female   name = Broddie                       \n",
      "correct = male     guess = female   name = Carleigh                      \n",
      "correct = male     guess = female   name = Cary                          \n",
      "correct = male     guess = female   name = Cecil                         \n",
      "correct = male     guess = female   name = Charlie                       \n",
      "correct = male     guess = female   name = Chauncey                      \n",
      "correct = male     guess = female   name = Chris                         \n",
      "correct = male     guess = female   name = Clayborne                     \n",
      "correct = male     guess = female   name = Clemente                      \n",
      "correct = male     guess = female   name = Corey                         \n",
      "correct = male     guess = female   name = Corrie                        \n",
      "correct = male     guess = female   name = Davey                         \n",
      "correct = male     guess = female   name = Davidde                       \n",
      "correct = male     guess = female   name = Demetre                       \n",
      "correct = male     guess = female   name = Demosthenis                   \n",
      "correct = male     guess = female   name = Elliott                       \n",
      "correct = male     guess = female   name = Ellsworth                     \n",
      "correct = male     guess = female   name = Elmore                        \n",
      "correct = male     guess = female   name = Ezechiel                      \n",
      "correct = male     guess = female   name = Ezra                          \n",
      "correct = male     guess = female   name = Fazeel                        \n",
      "correct = male     guess = female   name = Felipe                        \n",
      "correct = male     guess = female   name = Freddy                        \n",
      "correct = male     guess = female   name = Gale                          \n",
      "correct = male     guess = female   name = Garey                         \n",
      "correct = male     guess = female   name = Garfinkel                     \n",
      "correct = male     guess = female   name = Germaine                      \n",
      "correct = male     guess = female   name = Gerome                        \n",
      "correct = male     guess = female   name = Gregory                       \n",
      "correct = male     guess = female   name = Immanuel                      \n",
      "correct = male     guess = female   name = Jae                           \n",
      "correct = male     guess = female   name = Jessey                        \n",
      "correct = male     guess = female   name = Johnathan                     \n",
      "correct = male     guess = female   name = Jordy                         \n",
      "correct = male     guess = female   name = Judah                         \n",
      "correct = male     guess = female   name = Juergen                       \n",
      "correct = male     guess = female   name = Kalvin                        \n",
      "correct = male     guess = female   name = Kerry                         \n",
      "correct = male     guess = female   name = Mahesh                        \n",
      "correct = male     guess = female   name = Mattie                        \n",
      "correct = male     guess = female   name = Meade                         \n",
      "correct = male     guess = female   name = Merrel                        \n",
      "correct = male     guess = female   name = Mikey                         \n",
      "correct = male     guess = female   name = Natale                        \n",
      "correct = male     guess = female   name = Nate                          \n",
      "correct = male     guess = female   name = Ollie                         \n",
      "correct = male     guess = female   name = Pearce                        \n",
      "correct = male     guess = female   name = Price                         \n",
      "correct = male     guess = female   name = Reza                          \n",
      "correct = male     guess = female   name = Roni                          \n",
      "correct = male     guess = female   name = Ronnie                        \n",
      "correct = male     guess = female   name = Roth                          \n",
      "correct = male     guess = female   name = Sonnie                        \n",
      "correct = male     guess = female   name = Stacy                         \n",
      "correct = male     guess = female   name = Tammy                         \n",
      "correct = male     guess = female   name = Timmy                         \n",
      "correct = male     guess = female   name = Tracie                        \n",
      "correct = male     guess = female   name = Tremayne                      \n",
      "correct = male     guess = female   name = Woodie                        \n",
      "correct = male     guess = female   name = Yale                          \n",
      "correct = male     guess = female   name = Yuri                          \n",
      "correct = male     guess = female   name = Zacharie                      \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name))\n",
    "\n",
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct = {:<8} guess = {:<8} name = {:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The accuracy of the classifier is slightly less when evaluating the test set (82.2%) than when evaluating the devtest set (84.2%).  Frankly, I would credit this more to chance than anything else.  I went through the errors in the devtest set several times, and I don't think there are any more fine distinctions that I could tease out from the errors.  Some of the names even I would have guessed incorrectly.  Short of having a lookup table, I don't think we can do anything to make this more accurate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "\n",
    "☼ The Senseval 2 Corpus contains data intended to train word-sense disambiguation classifiers. It contains data for four words: hard, interest, line, and serve. Choose one of these four words, and load the corresponding data:\n",
    "\n",
    "\n",
    "```\n",
    "from nltk.corpus import senseval\n",
    "instances = senseval.instances('hard.pos')\n",
    "size = int(len(instances) * 0.1)\n",
    "train_set, test_set = instances[size:], instances[:size]\n",
    "```\n",
    "\n",
    "Using this dataset, build a classifier that predicts the correct sense tag for a given instance. See the corpus HOWTO [here](http://nltk.org/howto \"HOWTO\") for information on using the instance objects returned by the Senseval 2 Corpus.\n",
    "\n",
    "*I feel a bit like I was led up the garden path with the code given in the instructions for this exercise.  It's easy enough to access the instance objects from the Senseval 2 corpus; the problem is with the train/test split suggested by the book.  Some exploratory analysis leads me to strongly believe that the data is ordered, and plotting confirms this:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAUSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import senseval\n",
    "import random\n",
    "instances = senseval.instances('hard.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses = [instances[i].senses[0][4] for i in range(len(instances))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(instances)), senses, alpha = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So if had just blindly followed the recommended code in the book, the test set would have been comprised solely of instances with sense 1.*\n",
    "\n",
    "*Let's randomize the ordering of the instances.  The instances have to be converted to a `list` so that they can be shuffled:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONG RUN HERE\n",
    "instances = list(instances)\n",
    "random.shuffle(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "senses = [int(instances[i].senses[0][4]) for i in range(len(instances))]\n",
    "\n",
    "plt.scatter(range(len(instances)), senses, alpha = 0.1)\n",
    "plt.yticks([1, 2, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There aren't that many features we can extract from instances, and simple trial and error reveals that these are the best.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_features(inst):\n",
    "    p = inst.position\n",
    "    return {'left2':  inst.context[p - 2],\n",
    "            'left1':  inst.context[p - 1],\n",
    "            'right':  inst.context[p + 1]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left2': ('but', 'CC'), 'left1': ('no', 'DT'), 'right': ('answers', 'NNS')}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_features(shuffled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(hard_features(n), n.senses) for n in instances]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9030023094688222"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating feature, training, and test sets:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HARD1',)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(hard_features(instances[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SensevalInstance(word='hard-a', position=3, context=[('``', '``'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('play', 'VB'), ('against', 'IN'), ('them', 'PRP'), ('in', 'IN'), ('practice', 'NN'), ('and', 'CC'), ('they', 'PRP'), ('don', 'VBP'), (\"'t\", 'VBG'), ('even', 'RB'), ('tackle', 'VB'), ('us', 'PRP'), (';', ':'), ('what', 'WP'), ('they', 'PRP'), ('do', 'VBP'), ('is', 'VBZ'), ('something', 'NN'), ('called', 'VBD'), ('button-up', 'JJ'), ('(', '('), ('halt', 'NN'), ('the', 'DT'), ('runners', 'NNS'), (')', 'SYM'), ('and', 'CC'), ('it', 'PRP'), ('gets', 'VBZ'), ('so', 'RB'), ('physical', 'JJ'), ('out', 'IN'), ('there', 'EX'), ('and', 'CC'), ('they', 'PRP'), ('get', 'VBP'), ('so', 'RB'), ('pumped', 'VBN'), ('up', 'IN'), (',', ','), ('they', 'PRP'), ('just', 'RB'), ('forget', 'VBP'), ('that', 'IN'), ('we', 'PRP'), (\"'re\", 'VBP'), ('on', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('team', 'NN'), ('.', '.')], senses=('HARD1',))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.\n",
    "\n",
    "☼ Using the movie review document classifier discussed in this chapter, generate a list of the 30 features that the classifier finds to be most informative. Can you explain why these particular features are informative? Do you find any of them surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "    contains(schumacher) = True              neg : pos    =      7.5 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      7.1 : 1.0\n",
      "       contains(martian) = True              neg : pos    =      7.1 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      7.1 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      7.1 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      7.1 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      7.1 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.6 : 1.0\n",
      "           contains(ugh) = True              neg : pos    =      5.8 : 1.0\n",
      "       contains(bronson) = True              neg : pos    =      5.7 : 1.0\n",
      "       contains(singers) = True              pos : neg    =      5.6 : 1.0\n",
      "       contains(unravel) = True              pos : neg    =      5.6 : 1.0\n",
      "        contains(justin) = True              neg : pos    =      5.4 : 1.0\n",
      "     contains(stretched) = True              neg : pos    =      5.4 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      5.3 : 1.0\n",
      "        contains(poorly) = True              neg : pos    =      5.2 : 1.0\n",
      "         contains(awful) = True              neg : pos    =      5.2 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      5.1 : 1.0\n",
      "    contains(ridiculous) = True              neg : pos    =      5.0 : 1.0\n",
      "        contains(welles) = True              neg : pos    =      5.0 : 1.0\n",
      "        contains(canyon) = True              neg : pos    =      5.0 : 1.0\n",
      "     contains(underwood) = True              neg : pos    =      5.0 : 1.0\n",
      "       contains(miscast) = True              neg : pos    =      4.8 : 1.0\n",
      "         contains(kudos) = True              pos : neg    =      4.7 : 1.0\n",
      "        contains(sexist) = True              neg : pos    =      4.6 : 1.0\n",
      "    contains(uninspired) = True              neg : pos    =      4.6 : 1.0\n",
      "      contains(explores) = True              pos : neg    =      4.5 : 1.0\n",
      "      contains(banality) = True              neg : pos    =      4.4 : 1.0\n",
      "           contains(h20) = True              neg : pos    =      4.4 : 1.0\n",
      "          contains(oops) = True              neg : pos    =      4.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most of the most informative features are strong adjectives (e.g., 'atrocious', 'shoddy', 'unimaginative', ...) so no surprise there .  There are also quite a few names, both first and last.  I was surprised to see the name 'welles' as one of the features, as this almost surely refers to Orson Welles, and the name highly correlates with a negative review.  While Orson Welles took on roles in some questionable movies later in life, his earlier films (e.g., \"Citizen Kane\") are considered to be masterpieces of cinema.*\n",
    "\n",
    "##### 5. \n",
    "\n",
    "☼ Select one of the classification tasks described in this chapter, such as name gender detection, document classification, part-of-speech tagging, or dialog act classification. Using the same training and test data, and the same feature extractor, build three classifiers for the task: a decision tree, a naive Bayes classifier, and a Maximum Entropy classifier. Compare the performance of the three classifiers on your selected task. How do you think that your results might be different if you used a different feature extractor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I actually tried this exercise with several of the classification tasks from this chapter - namely, part-of-speech tagging, dialogue act classifcation, and name gender detection.  However, the decision tree classifier is so painfully slow - taking over 30 minutes in some cases - that I decided I could only use it with the most basic classification task, i.e., name gender detection.*\n",
    "\n",
    "*As you can see below, the MaxEnt classifier had the best accuracy, followed by the Naive Bayes classifier and the Decision Tree.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1],\n",
    "            'length'       : len(word),\n",
    "            'first_letter' : word[0],\n",
    "            'last_2letters': word[-2:],\n",
    "            'first_2letters': word[:2],\n",
    "            'number_vowels': sum([1 for ch in word if ch in 'AEIOUaeiouy']),\n",
    "            'first_vowel'  : [i for i in range(len(word)) \n",
    "                              if word[i] in 'AEIOUaeiouy'][0],\n",
    "            'double_letters': sum([1 for ch in range(len(word) - 1)\n",
    "                                  if word[ch] == word[ch + 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (50 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.48462        0.735\n",
      "             3          -0.41700        0.804\n",
      "             4          -0.37818        0.816\n",
      "             5          -0.35383        0.822\n",
      "             6          -0.33730        0.826\n",
      "             7          -0.32540        0.829\n",
      "             8          -0.31643        0.831\n",
      "             9          -0.30942        0.831\n",
      "            10          -0.30379        0.833\n",
      "            11          -0.29916        0.833\n",
      "            12          -0.29528        0.834\n",
      "            13          -0.29198        0.835\n",
      "            14          -0.28913        0.835\n",
      "            15          -0.28664        0.836\n",
      "            16          -0.28445        0.837\n",
      "            17          -0.28250        0.837\n",
      "            18          -0.28075        0.837\n",
      "            19          -0.27917        0.838\n",
      "            20          -0.27774        0.839\n",
      "            21          -0.27643        0.838\n",
      "            22          -0.27523        0.838\n",
      "            23          -0.27413        0.839\n",
      "            24          -0.27311        0.839\n",
      "            25          -0.27216        0.840\n",
      "            26          -0.27128        0.840\n",
      "            27          -0.27046        0.840\n",
      "            28          -0.26969        0.841\n",
      "            29          -0.26897        0.841\n",
      "            30          -0.26829        0.840\n",
      "            31          -0.26765        0.841\n",
      "            32          -0.26705        0.841\n",
      "            33          -0.26648        0.840\n",
      "            34          -0.26594        0.841\n",
      "            35          -0.26542        0.840\n",
      "            36          -0.26494        0.841\n",
      "            37          -0.26447        0.841\n",
      "            38          -0.26403        0.841\n",
      "            39          -0.26361        0.841\n",
      "            40          -0.26321        0.841\n",
      "            41          -0.26282        0.841\n",
      "            42          -0.26246        0.841\n",
      "            43          -0.26210        0.841\n",
      "            44          -0.26176        0.841\n",
      "            45          -0.26144        0.841\n",
      "            46          -0.26113        0.842\n",
      "            47          -0.26083        0.842\n",
      "            48          -0.26054        0.842\n",
      "            49          -0.26026        0.842\n",
      "         Final          -0.25999        0.842\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, devtest_set, test_set = featuresets[1000:], featuresets[500:1000], featuresets[:500]\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, max_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the sake of argument, I repeated the experiment with a very simple model.  In this case, the MaxEnt classifier performed the worst, and the Decision Tree and Naive Bayes classifier produced identical results:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter'  : word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    "\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (50 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.372\n",
      "             2          -0.37746        0.760\n",
      "             3          -0.37703        0.760\n",
      "             4          -0.37676        0.760\n",
      "             5          -0.37659        0.760\n",
      "             6          -0.37646        0.760\n",
      "             7          -0.37637        0.760\n",
      "             8          -0.37630        0.760\n",
      "             9          -0.37624        0.760\n",
      "            10          -0.37619        0.760\n",
      "            11          -0.37615        0.760\n",
      "            12          -0.37612        0.760\n",
      "            13          -0.37609        0.760\n",
      "            14          -0.37606        0.760\n",
      "            15          -0.37604        0.760\n",
      "            16          -0.37602        0.760\n",
      "            17          -0.37601        0.760\n",
      "            18          -0.37599        0.760\n",
      "            19          -0.37598        0.760\n",
      "            20          -0.37596        0.760\n",
      "            21          -0.37595        0.760\n",
      "            22          -0.37594        0.760\n",
      "            23          -0.37593        0.760\n",
      "            24          -0.37592        0.760\n",
      "            25          -0.37592        0.760\n",
      "            26          -0.37591        0.760\n",
      "            27          -0.37590        0.760\n",
      "            28          -0.37590        0.760\n",
      "            29          -0.37589        0.760\n",
      "            30          -0.37588        0.760\n",
      "            31          -0.37588        0.760\n",
      "            32          -0.37587        0.760\n",
      "            33          -0.37587        0.760\n",
      "            34          -0.37586        0.760\n",
      "            35          -0.37586        0.760\n",
      "            36          -0.37586        0.760\n",
      "            37          -0.37585        0.760\n",
      "            38          -0.37585        0.760\n",
      "            39          -0.37585        0.760\n",
      "            40          -0.37584        0.760\n",
      "            41          -0.37584        0.760\n",
      "            42          -0.37584        0.760\n",
      "            43          -0.37583        0.760\n",
      "            44          -0.37583        0.760\n",
      "            45          -0.37583        0.760\n",
      "            46          -0.37583        0.760\n",
      "            47          -0.37582        0.760\n",
      "            48          -0.37582        0.760\n",
      "            49          -0.37582        0.760\n",
      "         Final          -0.37582        0.760\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, devtest_set, test_set = featuresets[1000:], featuresets[500:1000], featuresets[:500]\n",
    "classifier = nltk.classify.MaxentClassifier.train(train_set, max_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I would argue that the iterative optimization of MaxEnt produces the best accuracy for all but the simplest models.  Decision trees don't do as well with complicated feature models, since they can't handle very well cases where values interact.   Naive Bayes classifiers do slightly better, but they also suffer in complicated models, since they can't handle correlation very well.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. \n",
    "\n",
    "☼ The synonyms *strong* and *powerful* pattern differently (try combining them with *chip* and *sales*). What features are relevant in this distinction? Build a classifier that predicts when each word should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First, I converted `brown.sents()` into an `nltk.Text()` so that I could make concordances for both words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text([w for s in brown.sents() for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 202 matches:\n",
      "n . Georgia Republicans are getting strong encouragement to enter a candidate \n",
      "industry by Southern states , and a strong fight against discrimination in bus\n",
      "cit on where I stand to bring you a strong , dynamic administration . I'm not \n",
      "using bill is expected to encounter strong opposition by the coalition of Sout\n",
      "p with some good stops and showed a strong arm at third base . Bingles and bob\n",
      " aerials . Skorich , however , is a strong advocate of a balanced attack -- sp\n",
      "and swept all championships . Carol Strong , 13 , of Cedar Mill cooked the cha\n",
      "emain are those that were headed by strong executives , men with the abilities\n",
      "s the only antidote -- a Biblically strong Christianity . So the Christian You\n",
      " control machinery . In the U. S. , strong pressures have been building up for\n",
      " the forthcoming encounter produced strong reactions in the U. S. of both appr\n",
      ", Player said later , `` and I felt strong . When you're playing like that you\n",
      "s and good nature and by his kind , strong and somewhat sly face '' . G. David\n",
      "favored a unified Congo with a very strong central government . And fourth , t\n",
      "f Dr. Conant's specific views . His strong opposition to the transfer of Negro\n",
      " and graduate schools will run into strong opposition from these colleges them\n",
      "reak of violence . They are putting strong pressure on their police department\n",
      "nt misbehavior . His desires are so strong that he needs constant reassurance \n",
      " . Dissenting views of senators Two strong dissents from the majority report o\n",
      "self-determination , when we become strong enough to resist any more drifts to\n",
      "om the other also developed , and a strong conviction grew that each had insig\n",
      "osition of yet another pressure , a strong one , from the outside , might caus\n",
      " the Albanian incident shows , have strong suspicions that Khrushchev is anxio\n",
      " feel that , militarily , Russia is strong enough to support them in the `` ju\n",
      "'' nearly liquidated him . He seems strong enough inside the party to cope wit\n"
     ]
    }
   ],
   "source": [
    "text.concordance('strong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 63 matches:\n",
      " . Cincinnati , Ohio ( AP ) -- The powerful New York Yankees won their 19th wo\n",
      "ecrets off to Moscow , either on a powerful transmitter buried under the kitch\n",
      "gerous precedent '' ! ! Cannon , a powerful , conservative man , brought welco\n",
      "ker of the House , and second most powerful man in Washington . Mr. Rayburn wa\n",
      "d . But these are side issues to a powerful central theme . That theme cuts th\n",
      " world is dominated by two large , powerful nations armed with murderous nucle\n",
      "sively in his footsteps . When one powerful nation strives to emulate the succ\n",
      ", because it provides a direct and powerful mirror in which the Negro can hear\n",
      "( the charge-excess ) are far more powerful than gravitation , the surface hyd\n",
      "world in balanced harmony around a powerful central axis . The tremendous emph\n",
      ". But such a reaction obscures the powerful efforts made in the past by both N\n",
      "ch a decision should have placed a powerful weapon in the hands of the entire \n",
      ", imagine that you can put on more powerful glasses and go back inside the ato\n",
      "ual control over already supremely powerful divine spirits , was held responsi\n",
      "es a toughness and bravado that no powerful otherworldly ancestor could ever i\n",
      " burning lusts of the flesh or the powerful victory of Holy Spirit discipline \n",
      "ir of our new presses . Here was a powerful , ready-made medium , but it could\n",
      "ways , but they would require more powerful engines than we have today . There\n",
      "for spending money to develop more powerful engines because of the erroneous b\n",
      "Credo of words -- with torrents of powerful music . Compare the vast differenc\n",
      "ted from the elements , gives them powerful visual appeal at night ; ; during \n",
      "ce prolongs the pain while a swift powerful act gets it over with and leaves t\n",
      "uville where she met a member of a powerful Greek syndicate of gamblers '' . T\n",
      "igious organization , especially a powerful or dominant one , to impose throug\n",
      "ts as Torrio and Capone , the most powerful and most dangerous mob leader in t\n"
     ]
    }
   ],
   "source": [
    "text.concordance('powerful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Then I made lists containing all the sentences with either 'strong' or 'powerful'. In these sentences I replaced the two words with an ellipsis, and then placed each of these sentences in a tuple, along with a value that would identify whether the sentence used to conatain 'strong' or 'powerful':*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts = brown.tagged_sents()\n",
    "strong = [s for s in bts for t in s if 'strong' in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in strong:\n",
    "    for i in range(len(s)):\n",
    "        if s[i][0] == 'strong':\n",
    "            s[i] = '_____'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_strong = [(s, 'strong') for s in strong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerful = [s for s in bts for t in s if 'powerful' in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in powerful:\n",
    "    for i in range(len(s)):\n",
    "        if s[i][0] == 'powerful':\n",
    "            s[i] = '_____'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_powerful = [(s, 'powerful') for s in powerful]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example sentence:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('--', '--'),\n",
       "  ('The', 'AT'),\n",
       "  '_____',\n",
       "  ('New', 'JJ-TL'),\n",
       "  ('York', 'NP-TL'),\n",
       "  ('Yankees', 'NPS-TL'),\n",
       "  ('won', 'VBD'),\n",
       "  ('their', 'PP$'),\n",
       "  ('19th', 'OD'),\n",
       "  ('world', 'NN'),\n",
       "  ('series', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('5-game', 'JJ'),\n",
       "  ('romp', 'NN'),\n",
       "  ('over', 'IN'),\n",
       "  ('outclassed', 'VBN'),\n",
       "  ('Cincinnati', 'NP'),\n",
       "  (',', ','),\n",
       "  ('crushing', 'VBG'),\n",
       "  ('the', 'AT'),\n",
       "  ('Reds', 'NNS-TL'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'AT'),\n",
       "  ('humiliating', 'JJ'),\n",
       "  ('13-5', 'CD'),\n",
       "  ('barrage', 'NN'),\n",
       "  ('Monday', 'NR'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('loosely', 'RB'),\n",
       "  ('played', 'VBN'),\n",
       "  ('finale', 'NN'),\n",
       "  ('.', '.')],\n",
       " 'powerful')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_powerful[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*My first idea for features was to look at the words immediately before and after the words in question.  After a little trial and error, I realized that the POS for these words wasn't helping, so I only looked at the words themselves:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_features(sent):\n",
    "    p = sent.index('_____')\n",
    "    if p >= 2:\n",
    "        left2 = sent[p - 2][0]\n",
    "    else:\n",
    "        left2 = (None, None)\n",
    "    if p >= 1:\n",
    "        left1 = sent[p - 1][0]\n",
    "    else:\n",
    "        left1 = (None, None)\n",
    "    return {'left2':  left2,\n",
    "            'left1':  left1,\n",
    "            'right':  sent[p + 1][0]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left2': '--', 'left1': 'The', 'right': 'New'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the above function\n",
    "adj_features(labeled_powerful[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating and shuffling the sentences\n",
    "\n",
    "labeled_sents = (labeled_strong + labeled_powerful)\n",
    "\n",
    "random.shuffle(labeled_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the classifier\n",
    "\n",
    "featuresets = [(adj_features(s), adj) for (s, adj) in labeled_sents]\n",
    "test_set, train_set = featuresets[:25], featuresets[25:]\n",
    "classifier = nltk.classify.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The initial results are not good at all.  Since there are 265 sentences and 202 use the word 'strong', we could just select 'strong' for every sentence and get 76% accuracy.  We did worse than that.*\n",
    "\n",
    "*I next decided to use as features all of the other words in the sentences.  I used code similar to that for document classification, where we made a dictionary containing all of the words in the sent.  These words were checked against the most common words in the Brown Corpus.  I repeated the experiment several times for different sizes of common words.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [1000, 2000, 3000, 4000, 5000]\n",
    "perfs = []\n",
    "\n",
    "for v in vocab:\n",
    "    all_words = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "    word_features = [w for w, _ in all_words.most_common(v)]\n",
    "\n",
    "    def sent_features(sent):\n",
    "        sent_words = set([t[0] for t in sent])\n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            features['contains({})'.format(word)] = (word in sent_words)\n",
    "        return features\n",
    "    \n",
    "    featuresets = [(sent_features(n), adj) for (n, adj) in labeled_sents]\n",
    "    test_set, train_set = featuresets[:25], featuresets[25:]\n",
    "    classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    perfs.append(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.76, 0.68, 0.76, 0.76, 0.76]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The results are better, but they are still only comparable to what we would get if we simply chose the most common adjective ('strong') for every sentence.*\n",
    "\n",
    "*I repeated the experiment with other classifiers: the decision tree classifier performs no better and takes very long to run.  The MaxEnt classifier only works with a small vocabulary size - when I tried to run the classifier with larger vocabularies, it only ran two iterations before throwing an error.*\n",
    "\n",
    "*Although the results are not optimal, we also have to remember that our corpus is very small: only about 250 examples.  I would argue that it is perfectly reasonable to expect our accuracy to increase significantly with a large corpus.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. \n",
    "\n",
    "◑ The dialog act classifier assigns labels to individual posts, without considering the context in which the post is found. However, dialog acts are highly dependent on context, and some sequences of dialog act are much more likely than others. For example, a ynQuestion dialog act is much more likely to be answered by a `yanswer` than by a `greeting`. Make use of this fact to build a consecutive classifier for labeling dialog acts. Be sure to consider what features might be useful. See the code for the consecutive classifier for part-of-speech tags in [1.7](https://www.nltk.org/book/ch06.html#code-consecutive-pos-tagger \"1.7\") to get some ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = nltk.corpus.nps_chat.xml_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now im left with this gay name Statement\n",
      ":P Emotion\n",
      "PART System\n",
      "hey everyone   Greet\n",
      "ah well Statement\n",
      "NICK :10-19-20sUser7 System\n",
      "10-19-20sUser7 is a gay name. Accept\n",
      ".ACTION gives 10-19-20sUser121 a golf clap. System\n",
      ":) Emotion\n",
      "JOIN System\n",
      "hi 10-19-20sUser59 Greet\n",
      "26/ m/ ky women that are nice please pm me Statement\n",
      "JOIN System\n",
      "PART System\n",
      "there ya go 10-19-20sUser7 Statement\n",
      "don't golf clap me. Reject\n",
      "fuck you 10-19-20sUser121:@ Reject\n",
      "whats everyone up to? whQuestion\n",
      "PART System\n",
      "PART System\n"
     ]
    }
   ],
   "source": [
    "# Examining the posts\n",
    "\n",
    "for i in range(20):\n",
    "    print(post[i].text, post[i].get('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_words = [w.lower() for i in range(len(post)) \n",
    "              for w in post[i].text.split()]\n",
    "fd = nltk.FreqDist(post_words)\n",
    "word_features = [w for (w, _) in fd.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialog_features(post, i):\n",
    "    post_words = set(post[i].text.split())\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in post_words)\n",
    "    if i == 0:\n",
    "        features[\"prev-class\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-class\"] = post[i - 1].get('class')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gay', 'im', 'left', 'name', 'now', 'this', 'with'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(post[0].text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "for i in range(len(post)):\n",
    "    featuresets.append( (dialog_features(post, i), post[i].get('class') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.9)\n",
    "train_set, test_set = featuresets[:size], featuresets[size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = set([c for (_, c) in featuresets])\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Clarify', 'yAnswer', 'Bye', 'Reject', 'Emotion', 'Continuer', 'Other', 'System', 'ynQuestion', 'nAnswer', 'Statement', 'Emphasis', 'Accept', 'whQuestion', 'Greet'}"
     ]
    }
   ],
   "source": [
    "print(classes, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It takes a while to evaluate accuracy, but considering the number of classes (15) and how little time I put into this classifier, I think 67% accuracy is rather good.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.674550614947966"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. \n",
    "\n",
    "◑ Word features can be very useful for performing document classification, since the words that appear in a document give a strong indication about what its semantic content is. However, many words occur very infrequently, and some of the most informative words in a document may never have occurred in our training data. One solution is to make use of a __lexicon__, which describes how different words relate to one another. Using WordNet lexicon, augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document, making it more likely that they will match words found in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = [w for (w, _) in all_words.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in document_words:\n",
    "        # if word amongst most common words, \n",
    "        # report that this word is in the document,\n",
    "        # otherwise, get wordnet synset\n",
    "        if word in word_features:\n",
    "            features['contains({})'.format(word)] = True\n",
    "        else:\n",
    "            ws = wn.synsets(word) # not bothering with POS\n",
    "            if ws != []:\n",
    "                features['{}'.format(ws[0])] = True\n",
    "    return features\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The baseline classifier exemplified in the book - which didn't use WordNet - had an accuracy of .81, so this one is slightly worse.  One immediate issue is that some last names (e.g., 'Damon') are actually listed in WordNet, although it's clear that the WordNet definition does not refer to the movie star:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "Synset('brilliant.s.03') = True              pos : neg    =     12.7 : 1.0\n",
      "   Synset('feeble.s.01') = True              neg : pos    =     11.5 : 1.0\n",
      "Synset('outstanding.s.01') = True              pos : neg    =     11.5 : 1.0\n",
      "     Synset('deft.s.01') = True              pos : neg    =     10.4 : 1.0\n",
      "    Synset('damon.n.01') = True              pos : neg    =     10.3 : 1.0\n",
      "     Synset('flop.n.02') = True              neg : pos    =     10.2 : 1.0\n",
      "   Synset('hudson.n.01') = True              neg : pos    =     10.2 : 1.0\n",
      " Synset('seamless.a.01') = True              pos : neg    =      9.8 : 1.0\n",
      " Synset('thematic.a.01') = True              pos : neg    =      9.8 : 1.0\n",
      "  Synset('immerse.v.01') = True              pos : neg    =      9.5 : 1.0\n",
      "     Synset('diss.v.01') = True              neg : pos    =      9.2 : 1.0\n",
      "Synset('accessible.a.01') = True              pos : neg    =      9.1 : 1.0\n",
      "   Synset('annual.n.01') = True              pos : neg    =      9.1 : 1.0\n",
      "Synset('agonizingly.r.01') = True              neg : pos    =      8.9 : 1.0\n",
      "    Synset('chaff.n.01') = True              neg : pos    =      8.9 : 1.0\n",
      "Synset('crackbrained.s.01') = True              neg : pos    =      8.9 : 1.0\n",
      "Synset('illogical.a.01') = True              neg : pos    =      8.9 : 1.0\n",
      "Synset('incoherent.a.01') = True              neg : pos    =      8.9 : 1.0\n",
      "  Synset('misfire.n.01') = True              neg : pos    =      8.9 : 1.0\n",
      "Synset('odds_and_ends.n.01') = True              neg : pos    =      8.9 : 1.0\n",
      "Synset('refreshingly.r.01') = True              pos : neg    =      8.4 : 1.0\n",
      "    Synset('trash.n.02') = True              pos : neg    =      8.4 : 1.0\n",
      "     Synset('grip.v.01') = True              pos : neg    =      8.3 : 1.0\n",
      " Synset('cachexia.n.01') = True              neg : pos    =      8.2 : 1.0\n",
      " Synset('fairness.n.01') = True              neg : pos    =      8.2 : 1.0\n",
      " Synset('marauder.n.01') = True              neg : pos    =      8.2 : 1.0\n",
      "  Synset('sterile.s.03') = True              neg : pos    =      8.2 : 1.0\n",
      "    Synset('anger.n.01') = True              pos : neg    =      8.2 : 1.0\n",
      "Synset('unintentional.s.01') = True              neg : pos    =      8.1 : 1.0\n",
      "Synset('all_right.s.01') = True              pos : neg    =      8.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the friend of Phintias who pledged his life that Phintias would return (4th century BC)'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not the guy in Good Will Hunting\n",
    "\n",
    "wn.synset('damon.n.01').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I tried repeating the experiment with various vocabulary thresholds - i.e., for words not amongst the $n$ most common words, the features classifier would use the WordNet synset. As the results show, there wasn't a tremendous difference between word sizes:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "perfs = []\n",
    "\n",
    "for v in vocab:\n",
    "\n",
    "\n",
    "    all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "    word_features = [w for (w, _) in all_words.most_common(v)]\n",
    "\n",
    "    from nltk.corpus import wordnet as wn\n",
    "\n",
    "    def document_features(document):\n",
    "        document_words = set(document)\n",
    "        features = {}\n",
    "        for word in document_words:\n",
    "            if word in word_features:\n",
    "                features['contains({})'.format(word)] = True\n",
    "            else:\n",
    "                ws = wn.synsets(word)\n",
    "                if ws != []:\n",
    "                    features['{}'.format(ws[0])] = True\n",
    "        return features\n",
    "\n",
    "\n",
    "    featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "\n",
    "\n",
    "    train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    perfs.append(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.76, 0.77, 0.77, 0.77, 0.78, 0.78]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(vocab, perfs)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. \n",
    "\n",
    "★ The PP Attachment Corpus is a corpus describing prepositional phrase attachment decisions. Each instance in the corpus is encoded as a `PPAttachment` object:\n",
    "\n",
    "```\n",
    "from nltk.corpus import ppattach\n",
    "ppattach.attachments('training')\n",
    "\n",
    "[PPAttachment(sent='0', verb='join', noun1='board', prep='as', noun2='director', attachment='V'), PPAttachment(sent='1', verb='is', noun1='chairman', prep='of', noun2='N.V.', attachment='N'), ...]\n",
    "\n",
    "inst = ppattach.attachments('training')[1]\n",
    "(inst.noun1, inst.prep, inst.noun2)\n",
    "\n",
    "```\n",
    "Select only the instances where inst.attachment is N:\n",
    "\n",
    "```\n",
    "nattach = [inst for inst in ppattach.attachments('training') \n",
    "           if inst.attachment == 'N']\n",
    "```\n",
    "Using this sub-corpus, build a classifier that attempts to predict which preposition is used to connect a given pair of nouns. For example, given the pair of nouns \"team\" and \"researchers,\" the classifier should predict the preposition \"of\". See the corpus HOWTO at http://nltk.org/howto for more information on using the PP attachment corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PPAttachment(sent='0', verb='join', noun1='board', prep='as', noun2='director', attachment='V'), PPAttachment(sent='1', verb='is', noun1='chairman', prep='of', noun2='N.V.', attachment='N'), ...]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import ppattach\n",
    "ppattach.attachments('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chairman', 'of', 'N.V.')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = ppattach.attachments('training')[1]\n",
    "(inst.noun1, inst.prep, inst.noun2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "nattach = [inst for inst in ppattach.attachments('training') \n",
    "           if inst.attachment == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PPAttachment(sent='1', verb='is', noun1='chairman', prep='of', noun2='N.V.', attachment='N'),\n",
       " PPAttachment(sent='2', verb='named', noun1='director', prep='of', noun2='conglomerate', attachment='N'),\n",
       " PPAttachment(sent='3', verb='caused', noun1='percentage', prep='of', noun2='deaths', attachment='N'),\n",
       " PPAttachment(sent='9', verb='is', noun1='asbestos', prep='in', noun2='products', attachment='N'),\n",
       " PPAttachment(sent='12', verb='led', noun1='team', prep='of', noun2='researchers', attachment='N')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nattach[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are really only three variables of interest here: `verb`, `noun1`, and `noun2`.  Through simple trial and error we can determine the most relevant one is `noun1`, which makes sense: if we have a phrase that starts with \"a team of ...\", it doesn't matter which noun follows the phrase, nor does it matter which verb comes before it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_features(inst):\n",
    "    features = {'noun1': inst.noun1}\n",
    "    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noun1': 'chairman'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checking the feature extractor\n",
    "prep_features(nattach[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(prep_features(inst), inst.prep) for inst in nattach]\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.647329650092081"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The performance of the classifier isn't spectacular.  If we just assigned \"of\" to every case, we'd have an accuracy of 51%.  However, there are 59 possible prepositions, and there really aren't any other features we can extract...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preps = [p for (_, p) in featuresets]\n",
    "len(set(preps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(preps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.508697653014266"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd['of']/len(preps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.\n",
    "\n",
    "★ Suppose you wanted to automatically generate a prose description of a scene, and already had a word to uniquely describe each entity, such as *the jar*, and simply wanted to decide whether to use *in* or *on* in relating various items, e.g. *the book is in the cupboard* vs *the book is on the shelf*. Explore this issue by looking at corpus data; writing programs as needed.\n",
    "\t\t\n",
    "* a.\t\tin the car *versus* on the train\n",
    "\n",
    "* b.\t\tin town *versus* on campus\n",
    "\n",
    "* c.\t\tin the picture *versus* on the screen\n",
    "\n",
    "* d.\t\tin Macbeth *versus* on Letterman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "bts = brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I made a function that will pull out those sentences containing a given phrase:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(corpus, *words):\n",
    "    \"\"\"\n",
    "    Returns from a tagged corpus sentences that \n",
    "    contain a phrase.  Words in the phrase must be\n",
    "    in the same order as listed in the arguments.\n",
    "    \n",
    "    Arguments:\n",
    "    corpus: Tagged corpus.\n",
    "    *words: Words in the phrase to be matched.  Words\n",
    "            must be in the order in which they are to be found.\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = []\n",
    "    lw = len(words)\n",
    "    for s in corpus:\n",
    "        flag = False\n",
    "        for i in range(len(s) - lw):\n",
    "            for j in range(lw):\n",
    "                if s[i + j][0] == words[j]:\n",
    "                    flag = True\n",
    "                else:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                sents.append(s)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = find_sents(bts, \"in\", \"the\", \"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = find_sents(bts, \"on\", \"the\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "town = find_sents(bts, \"in\", \"town\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "campus = find_sents(bts, \"on\", \"campus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture = find_sents(bts, \"in\", \"the\", \"picture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = find_sents(bts, \"on\", \"the\", \"screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macbeth = find_sents(bts, \"in\", \"Macbeth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letterman = find_sents(bts, \"on\", \"Letterman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "21\n",
      "1\n",
      "5\n",
      "6\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sets = [car, train, town, campus, picture, screen, Macbeth, Letterman]\n",
    "for s in sets:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Considering how rarely any of the phrases occur in the largest corpus available in the NLTK (the Brown Corpus), it will be impossible to do this experiment currently.  There are larger corpora available online (e.g., COCA), but these have to be purchased, and are not cheap (approx. \\$250).  The upshot is that I won't be able to complete this problem at this time.* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
